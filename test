import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import plotly.graph_objects as go
from plotly.subplots import make_subplots

warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ==================== DATA LOADING ====================
# Load your actual data here
# df = pd.read_csv('your_data.csv')
# df['Date'] = pd.to_datetime(df['Date'])

# For now, using the structure you described
# Columns: Date, Customer_Name, Customer_Industry, Investment, Amount, Security_Type
# WHERE Amount is ALREADY NET (positive for buy, negative for sell)

# Key event - ONLY April 2nd
apr_event = pd.Timestamp('2025-04-02')

# ==================== DATA PREPROCESSING ====================

# Add derived features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Week'] = df['Date'].dt.isocalendar().week
df['DayOfWeek'] = df['Date'].dt.dayofweek

# Define periods for April event only
df['Period'] = 'Pre-April'
df.loc[df['Date'] >= apr_event, 'Period'] = 'Post-April'

# Create absolute amount for volume calculations
df['Abs_Amount'] = df['Amount'].abs()

# ==================== ANALYSIS FUNCTIONS ====================

def remove_outliers(data, column, percentile=99):
    """Remove outliers beyond 99th percentile"""
    threshold = np.percentile(data[column].abs(), percentile)
    return data[data[column].abs() <= threshold]

def perform_event_study(df, event_date, window_pre=30, window_post=30):
    """Perform event study analysis"""
    event_date = pd.Timestamp(event_date)
    
    # Define windows
    pre_start = event_date - timedelta(days=window_pre)
    post_end = event_date + timedelta(days=window_post)
    
    # Filter data
    event_data = df[(df['Date'] >= pre_start) & (df['Date'] <= post_end)].copy()
    event_data['Days_from_event'] = (event_data['Date'] - event_date).dt.days
    
    # Calculate daily metrics using NET amounts
    daily_metrics = event_data.groupby('Days_from_event').agg({
        'Amount': 'sum',  # Net position (already net)
        'Abs_Amount': 'sum',  # Total volume
        'Customer_Name': 'count'  # Number of trades
    }).reset_index()
    
    daily_metrics.columns = ['Days_from_event', 'Net_Position', 'Total_Volume', 'Trade_Count']
    
    # Calculate abnormal trading (using pre-event average as baseline)
    pre_event = daily_metrics[daily_metrics['Days_from_event'] < 0]
    baseline_volume = pre_event['Total_Volume'].mean()
    baseline_net = pre_event['Net_Position'].mean()
    
    daily_metrics['Abnormal_Volume'] = daily_metrics['Total_Volume'] - baseline_volume
    daily_metrics['Abnormal_Net'] = daily_metrics['Net_Position'] - baseline_net
    
    # Cumulative abnormal
    daily_metrics['CAV'] = daily_metrics['Abnormal_Volume'].cumsum()
    daily_metrics['CAN'] = daily_metrics['Abnormal_Net'].cumsum()
    
    return daily_metrics

def portfolio_shift_analysis(df, date_cutoff):
    """Analyze portfolio composition shifts"""
    pre_data = df[df['Date'] < date_cutoff]
    post_data = df[df['Date'] >= date_cutoff]
    
    # Calculate security type distribution using absolute amounts
    pre_dist = pre_data.groupby('Security_Type')['Abs_Amount'].sum()
    post_dist = post_data.groupby('Security_Type')['Abs_Amount'].sum()
    
    pre_pct = (pre_dist / pre_dist.sum() * 100).round(2)
    post_pct = (post_dist / post_dist.sum() * 100).round(2)
    
    shift_df = pd.DataFrame({
        'Pre_Period_%': pre_pct,
        'Post_Period_%': post_pct,
        'Change_%': post_pct - pre_pct
    })
    
    return shift_df

def customer_behavior_clustering(df, event_date):
    """Cluster customers based on behavioral changes"""
    event_date = pd.Timestamp(event_date)
    
    # Calculate customer metrics pre/post
    customer_metrics = []
    
    for customer in df['Customer_Name'].unique():
        cust_data = df[df['Customer_Name'] == customer]
        
        pre_data = cust_data[cust_data['Date'] < event_date]
        post_data = cust_data[cust_data['Date'] >= event_date]
        
        if len(pre_data) > 0 and len(post_data) > 0:
            # Calculate metrics using absolute amounts for volumes
            pre_volume = pre_data['Abs_Amount'].sum()
            post_volume = post_data['Abs_Amount'].sum()
            
            # Net positions
            pre_net = pre_data['Amount'].sum()
            post_net = post_data['Amount'].sum()
            
            customer_metrics.append({
                'Customer': customer,
                'Volume_Change_%': ((post_volume - pre_volume) / pre_volume * 100) if pre_volume > 0 else 0,
                'Net_Position_Change': post_net - pre_net,
                'Pre_Trade_Count': len(pre_data),
                'Post_Trade_Count': len(post_data),
                'Total_Volume': pre_volume + post_volume
            })
    
    customer_df = pd.DataFrame(customer_metrics)
    
    # Remove outliers at 99th percentile
    customer_df = remove_outliers(customer_df, 'Volume_Change_%', 99)
    customer_df = remove_outliers(customer_df, 'Net_Position_Change', 99)
    
    # Clustering
    features = ['Volume_Change_%', 'Net_Position_Change']
    X = customer_df[features].fillna(0)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Determine optimal number of clusters
    n_clusters = min(4, max(2, len(customer_df) // 50))
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    customer_df['Cluster'] = kmeans.fit_predict(X_scaled)
    
    return customer_df

def statistical_significance_tests(df, event_date, window=30):
    """Perform statistical tests for significance"""
    event_date = pd.Timestamp(event_date)
    
    # Define periods
    pre_start = event_date - timedelta(days=window)
    pre_end = event_date - timedelta(days=1)
    post_start = event_date
    post_end = event_date + timedelta(days=window)
    
    pre_data = df[(df['Date'] >= pre_start) & (df['Date'] <= pre_end)]
    post_data = df[(df['Date'] >= post_start) & (df['Date'] <= post_end)]
    
    results = {}
    
    # 1. T-test for average daily trading volume
    pre_daily = pre_data.groupby('Date')['Abs_Amount'].sum()
    post_daily = post_data.groupby('Date')['Abs_Amount'].sum()
    
    t_stat, p_value = stats.ttest_ind(pre_daily, post_daily)
    results['Volume_Change'] = {
        't_statistic': t_stat,
        'p_value': p_value,
        'pre_mean': pre_daily.mean(),
        'post_mean': post_daily.mean(),
        'percent_change': ((post_daily.mean() - pre_daily.mean()) / pre_daily.mean() * 100)
    }
    
    # 2. Chi-square test for security type distribution
    pre_security = pre_data['Security_Type'].value_counts()
    post_security = post_data['Security_Type'].value_counts()
    
    # Align indices
    all_securities = list(set(pre_security.index) | set(post_security.index))
    pre_counts = [pre_security.get(s, 0) for s in all_securities]
    post_counts = [post_security.get(s, 0) for s in all_securities]
    
    # Scale expected frequencies
    post_total = sum(post_counts)
    pre_total = sum(pre_counts)
    expected_counts = [count * post_total / pre_total for count in pre_counts]
    
    chi2, p_chi = stats.chisquare(post_counts, expected_counts)
    results['Security_Distribution'] = {
        'chi2_statistic': chi2,
        'p_value': p_chi
    }
    
    # 3. Wilcoxon signed-rank test for paired customer data
    customer_changes = []
    for customer in df['Customer_Name'].unique():
        pre_cust = pre_data[pre_data['Customer_Name'] == customer]['Abs_Amount'].sum()
        post_cust = post_data[post_data['Customer_Name'] == customer]['Abs_Amount'].sum()
        if pre_cust > 0 and post_cust > 0:
            customer_changes.append(post_cust - pre_cust)
    
    if len(customer_changes) > 20:
        wilcoxon_stat, wilcoxon_p = stats.wilcoxon(customer_changes)
        results['Customer_Behavior'] = {
            'wilcoxon_statistic': wilcoxon_stat,
            'p_value': wilcoxon_p,
            'n_customers': len(customer_changes)
        }
    
    return results

# ==================== MAIN ANALYSIS FOR APRIL 2 ONLY ====================

event_date = apr_event
event_name = "Liberation Day Tariff (Apr 2, 2025)"

print("=" * 60)
print(f"BANKING TRADE ANALYSIS - {event_name}")
print("=" * 60)

# Remove outliers at 99th percentile
print("\nRemoving outliers at 99th percentile...")
original_count = len(df)
df_clean = remove_outliers(df, 'Amount', 99)
removed_count = original_count - len(df_clean)
print(f"Removed {removed_count} outlier trades ({removed_count/original_count*100:.2f}%)")

# Use cleaned data for analysis
df = df_clean

# 1. Event Study Analysis
print("\n1. EVENT STUDY ANALYSIS")
print("-" * 40)

event_study_result = perform_event_study(df, event_date, window_pre=30, window_post=30)

# Create event study visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle(f'Event Study: {event_name}', fontsize=16)

# Plot 1: Daily Trading Volume
ax1 = axes[0, 0]
ax1.plot(event_study_result['Days_from_event'], event_study_result['Total_Volume']/1e6, 
         marker='o', markersize=4)
ax1.axvline(x=0, color='red', linestyle='--', label='Event Date')
ax1.set_xlabel('Days from Event')
ax1.set_ylabel('Trading Volume (Million $)')
ax1.set_title('Daily Trading Volume')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Net Position
ax2 = axes[0, 1]
colors = ['green' if x > 0 else 'red' for x in event_study_result['Net_Position']]
ax2.bar(event_study_result['Days_from_event'], event_study_result['Net_Position']/1e6, 
        color=colors, alpha=0.7)
ax2.axvline(x=0, color='red', linestyle='--', label='Event Date')
ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
ax2.set_xlabel('Days from Event')
ax2.set_ylabel('Net Position (Million $)')
ax2.set_title('Daily Net Position (Buy - Sell)')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Cumulative Abnormal Volume
ax3 = axes[1, 0]
ax3.plot(event_study_result['Days_from_event'], event_study_result['CAV']/1e6, 
         marker='o', markersize=4, color='blue')
ax3.axvline(x=0, color='red', linestyle='--', label='Event Date')
ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
ax3.set_xlabel('Days from Event')
ax3.set_ylabel('Cumulative Abnormal Volume (Million $)')
ax3.set_title('Cumulative Abnormal Trading Volume')
ax3.legend()
ax3.grid(True, alpha=0.3)

# Plot 4: Trade Count
ax4 = axes[1, 1]
ax4.bar(event_study_result['Days_from_event'], event_study_result['Trade_Count'], 
        color='skyblue', alpha=0.7)
ax4.axvline(x=0, color='red', linestyle='--', label='Event Date')
ax4.set_xlabel('Days from Event')
ax4.set_ylabel('Number of Trades')
ax4.set_title('Daily Trade Count')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('april_event_study.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"Trading volume change: {event_study_result['Abnormal_Volume'].sum()/1e6:.2f}M")
print(f"Net position change: {event_study_result['Abnormal_Net'].sum()/1e6:.2f}M")

# 2. Portfolio Shift Analysis
print("\n2. PORTFOLIO COMPOSITION SHIFT")
print("-" * 40)

portfolio_shift = portfolio_shift_analysis(df, event_date)
print("\nSecurity Type Distribution Changes:")
print(portfolio_shift)

# 3. Statistical Significance Tests
print("\n3. STATISTICAL SIGNIFICANCE TESTS")
print("-" * 40)

stat_results = statistical_significance_tests(df, event_date)

print("\nVolume Change Test:")
print(f"  Pre-period mean: ${stat_results['Volume_Change']['pre_mean']/1e6:.2f}M")
print(f"  Post-period mean: ${stat_results['Volume_Change']['post_mean']/1e6:.2f}M")
print(f"  Change: {stat_results['Volume_Change']['percent_change']:.1f}%")
print(f"  T-statistic: {stat_results['Volume_Change']['t_statistic']:.3f}")
print(f"  P-value: {stat_results['Volume_Change']['p_value']:.4f}")

print("\nSecurity Distribution Test:")
print(f"  Chi-square statistic: {stat_results['Security_Distribution']['chi2_statistic']:.3f}")
print(f"  P-value: {stat_results['Security_Distribution']['p_value']:.4f}")

if 'Customer_Behavior' in stat_results:
    print("\nCustomer Behavior Test (Wilcoxon):")
    print(f"  Number of customers: {stat_results['Customer_Behavior']['n_customers']}")
    print(f"  Wilcoxon statistic: {stat_results['Customer_Behavior']['wilcoxon_statistic']:.3f}")
    print(f"  P-value: {stat_results['Customer_Behavior']['p_value']:.4f}")

# 4. Customer Clustering Analysis
print("\n4. CUSTOMER SEGMENTATION ANALYSIS")
print("-" * 40)

customer_clusters = customer_behavior_clustering(df, event_date)
cluster_summary = customer_clusters.groupby('Cluster').agg({
    'Volume_Change_%': ['mean', 'std'],
    'Net_Position_Change': ['mean', 'std'],
    'Customer': 'count'
}).round(2)

print("\nCustomer Segment Characteristics:")
print(cluster_summary)

# 5. Seasonality Check with 2024
print("\n5. SEASONALITY ANALYSIS - 2024 vs 2025 Comparison")
print("-" * 40)

event_2025 = pd.Timestamp('2025-04-02')
event_2024 = pd.Timestamp('2024-04-02')
window_days = 30

# Compare with 2024 same period
def get_period_metrics(df, center_date, window):
    start_date = center_date - timedelta(days=window)
    end_date = center_date + timedelta(days=window)
    
    pre_data = df[(df['Date'] >= start_date) & (df['Date'] < center_date)]
    post_data = df[(df['Date'] >= center_date) & (df['Date'] <= end_date)]
    
    # Calculate metrics
    pre_volume = pre_data['Abs_Amount'].sum()
    post_volume = post_data['Abs_Amount'].sum()
    volume_change = ((post_volume - pre_volume) / pre_volume * 100) if pre_volume > 0 else 0
    
    return {
        'pre_volume': pre_volume,
        'post_volume': post_volume,
        'volume_change_%': volume_change
    }

metrics_2024 = get_period_metrics(df, event_2024, window_days)
metrics_2025 = get_period_metrics(df, event_2025, window_days)

print(f"\nApril Period Comparison:")
print(f"\n2024 (Same period):")
print(f"  Pre-period volume: ${metrics_2024['pre_volume']/1e6:.2f}M")
print(f"  Post-period volume: ${metrics_2024['post_volume']/1e6:.2f}M")
print(f"  Volume change: {metrics_2024['volume_change_%']:.1f}%")

print(f"\n2025 (Event year):")
print(f"  Pre-period volume: ${metrics_2025['pre_volume']/1e6:.2f}M")
print(f"  Post-period volume: ${metrics_2025['post_volume']/1e6:.2f}M")
print(f"  Volume change: {metrics_2025['volume_change_%']:.1f}%")

print(f"\nDifference-in-Differences:")
print(f"  2025 change - 2024 change = {metrics_2025['volume_change_%'] - metrics_2024['volume_change_%']:.1f}%")
print(f"  This is the TRUE event impact after controlling for seasonality")

print("\n" + "=" * 60)
print("Analysis complete. Generated files:")
print("- april_event_study.png")
print("=" * 60)
