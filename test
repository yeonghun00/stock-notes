 ==================== ANALYSIS FUNCTIONS ====================

def calculate_trading_metrics(data, groupby_cols=['Date']):
    """Calculate key trading metrics"""
    metrics = data.groupby(groupby_cols).agg({
        'Amount': ['sum', 'mean', 'count'],
        'Net_Amount': 'sum'
    }).reset_index()
    
    metrics.columns = ['_'.join(col).strip() for col in metrics.columns.values]
    metrics.columns = [col.replace('_', '') if col.endswith('_') else col for col in metrics.columns]
    
    return metrics

def perform_event_study(df, event_date, window_pre=30, window_post=30):
    """Perform event study analysis"""
    event_date = pd.Timestamp(event_date)
    
    # Define windows
    pre_start = event_date - timedelta(days=window_pre)
    post_end = event_date + timedelta(days=window_post)
    
    # Filter data
    event_data = df[(df['Date'] >= pre_start) & (df['Date'] <= post_end)].copy()
    event_data['Days_from_event'] = (event_data['Date'] - event_date).dt.days
    
    # Calculate daily metrics
    daily_metrics = event_data.groupby('Days_from_event').agg({
        'Amount': ['sum', 'count'],
        'Net_Amount': 'sum'
    }).reset_index()
    
    daily_metrics.columns = ['Days_from_event', 'Total_Amount', 'Trade_Count', 'Net_Position']
    
    # Calculate abnormal trading (using pre-event average as baseline)
    pre_event = daily_metrics[daily_metrics['Days_from_event'] < 0]
    baseline_amount = pre_event['Total_Amount'].mean()
    baseline_count = pre_event['Trade_Count'].mean()
    
    daily_metrics['Abnormal_Amount'] = daily_metrics['Total_Amount'] - baseline_amount
    daily_metrics['Abnormal_Count'] = daily_metrics['Trade_Count'] - baseline_count
    
    # Cumulative abnormal
    daily_metrics['CAR'] = daily_metrics['Abnormal_Amount'].cumsum()
    
    return daily_metrics

def portfolio_shift_analysis(df, date_cutoff):
    """Analyze portfolio composition shifts"""
    pre_data = df[df['Date'] < date_cutoff]
    post_data = df[df['Date'] >= date_cutoff]
    
    # Calculate security type distribution
    pre_dist = pre_data.groupby('Security_Type')['Amount'].sum()
    post_dist = post_data.groupby('Security_Type')['Amount'].sum()
    
    pre_pct = (pre_dist / pre_dist.sum() * 100).round(2)
    post_pct = (post_dist / post_dist.sum() * 100).round(2)
    
    shift_df = pd.DataFrame({
        'Pre_Period_%': pre_pct,
        'Post_Period_%': post_pct,
        'Change_%': post_pct - pre_pct
    })
    
    return shift_df

def customer_behavior_clustering(df, event_date):
    """Cluster customers based on behavioral changes"""
    event_date = pd.Timestamp(event_date)
    
    # Calculate customer metrics pre/post
    customer_metrics = []
    
    for customer in df['Customer_Name'].unique():
        cust_data = df[df['Customer_Name'] == customer]
        
        pre_data = cust_data[cust_data['Date'] < event_date]
        post_data = cust_data[cust_data['Date'] >= event_date]
        
        if len(pre_data) > 0 and len(post_data) > 0:
            # Calculate metrics
            pre_foreign = pre_data[pre_data['Security_Type'] == 'Foreign Stocks']['Amount'].sum()
            post_foreign = post_data[post_data['Security_Type'] == 'Foreign Stocks']['Amount'].sum()
            
            pre_total = pre_data['Amount'].sum()
            post_total = post_data['Amount'].sum()
            
            pre_foreign_pct = (pre_foreign / pre_total * 100) if pre_total > 0 else 0
            post_foreign_pct = (post_foreign / post_total * 100) if post_total > 0 else 0
            
            # Calculate absolute changes for better clustering
            foreign_abs_change = abs(post_foreign_pct - pre_foreign_pct)
            volume_abs_change = abs((post_total - pre_total) / pre_total * 100) if pre_total > 0 else 0
            
            customer_metrics.append({
                'Customer': customer,
                'Pre_Foreign_%': pre_foreign_pct,
                'Post_Foreign_%': post_foreign_pct,
                'Foreign_Change_%': post_foreign_pct - pre_foreign_pct,
                'Volume_Change_%': ((post_total - pre_total) / pre_total * 100) if pre_total > 0 else 0,
                'Foreign_Abs_Change_%': foreign_abs_change,
                'Volume_Abs_Change_%': volume_abs_change,
                'Pre_Trade_Count': len(pre_data),
                'Post_Trade_Count': len(post_data),
                'Total_Amount': pre_total + post_total
            })
    
    customer_df = pd.DataFrame(customer_metrics)
    
    # Filter out customers with minimal changes (less than 5% in both dimensions)
    significant_customers = customer_df[
        (customer_df['Foreign_Abs_Change_%'] > 5) | 
        (customer_df['Volume_Abs_Change_%'] > 10)
    ].copy()
    
    # If too few customers show significant changes, use all customers but different clustering
    if len(significant_customers) < 50:
        print(f"Warning: Only {len(significant_customers)} customers show significant changes")
        significant_customers = customer_df.copy()
    
    # Perform clustering on significant customers
    features = ['Foreign_Change_%', 'Volume_Change_%']
    X = significant_customers[features].fillna(0)
    
    # Use better clustering parameters
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Determine optimal number of clusters using elbow method
    n_clusters = min(4, max(2, len(significant_customers) // 50))
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    significant_customers['Cluster'] = kmeans.fit_predict(X_scaled)
    
    # Name clusters based on actual behavior patterns
    cluster_centers = pd.DataFrame(
        scaler.inverse_transform(kmeans.cluster_centers_), 
        columns=features
    )
    
    # Dynamic cluster naming based on actual patterns
    cluster_names = {}
    for i in range(n_clusters):
        foreign_change = cluster_centers.iloc[i]['Foreign_Change_%']
        volume_change = cluster_centers.iloc[i]['Volume_Change_%']
        
        if abs(foreign_change) < 5 and abs(volume_change) < 10:
            cluster_names[i] = 'Stable Investors'
        elif foreign_change < -10:
            cluster_names[i] = 'Foreign Reducers'
        elif foreign_change > 10:
            cluster_names[i] = 'Foreign Increasers'
        elif volume_change > 20:
            cluster_names[i] = 'Volume Increasers'
        elif volume_change < -20:
            cluster_names[i] = 'Volume Reducers'
        else:
            cluster_names[i] = 'Moderate Adjusters'
    
    significant_customers['Cluster_Name'] = significant_customers['Cluster'].map(cluster_names)
    
    # Add back the stable customers if they were filtered out
    if len(significant_customers) < len(customer_df):
        stable_customers = customer_df[
            (customer_df['Foreign_Abs_Change_%'] <= 5) & 
            (customer_df['Volume_Abs_Change_%'] <= 10)
        ].copy()
        stable_customers['Cluster'] = -1
        stable_customers['Cluster_Name'] = 'Stable Investors'
        
        customer_df = pd.concat([significant_customers, stable_customers], ignore_index=True)
    else:
        customer_df = significant_customers
    
    return customer_df

def statistical_significance_tests(df, event_date, window=30):
    """Perform statistical tests for significance"""
    event_date = pd.Timestamp(event_date)
    
    # Define periods
    pre_start = event_date - timedelta(days=window)
    pre_end = event_date - timedelta(days=1)
    post_start = event_date
    post_end = event_date + timedelta(days=window)
    
    pre_data = df[(df['Date'] >= pre_start) & (df['Date'] <= pre_end)]
    post_data = df[(df['Date'] >= post_start) & (df['Date'] <= post_end)]
    
    results = {}
    
    # 1. T-test for average daily trading volume
    pre_daily = pre_data.groupby('Date')['Amount'].sum()
    post_daily = post_data.groupby('Date')['Amount'].sum()
    
    t_stat, p_value = stats.ttest_ind(pre_daily, post_daily)
    results['Volume_Change'] = {
        't_statistic': t_stat,
        'p_value': p_value,
        'pre_mean': pre_daily.mean(),
        'post_mean': post_daily.mean(),
        'percent_change': ((post_daily.mean() - pre_daily.mean()) / pre_daily.mean() * 100)
    }
    
    # 2. Chi-square test for security type distribution
    pre_security = pre_data['Security_Type'].value_counts()
    post_security = post_data['Security_Type'].value_counts()
    
    # Align indices
    all_securities = list(set(pre_security.index) | set(post_security.index))
    pre_counts = [pre_security.get(s, 0) for s in all_securities]
    post_counts = [post_security.get(s, 0) for s in all_securities]
    
    # Method 1: Scale expected frequencies to match observed total
    # This maintains the pre-period distribution but scales to post-period total
    post_total = sum(post_counts)
    pre_total = sum(pre_counts)
    expected_counts = [count * post_total / pre_total for count in pre_counts]
    
    chi2, p_chi = stats.chisquare(post_counts, expected_counts)
    results['Security_Distribution'] = {
        'chi2_statistic': chi2,
        'p_value': p_chi
    }
    
    # Alternative Method 2: Use contingency table approach
    # This tests independence between period and security type
    contingency_table = pd.crosstab(
        df.loc[df['Date'].isin(pd.concat([pre_data['Date'], post_data['Date']])), 'Period'],
        df.loc[df['Date'].isin(pd.concat([pre_data['Date'], post_data['Date']])), 'Security_Type']
    )
    chi2_alt, p_chi_alt, dof, expected = stats.chi2_contingency(contingency_table)
    results['Security_Distribution_Alt'] = {
        'chi2_statistic': chi2_alt,
        'p_value': p_chi_alt,
        'degrees_of_freedom': dof
    }
    
    # 3. Wilcoxon signed-rank test for paired customer data
    customer_changes = []
    for customer in df['Customer_Name'].unique():
        pre_cust = pre_data[pre_data['Customer_Name'] == customer]['Amount'].sum()
        post_cust = post_data[post_data['Customer_Name'] == customer]['Amount'].sum()
        if pre_cust > 0 and post_cust > 0:
            customer_changes.append(post_cust - pre_cust)
    
    if len(customer_changes) > 20:  # Need sufficient sample
        wilcoxon_stat, wilcoxon_p = stats.wilcoxon(customer_changes)
        results['Customer_Behavior'] = {
            'wilcoxon_statistic': wilcoxon_stat,
            'p_value': wilcoxon_p,
            'n_customers': len(customer_changes)
        }
    
    return results

# ==================== VISUALIZATION FUNCTIONS ====================

def create_event_study_plot(event_metrics, event_name):
    """Create event study visualization"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'Event Study: {event_name}', fontsize=16)
    
    # Plot 1: Daily Trading Volume
    ax1 = axes[0, 0]
    ax1.plot(event_metrics['Days_from_event'], event_metrics['Total_Amount']/1e6, 
             marker='o', markersize=4)
    ax1.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax1.set_xlabel('Days from Event')
    ax1.set_ylabel('Trading Volume (Million $)')
    ax1.set_title('Daily Trading Volume')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Cumulative Abnormal Returns
    ax2 = axes[0, 1]
    ax2.plot(event_metrics['Days_from_event'], event_metrics['CAR']/1e6, 
             marker='o', markersize=4, color='green')
    ax2.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax2.set_xlabel('Days from Event')
    ax2.set_ylabel('Cumulative Abnormal Trading (Million $)')
    ax2.set_title('Cumulative Abnormal Trading')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Trade Count
    ax3 = axes[1, 0]
    ax3.bar(event_metrics['Days_from_event'], event_metrics['Trade_Count'], 
            color='skyblue', alpha=0.7)
    ax3.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax3.set_xlabel('Days from Event')
    ax3.set_ylabel('Number of Trades')
    ax3.set_title('Daily Trade Count')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Net Position
    ax4 = axes[1, 1]
    colors = ['green' if x > 0 else 'red' for x in event_metrics['Net_Position']]
    ax4.bar(event_metrics['Days_from_event'], event_metrics['Net_Position']/1e6, 
            color=colors, alpha=0.7)
    ax4.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax4.set_xlabel('Days from Event')
    ax4.set_ylabel('Net Position (Million $)')
    ax4.set_title('Daily Net Position (Buy - Sell)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_portfolio_shift_visualization(shift_df):
    """Create portfolio composition shift visualization"""
    fig = make_subplots(rows=1, cols=2, 
                        subplot_titles=('Portfolio Composition Change', 'Absolute % Change'),
                        specs=[[{'type': 'bar'}, {'type': 'bar'}]])
    
    # Grouped bar chart
    fig.add_trace(
        go.Bar(name='Pre-Period', 
               x=shift_df.index, 
               y=shift_df['Pre_Period_%'],
               marker_color='lightblue'),
        row=1, col=1
    )
    
    fig.add_trace(
        go.Bar(name='Post-Period', 
               x=shift_df.index, 
               y=shift_df['Post_Period_%'],
               marker_color='darkblue'),
        row=1, col=1
    )
    
    # Change bar chart
    colors = ['red' if x < 0 else 'green' for x in shift_df['Change_%']]
    fig.add_trace(
        go.Bar(x=shift_df.index, 
               y=shift_df['Change_%'],
               marker_color=colors,
               showlegend=False),
        row=1, col=2
    )
    
    fig.update_yaxes(title_text="Percentage (%)", row=1, col=1)
    fig.update_yaxes(title_text="Change (% points)", row=1, col=2)
    fig.update_xaxes(title_text="Security Type", row=1, col=1)
    fig.update_xaxes(title_text="Security Type", row=1, col=2)
    
    fig.update_layout(height=500, title_text="Portfolio Composition Shift Analysis")
    return fig

def create_customer_clustering_plot(customer_df):
    """Create customer clustering visualization"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Scatter plot of clusters
    colors = ['blue', 'red', 'green', 'orange']
    for i, cluster in enumerate(customer_df['Cluster_Name'].unique()):
        cluster_data = customer_df[customer_df['Cluster_Name'] == cluster]
        ax1.scatter(cluster_data['Foreign_Change_%'], 
                   cluster_data['Volume_Change_%'],
                   label=cluster, color=colors[i], alpha=0.6)
    
    ax1.set_xlabel('Foreign Stock % Change')
    ax1.set_ylabel('Trading Volume % Change')
    ax1.set_title('Customer Behavioral Clusters')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)
    
    # Cluster distribution
    cluster_counts = customer_df['Cluster_Name'].value_counts()
    ax2.pie(cluster_counts.values, labels=cluster_counts.index, autopct='%1.1f%%',
            colors=colors[:len(cluster_counts)])
    ax2.set_title('Distribution of Customer Segments')
    
    plt.tight_layout()
    return fig

def create_industry_heatmap(df, event_date, top_n=10):
    """Create industry impact heatmap for top industries and security types"""
    event_date = pd.Timestamp(event_date)
    
    # Calculate change by industry and security type
    pre_data = df[df['Date'] < event_date]
    post_data = df[df['Date'] >= event_date]
    
    # Get top industries by total trading volume
    industry_volume = df.groupby('Customer_Industry')['Amount'].sum().sort_values(ascending=False)
    top_industries = industry_volume.head(top_n).index.tolist()
    
    # Get top security types by total trading volume
    security_volume = df.groupby('Security_Type')['Amount'].sum().sort_values(ascending=False)
    top_securities = security_volume.head(top_n).index.tolist()
    
    # Filter data for top industries and securities
    pre_filtered = pre_data[
        (pre_data['Customer_Industry'].isin(top_industries)) & 
        (pre_data['Security_Type'].isin(top_securities))
    ]
    post_filtered = post_data[
        (post_data['Customer_Industry'].isin(top_industries)) & 
        (post_data['Security_Type'].isin(top_securities))
    ]
    
    # Create pivot tables
    pre_pivot = pre_filtered.pivot_table(values='Amount', 
                                         index='Customer_Industry', 
                                         columns='Security_Type', 
                                         aggfunc='sum', fill_value=0)
    
    post_pivot = post_filtered.pivot_table(values='Amount', 
                                          index='Customer_Industry', 
                                          columns='Security_Type', 
                                          aggfunc='sum', fill_value=0)
    
    # Ensure same shape
    all_industries = sorted(list(set(pre_pivot.index) | set(post_pivot.index)))
    all_securities = sorted(list(set(pre_pivot.columns) | set(post_pivot.columns)))
    
    pre_pivot = pre_pivot.reindex(index=all_industries, columns=all_securities, fill_value=0)
    post_pivot = post_pivot.reindex(index=all_industries, columns=all_securities, fill_value=0)
    
    # Calculate percentage change
    change_pivot = ((post_pivot - pre_pivot) / pre_pivot * 100).replace([np.inf, -np.inf], 0).fillna(0).round(1)
    
    # Create heatmap
    plt.figure(figsize=(12, 8))
    sns.heatmap(change_pivot, annot=True, fmt='.1f', cmap='RdYlGn', center=0,
                cbar_kws={'label': '% Change'}, vmin=-100, vmax=100)
    plt.title(f'Top {top_n} Industries & Security Types - Trading Change (%)')
    plt.xlabel('Security Type')
    plt.ylabel('Industry')
    plt.tight_layout()
    return plt.gcf()

# ==================== MAIN ANALYSIS ====================

# Choose which event to analyze
ANALYZE_EVENT = 'trump'  # Change to 'april' to analyze April 2nd event

if ANALYZE_EVENT == 'trump':
    event_date = trump_date
    event_name = "Trump Inauguration (Jan 20, 2025)"
    pre_period_name = 'Pre-Event'
    post_period_name = 'Post-Event'
else:  # april
    event_date = apr_event
    event_name = "Liberation Day Tariff (Apr 2, 2025)"
    pre_period_name = 'Pre-Event'
    post_period_name = 'Post-Event'

# Update periods for the selected event
df['Event_Period'] = pre_period_name
df.loc[df['Date'] >= event_date, 'Event_Period'] = post_period_name

print("=" * 60)
print(f"BANKING TRADE ANALYSIS - {event_name}")
print("=" * 60)

# 1. Event Study Analysis
print("\n1. EVENT STUDY ANALYSIS")
print("-" * 40)

# Event study for selected event
event_study_result = perform_event_study(df, event_date, window_pre=30, window_post=40)
event_fig = create_event_study_plot(event_study_result, event_name)
plt.savefig(f'{ANALYZE_EVENT}_event_study.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"{event_name} - Trading volume change: {event_study_result['Abnormal_Amount'].sum()/1e6:.2f}M")
print(f"Maximum daily impact: {event_study_result['Abnormal_Amount'].max()/1e6:.2f}M")

# 2. Portfolio Shift Analysis
print("\n2. PORTFOLIO COMPOSITION SHIFT")
print("-" * 40)

portfolio_shift = portfolio_shift_analysis(df, event_date)
print("\nSecurity Type Distribution Changes:")
print(portfolio_shift)

shift_fig = create_portfolio_shift_visualization(portfolio_shift)
shift_fig.write_html(f'{ANALYZE_EVENT}_portfolio_shift.html')

# 3. Statistical Significance Tests
print("\n3. STATISTICAL SIGNIFICANCE TESTS")
print("-" * 40)

stat_results = statistical_significance_tests(df, event_date)

print("\nVolume Change Test:")
print(f"  Pre-period mean: ${stat_results['Volume_Change']['pre_mean']/1e6:.2f}M")
print(f"  Post-period mean: ${stat_results['Volume_Change']['post_mean']/1e6:.2f}M")
print(f"  Change: {stat_results['Volume_Change']['percent_change']:.1f}%")
print(f"  T-statistic: {stat_results['Volume_Change']['t_statistic']:.3f}")
print(f"  P-value: {stat_results['Volume_Change']['p_value']:.4f}")

print("\nSecurity Distribution Test:")
print(f"  Chi-square statistic: {stat_results['Security_Distribution']['chi2_statistic']:.3f}")
print(f"  P-value: {stat_results['Security_Distribution']['p_value']:.4f}")

if 'Security_Distribution_Alt' in stat_results:
    print("\nSecurity Distribution Test (Contingency Table):")
    print(f"  Chi-square statistic: {stat_results['Security_Distribution_Alt']['chi2_statistic']:.3f}")
    print(f"  P-value: {stat_results['Security_Distribution_Alt']['p_value']:.4f}")
    print(f"  Degrees of freedom: {stat_results['Security_Distribution_Alt']['degrees_of_freedom']}")

if 'Customer_Behavior' in stat_results:
    print("\nCustomer Behavior Test (Wilcoxon):")
    print(f"  Number of customers: {stat_results['Customer_Behavior']['n_customers']}")
    print(f"  Wilcoxon statistic: {stat_results['Customer_Behavior']['wilcoxon_statistic']:.3f}")
    print(f"  P-value: {stat_results['Customer_Behavior']['p_value']:.4f}")

# 4. Customer Clustering Analysis
print("\n4. CUSTOMER SEGMENTATION ANALYSIS")
print("-" * 40)

customer_clusters = customer_behavior_clustering(df, event_date)
cluster_summary = customer_clusters.groupby('Cluster_Name').agg({
    'Foreign_Change_%': 'mean',
    'Volume_Change_%': 'mean',
    'Customer': 'count'
}).round(2)

print("\nCustomer Segment Characteristics:")
print(cluster_summary)

cluster_fig = create_customer_clustering_plot(customer_clusters)
plt.savefig(f'{ANALYZE_EVENT}_customer_clusters.png', dpi=300, bbox_inches='tight')
plt.close()

# 5. Industry Impact Analysis - Fixed for flexible periods
print("\n5. INDUSTRY IMPACT ANALYSIS")
print("-" * 40)

industry_fig = create_industry_heatmap(df, event_date)
plt.savefig(f'{ANALYZE_EVENT}_industry_heatmap.png', dpi=300, bbox_inches='tight')
plt.close()

# Industry ranking by impact with customer count filter - Fixed
industry_customers = df.groupby('Customer_Industry')['Customer_Name'].nunique()
industry_impact = df[df['Event_Period'].isin([pre_period_name, post_period_name])].groupby(['Customer_Industry', 'Event_Period'])['Amount'].sum().unstack(fill_value=0)

# Add customer count
industry_impact['Customer_Count'] = industry_customers
industry_impact['Change_%'] = 0  # Initialize

# Calculate change only if both periods exist
if pre_period_name in industry_impact.columns and post_period_name in industry_impact.columns:
    mask = industry_impact[pre_period_name] > 0
    industry_impact.loc[mask, 'Change_%'] = (
        (industry_impact.loc[mask, post_period_name] - industry_impact.loc[mask, pre_period_name]) / 
        industry_impact.loc[mask, pre_period_name] * 100
    ).round(2)

# Filter industries with at least 10 customers
significant_industries = industry_impact[industry_impact['Customer_Count'] >= 10]
industry_ranking = significant_industries.sort_values('Change_%', ascending=False)

print("\nIndustry Impact Ranking (Industries with 10+ customers):")
print("\nTop 10 Increasers:")
print(industry_ranking[['Change_%', 'Customer_Count']].head(10))
print("\nTop 10 Decreasers:")
print(industry_ranking[['Change_%', 'Customer_Count']].tail(10))

# 6. Time Series Decomposition
print("\n6. TIME SERIES PATTERNS")
print("-" * 40)

# Daily aggregation
daily_metrics = df.groupby('Date').agg({
    'Amount': 'sum',
    'Net_Amount': 'sum'
}).reset_index()

# Remove outliers using IQR method
Q1 = daily_metrics['Amount'].quantile(0.25)
Q3 = daily_metrics['Amount'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Mark outliers but don't remove them completely - cap them
daily_metrics['Amount_cleaned'] = daily_metrics['Amount'].clip(lower=lower_bound, upper=upper_bound)
daily_metrics['Net_Amount_cleaned'] = daily_metrics['Net_Amount'].clip(
    lower=daily_metrics['Net_Amount'].quantile(0.05),
    upper=daily_metrics['Net_Amount'].quantile(0.95)
)

# Calculate rolling averages on cleaned data
daily_metrics['MA_7'] = daily_metrics['Amount_cleaned'].rolling(window=7, center=True).mean()
daily_metrics['MA_30'] = daily_metrics['Amount_cleaned'].rolling(window=30, center=True).mean()

# Plot time series with events marked
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)

# Trading volume
ax1.plot(daily_metrics['Date'], daily_metrics['Amount_cleaned']/1e6, alpha=0.3, label='Daily (outliers capped)', linewidth=0.5)
ax1.plot(daily_metrics['Date'], daily_metrics['MA_7']/1e6, label='7-day MA', linewidth=2)
ax1.plot(daily_metrics['Date'], daily_metrics['MA_30']/1e6, label='30-day MA', linewidth=2)
ax1.axvline(x=trump_date, color='red', linestyle='--', label='Trump Inauguration', linewidth=2)
ax1.axvline(x=apr_event, color='orange', linestyle='--', label='April Event', linewidth=2)
ax1.set_ylabel('Trading Volume (Million $)')
ax1.set_title('Trading Volume Time Series (Outliers Capped at 1.5 IQR)')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Net position
ax2.plot(daily_metrics['Date'], daily_metrics['Net_Amount_cleaned']/1e6, alpha=0.5, linewidth=1)
ax2.axvline(x=trump_date, color='red', linestyle='--', linewidth=2)
ax2.axvline(x=apr_event, color='orange', linestyle='--', linewidth=2)
ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
ax2.set_xlabel('Date')
ax2.set_ylabel('Net Position (Million $)')
ax2.set_title('Daily Net Trading Position (Buy - Sell) - 5th to 95th Percentile')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('time_series_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

# Report outliers found
n_outliers = len(daily_metrics[(daily_metrics['Amount'] < lower_bound) | (daily_metrics['Amount'] > upper_bound)])
print(f"\nOutliers detected and capped: {n_outliers} days out of {len(daily_metrics)}")

# 7. Recommendation Matrix - Fixed version with event date flexibility
print("\n7. ACTIONABLE INSIGHTS FOR RMs")
print("-" * 40)

# Calculate proper foreign/local exposure changes
customer_exposure = []
for customer in df['Customer_Name'].unique():
    cust_data = df[df['Customer_Name'] == customer]
    pre_data = cust_data[cust_data['Date'] < event_date]
    post_data = cust_data[cust_data['Date'] >= event_date]
    
    if len(pre_data) > 0 and len(post_data) > 0:
        # Foreign exposure
        pre_foreign = pre_data[pre_data['Security_Type'].str.contains('Foreign', case=False, na=False)]['Amount'].sum()
        post_foreign = post_data[post_data['Security_Type'].str.contains('Foreign', case=False, na=False)]['Amount'].sum()
        
        # Local (HK) exposure
        pre_local = pre_data[pre_data['Security_Type'].str.contains('Local|HK|Hong Kong', case=False, na=False)]['Amount'].sum()
        post_local = post_data[post_data['Security_Type'].str.contains('Local|HK|Hong Kong', case=False, na=False)]['Amount'].sum()
        
        pre_total = pre_data['Amount'].sum()
        post_total = post_data['Amount'].sum()
        
        if pre_total > 0 and post_total > 0:
            customer_exposure.append({
                'Customer': customer,
                'Foreign_Exposure_Change': (post_foreign/post_total - pre_foreign/pre_total) * 100,
                'Local_Exposure_Change': (post_local/post_total - pre_local/pre_total) * 100,
                'Volume_Change_%': ((post_total - pre_total) / pre_total * 100),
                'Pre_Total': pre_total,
                'Post_Total': post_total
            })

exposure_df = pd.DataFrame(customer_exposure)

# Identify different customer segments
foreign_reducers = exposure_df[exposure_df['Foreign_Exposure_Change'] < -10]
local_reducers = exposure_df[exposure_df['Local_Exposure_Change'] < -10]
volume_increasers = exposure_df[exposure_df['Volume_Change_%'] > 50]

print(f"\nCustomers who reduced foreign exposure by >10%: {len(foreign_reducers)}")
print(f"Customers who reduced local (HK) exposure by >10%: {len(local_reducers)}")
print(f"Customers who increased trading volume by >50%: {len(volume_increasers)}")

# High opportunity customers - those who changed behavior significantly
opportunity_customers = exposure_df[
    (abs(exposure_df['Foreign_Exposure_Change']) > 10) | 
    (abs(exposure_df['Local_Exposure_Change']) > 10) |
    (abs(exposure_df['Volume_Change_%']) > 50)
].sort_values('Post_Total', ascending=False).head(20)

print(f"\nHigh-Priority Customers for RM Outreach: {len(opportunity_customers)}")

# Create final summary statistics with corrected calculations
summary_stats = {
    'Total Customers Analyzed': len(exposure_df),
    'Customers with Reduced Foreign Exposure (>10%)': len(foreign_reducers),
    'Customers with Reduced Local Exposure (>10%)': len(local_reducers),
    'Customers with Increased Trading Volume (>50%)': len(volume_increasers),
    'Average Foreign Stock Change': f"{exposure_df['Foreign_Exposure_Change'].mean():.1f}%",
    'Average Local Stock Change': f"{exposure_df['Local_Exposure_Change'].mean():.1f}%",
    'Average Volume Change': f"{exposure_df['Volume_Change_%'].median():.1f}% (median)",
    'Statistical Significance (Distribution)': 'Yes' if stat_results['Security_Distribution']['p_value'] < 0.05 else 'No'
}

print("\n" + "=" * 60)
print("EXECUTIVE SUMMARY")
print("=" * 60)
for key, value in summary_stats.items():
    print(f"{key}: {value}")

print("\n" + "=" * 60)
print(f"Analysis complete for {event_name}. Generated files:")
print(f"- {ANALYZE_EVENT}_event_study.png")
print(f"- {ANALYZE_EVENT}_portfolio_shift.html")
print(f"- {ANALYZE_EVENT}_customer_clusters.png")
print(f"- {ANALYZE_EVENT}_industry_heatmap.png")
print("- time_series_analysis.png")
print("=" * 60)
