import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
from scipy import stats
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import plotly.graph_objects as go
from plotly.subplots import make_subplots

warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ==================== DATA GENERATION ====================
# Simulating realistic banking trade data
np.random.seed(42)

# Generate date range
start_date = '2023-01-01'
end_date = '2025-05-29'
dates = pd.date_range(start=start_date, end=end_date, freq='B')  # Business days only

# Key events
trump_date = pd.Timestamp('2025-01-20')
apr_event = pd.Timestamp('2025-04-02')

# Customer and security setup
n_customers = 500
customers = [f'Customer_{i:03d}' for i in range(n_customers)]
industries = ['Technology', 'Manufacturing', 'Finance', 'Retail', 'Healthcare', 
              'Energy', 'Real Estate', 'Consumer Goods']
security_types = ['Foreign Stocks', 'Local Stocks', 'Bonds', 'Certificate of Deposit', 
                  'Unit Trusts', 'ETFs']

# Generate trades
trades = []
for date in dates:
    # Adjust trading patterns based on events
    if date >= trump_date:
        foreign_bias = 0.3  # Reduced foreign investment
        volume_multiplier = 1.2  # Increased trading volume
    else:
        foreign_bias = 0.5
        volume_multiplier = 1.0
    
    # Daily trades (varying by day)
    n_trades = np.random.poisson(200 * volume_multiplier)
    
    for _ in range(n_trades):
        customer = np.random.choice(customers)
        industry = np.random.choice(industries)
        
        # Bias security selection based on period
        if date >= trump_date:
            security_probs = [foreign_bias, 1-foreign_bias, 0.1, 0.05, 0.05, 0.05]
        else:
            security_probs = [0.4, 0.4, 0.1, 0.05, 0.03, 0.02]
        
        security_probs = np.array(security_probs) / sum(security_probs)
        security_type = np.random.choice(security_types, p=security_probs)
        
        # Generate trade details
        investment = np.random.choice(['Buy', 'Sell'], p=[0.55, 0.45])
        amount = np.random.lognormal(10, 1.5) * 1000  # Log-normal distribution for amounts
        
        trades.append({
            'Date': date,
            'Customer_Name': customer,
            'Customer_Industry': industry,
            'Investment': investment,
            'Amount': amount,
            'Security_Type': security_type
        })

# Create DataFrame
df = pd.DataFrame(trades)
print(f"Generated {len(df)} trades from {df['Date'].min()} to {df['Date'].max()}")

# ==================== DATA PREPROCESSING ====================

# Add derived features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Week'] = df['Date'].dt.isocalendar().week
df['DayOfWeek'] = df['Date'].dt.dayofweek

# Define periods
df['Period'] = 'Pre-Trump'
df.loc[df['Date'] >= trump_date, 'Period'] = 'Post-Trump'
df.loc[df['Date'] >= apr_event, 'Period'] = 'Post-April'

# Create net position (Buy - Sell)
df['Net_Amount'] = df.apply(lambda x: x['Amount'] if x['Investment'] == 'Buy' else -x['Amount'], axis=1)

# ==================== ANALYSIS FUNCTIONS ====================

def calculate_trading_metrics(data, groupby_cols=['Date']):
    """Calculate key trading metrics"""
    metrics = data.groupby(groupby_cols).agg({
        'Amount': ['sum', 'mean', 'count'],
        'Net_Amount': 'sum'
    }).reset_index()
    
    metrics.columns = ['_'.join(col).strip() for col in metrics.columns.values]
    metrics.columns = [col.replace('_', '') if col.endswith('_') else col for col in metrics.columns]
    
    return metrics

def perform_event_study(df, event_date, window_pre=30, window_post=30):
    """Perform event study analysis"""
    event_date = pd.Timestamp(event_date)
    
    # Define windows
    pre_start = event_date - timedelta(days=window_pre)
    post_end = event_date + timedelta(days=window_post)
    
    # Filter data
    event_data = df[(df['Date'] >= pre_start) & (df['Date'] <= post_end)].copy()
    event_data['Days_from_event'] = (event_data['Date'] - event_date).dt.days
    
    # Calculate daily metrics
    daily_metrics = event_data.groupby('Days_from_event').agg({
        'Amount': ['sum', 'count'],
        'Net_Amount': 'sum'
    }).reset_index()
    
    daily_metrics.columns = ['Days_from_event', 'Total_Amount', 'Trade_Count', 'Net_Position']
    
    # Calculate abnormal trading (using pre-event average as baseline)
    pre_event = daily_metrics[daily_metrics['Days_from_event'] < 0]
    baseline_amount = pre_event['Total_Amount'].mean()
    baseline_count = pre_event['Trade_Count'].mean()
    
    daily_metrics['Abnormal_Amount'] = daily_metrics['Total_Amount'] - baseline_amount
    daily_metrics['Abnormal_Count'] = daily_metrics['Trade_Count'] - baseline_count
    
    # Cumulative abnormal
    daily_metrics['CAR'] = daily_metrics['Abnormal_Amount'].cumsum()
    
    return daily_metrics

def portfolio_shift_analysis(df, date_cutoff):
    """Analyze portfolio composition shifts"""
    pre_data = df[df['Date'] < date_cutoff]
    post_data = df[df['Date'] >= date_cutoff]
    
    # Calculate security type distribution
    pre_dist = pre_data.groupby('Security_Type')['Amount'].sum()
    post_dist = post_data.groupby('Security_Type')['Amount'].sum()
    
    pre_pct = (pre_dist / pre_dist.sum() * 100).round(2)
    post_pct = (post_dist / post_dist.sum() * 100).round(2)
    
    shift_df = pd.DataFrame({
        'Pre_Period_%': pre_pct,
        'Post_Period_%': post_pct,
        'Change_%': post_pct - pre_pct
    })
    
    return shift_df

def customer_behavior_clustering(df, event_date):
    """Cluster customers based on behavioral changes"""
    event_date = pd.Timestamp(event_date)
    
    # Calculate customer metrics pre/post
    customer_metrics = []
    
    for customer in df['Customer_Name'].unique():
        cust_data = df[df['Customer_Name'] == customer]
        
        pre_data = cust_data[cust_data['Date'] < event_date]
        post_data = cust_data[cust_data['Date'] >= event_date]
        
        if len(pre_data) > 0 and len(post_data) > 0:
            # Calculate metrics
            pre_foreign = pre_data[pre_data['Security_Type'] == 'Foreign Stocks']['Amount'].sum()
            post_foreign = post_data[post_data['Security_Type'] == 'Foreign Stocks']['Amount'].sum()
            
            pre_total = pre_data['Amount'].sum()
            post_total = post_data['Amount'].sum()
            
            pre_foreign_pct = (pre_foreign / pre_total * 100) if pre_total > 0 else 0
            post_foreign_pct = (post_foreign / post_total * 100) if post_total > 0 else 0
            
            customer_metrics.append({
                'Customer': customer,
                'Pre_Foreign_%': pre_foreign_pct,
                'Post_Foreign_%': post_foreign_pct,
                'Foreign_Change_%': post_foreign_pct - pre_foreign_pct,
                'Volume_Change_%': ((post_total - pre_total) / pre_total * 100) if pre_total > 0 else 0,
                'Pre_Trade_Count': len(pre_data),
                'Post_Trade_Count': len(post_data)
            })
    
    customer_df = pd.DataFrame(customer_metrics)
    
    # Perform clustering
    features = ['Foreign_Change_%', 'Volume_Change_%']
    X = customer_df[features].fillna(0)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    kmeans = KMeans(n_clusters=4, random_state=42)
    customer_df['Cluster'] = kmeans.fit_predict(X_scaled)
    
    # Name clusters based on behavior
    cluster_names = {
        0: 'Stable Investors',
        1: 'Foreign Reducers',
        2: 'Volume Increasers',
        3: 'Portfolio Rebalancers'
    }
    customer_df['Cluster_Name'] = customer_df['Cluster'].map(cluster_names)
    
    return customer_df

def statistical_significance_tests(df, event_date, window=30):
    """Perform statistical tests for significance"""
    event_date = pd.Timestamp(event_date)
    
    # Define periods
    pre_start = event_date - timedelta(days=window)
    pre_end = event_date - timedelta(days=1)
    post_start = event_date
    post_end = event_date + timedelta(days=window)
    
    pre_data = df[(df['Date'] >= pre_start) & (df['Date'] <= pre_end)]
    post_data = df[(df['Date'] >= post_start) & (df['Date'] <= post_end)]
    
    results = {}
    
    # 1. T-test for average daily trading volume
    pre_daily = pre_data.groupby('Date')['Amount'].sum()
    post_daily = post_data.groupby('Date')['Amount'].sum()
    
    t_stat, p_value = stats.ttest_ind(pre_daily, post_daily)
    results['Volume_Change'] = {
        't_statistic': t_stat,
        'p_value': p_value,
        'pre_mean': pre_daily.mean(),
        'post_mean': post_daily.mean(),
        'percent_change': ((post_daily.mean() - pre_daily.mean()) / pre_daily.mean() * 100)
    }
    
    # 2. Chi-square test for security type distribution
    pre_security = pre_data['Security_Type'].value_counts()
    post_security = post_data['Security_Type'].value_counts()
    
    # Align indices
    all_securities = list(set(pre_security.index) | set(post_security.index))
    pre_counts = [pre_security.get(s, 0) for s in all_securities]
    post_counts = [post_security.get(s, 0) for s in all_securities]
    
    chi2, p_chi = stats.chisquare(post_counts, pre_counts)
    results['Security_Distribution'] = {
        'chi2_statistic': chi2,
        'p_value': p_chi
    }
    
    # 3. Wilcoxon signed-rank test for paired customer data
    customer_changes = []
    for customer in df['Customer_Name'].unique():
        pre_cust = pre_data[pre_data['Customer_Name'] == customer]['Amount'].sum()
        post_cust = post_data[post_data['Customer_Name'] == customer]['Amount'].sum()
        if pre_cust > 0 and post_cust > 0:
            customer_changes.append(post_cust - pre_cust)
    
    if len(customer_changes) > 20:  # Need sufficient sample
        wilcoxon_stat, wilcoxon_p = stats.wilcoxon(customer_changes)
        results['Customer_Behavior'] = {
            'wilcoxon_statistic': wilcoxon_stat,
            'p_value': wilcoxon_p,
            'n_customers': len(customer_changes)
        }
    
    return results

# ==================== VISUALIZATION FUNCTIONS ====================

def create_event_study_plot(event_metrics, event_name):
    """Create event study visualization"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle(f'Event Study: {event_name}', fontsize=16)
    
    # Plot 1: Daily Trading Volume
    ax1 = axes[0, 0]
    ax1.plot(event_metrics['Days_from_event'], event_metrics['Total_Amount']/1e6, 
             marker='o', markersize=4)
    ax1.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax1.set_xlabel('Days from Event')
    ax1.set_ylabel('Trading Volume (Million $)')
    ax1.set_title('Daily Trading Volume')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Plot 2: Cumulative Abnormal Returns
    ax2 = axes[0, 1]
    ax2.plot(event_metrics['Days_from_event'], event_metrics['CAR']/1e6, 
             marker='o', markersize=4, color='green')
    ax2.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax2.set_xlabel('Days from Event')
    ax2.set_ylabel('Cumulative Abnormal Trading (Million $)')
    ax2.set_title('Cumulative Abnormal Trading')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Plot 3: Trade Count
    ax3 = axes[1, 0]
    ax3.bar(event_metrics['Days_from_event'], event_metrics['Trade_Count'], 
            color='skyblue', alpha=0.7)
    ax3.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax3.set_xlabel('Days from Event')
    ax3.set_ylabel('Number of Trades')
    ax3.set_title('Daily Trade Count')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # Plot 4: Net Position
    ax4 = axes[1, 1]
    colors = ['green' if x > 0 else 'red' for x in event_metrics['Net_Position']]
    ax4.bar(event_metrics['Days_from_event'], event_metrics['Net_Position']/1e6, 
            color=colors, alpha=0.7)
    ax4.axvline(x=0, color='red', linestyle='--', label='Event Date')
    ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax4.set_xlabel('Days from Event')
    ax4.set_ylabel('Net Position (Million $)')
    ax4.set_title('Daily Net Position (Buy - Sell)')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    return fig

def create_portfolio_shift_visualization(shift_df):
    """Create portfolio composition shift visualization"""
    fig = make_subplots(rows=1, cols=2, 
                        subplot_titles=('Portfolio Composition Change', 'Absolute % Change'),
                        specs=[[{'type': 'bar'}, {'type': 'bar'}]])
    
    # Grouped bar chart
    fig.add_trace(
        go.Bar(name='Pre-Period', 
               x=shift_df.index, 
               y=shift_df['Pre_Period_%'],
               marker_color='lightblue'),
        row=1, col=1
    )
    
    fig.add_trace(
        go.Bar(name='Post-Period', 
               x=shift_df.index, 
               y=shift_df['Post_Period_%'],
               marker_color='darkblue'),
        row=1, col=1
    )
    
    # Change bar chart
    colors = ['red' if x < 0 else 'green' for x in shift_df['Change_%']]
    fig.add_trace(
        go.Bar(x=shift_df.index, 
               y=shift_df['Change_%'],
               marker_color=colors,
               showlegend=False),
        row=1, col=2
    )
    
    fig.update_yaxes(title_text="Percentage (%)", row=1, col=1)
    fig.update_yaxes(title_text="Change (% points)", row=1, col=2)
    fig.update_xaxes(title_text="Security Type", row=1, col=1)
    fig.update_xaxes(title_text="Security Type", row=1, col=2)
    
    fig.update_layout(height=500, title_text="Portfolio Composition Shift Analysis")
    return fig

def create_customer_clustering_plot(customer_df):
    """Create customer clustering visualization"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Scatter plot of clusters
    colors = ['blue', 'red', 'green', 'orange']
    for i, cluster in enumerate(customer_df['Cluster_Name'].unique()):
        cluster_data = customer_df[customer_df['Cluster_Name'] == cluster]
        ax1.scatter(cluster_data['Foreign_Change_%'], 
                   cluster_data['Volume_Change_%'],
                   label=cluster, color=colors[i], alpha=0.6)
    
    ax1.set_xlabel('Foreign Stock % Change')
    ax1.set_ylabel('Trading Volume % Change')
    ax1.set_title('Customer Behavioral Clusters')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)
    
    # Cluster distribution
    cluster_counts = customer_df['Cluster_Name'].value_counts()
    ax2.pie(cluster_counts.values, labels=cluster_counts.index, autopct='%1.1f%%',
            colors=colors[:len(cluster_counts)])
    ax2.set_title('Distribution of Customer Segments')
    
    plt.tight_layout()
    return fig

def create_industry_heatmap(df, event_date):
    """Create industry impact heatmap"""
    event_date = pd.Timestamp(event_date)
    
    # Calculate change by industry and security type
    pre_data = df[df['Date'] < event_date]
    post_data = df[df['Date'] >= event_date]
    
    # Create pivot tables
    pre_pivot = pre_data.pivot_table(values='Amount', 
                                     index='Customer_Industry', 
                                     columns='Security_Type', 
                                     aggfunc='sum', fill_value=0)
    
    post_pivot = post_data.pivot_table(values='Amount', 
                                      index='Customer_Industry', 
                                      columns='Security_Type', 
                                      aggfunc='sum', fill_value=0)
    
    # Calculate percentage change
    change_pivot = ((post_pivot - pre_pivot) / pre_pivot * 100).round(1)
    
    # Create heatmap
    plt.figure(figsize=(12, 8))
    sns.heatmap(change_pivot, annot=True, fmt='.1f', cmap='RdYlGn', center=0,
                cbar_kws={'label': '% Change'})
    plt.title('Industry-wise Security Type Trading Change (%)')
    plt.xlabel('Security Type')
    plt.ylabel('Industry')
    plt.tight_layout()
    return plt.gcf()

# ==================== MAIN ANALYSIS ====================

print("=" * 60)
print("BANKING TRADE ANALYSIS - TARIFF IMPACT STUDY")
print("=" * 60)

# 1. Event Study Analysis
print("\n1. EVENT STUDY ANALYSIS")
print("-" * 40)

# Trump inauguration event study
trump_event = perform_event_study(df, trump_date, window_pre=30, window_post=40)
trump_fig = create_event_study_plot(trump_event, "Trump Inauguration (Jan 20, 2025)")
plt.savefig('trump_event_study.png', dpi=300, bbox_inches='tight')
plt.close()

print(f"Trump Event - Trading volume change: {trump_event['Abnormal_Amount'].sum()/1e6:.2f}M")
print(f"Maximum daily impact: {trump_event['Abnormal_Amount'].max()/1e6:.2f}M")

# 2. Portfolio Shift Analysis
print("\n2. PORTFOLIO COMPOSITION SHIFT")
print("-" * 40)

portfolio_shift = portfolio_shift_analysis(df, trump_date)
print("\nSecurity Type Distribution Changes:")
print(portfolio_shift)

shift_fig = create_portfolio_shift_visualization(portfolio_shift)
shift_fig.write_html('portfolio_shift.html')

# 3. Statistical Significance Tests
print("\n3. STATISTICAL SIGNIFICANCE TESTS")
print("-" * 40)

stat_results = statistical_significance_tests(df, trump_date)

print("\nVolume Change Test:")
print(f"  Pre-period mean: ${stat_results['Volume_Change']['pre_mean']/1e6:.2f}M")
print(f"  Post-period mean: ${stat_results['Volume_Change']['post_mean']/1e6:.2f}M")
print(f"  Change: {stat_results['Volume_Change']['percent_change']:.1f}%")
print(f"  T-statistic: {stat_results['Volume_Change']['t_statistic']:.3f}")
print(f"  P-value: {stat_results['Volume_Change']['p_value']:.4f}")

print("\nSecurity Distribution Test:")
print(f"  Chi-square statistic: {stat_results['Security_Distribution']['chi2_statistic']:.3f}")
print(f"  P-value: {stat_results['Security_Distribution']['p_value']:.4f}")

if 'Customer_Behavior' in stat_results:
    print("\nCustomer Behavior Test (Wilcoxon):")
    print(f"  Number of customers: {stat_results['Customer_Behavior']['n_customers']}")
    print(f"  Wilcoxon statistic: {stat_results['Customer_Behavior']['wilcoxon_statistic']:.3f}")
    print(f"  P-value: {stat_results['Customer_Behavior']['p_value']:.4f}")

# 4. Customer Clustering Analysis
print("\n4. CUSTOMER SEGMENTATION ANALYSIS")
print("-" * 40)

customer_clusters = customer_behavior_clustering(df, trump_date)
cluster_summary = customer_clusters.groupby('Cluster_Name').agg({
    'Foreign_Change_%': 'mean',
    'Volume_Change_%': 'mean',
    'Customer': 'count'
}).round(2)

print("\nCustomer Segment Characteristics:")
print(cluster_summary)

cluster_fig = create_customer_clustering_plot(customer_clusters)
plt.savefig('customer_clusters.png', dpi=300, bbox_inches='tight')
plt.close()

# 5. Industry Impact Analysis
print("\n5. INDUSTRY IMPACT ANALYSIS")
print("-" * 40)

industry_fig = create_industry_heatmap(df, trump_date)
plt.savefig('industry_heatmap.png', dpi=300, bbox_inches='tight')
plt.close()

# Industry ranking by impact
industry_impact = df.groupby(['Customer_Industry', 'Period'])['Amount'].sum().unstack()
industry_impact['Change_%'] = ((industry_impact['Post-Trump'] - industry_impact['Pre-Trump']) / 
                               industry_impact['Pre-Trump'] * 100).round(2)
industry_ranking = industry_impact.sort_values('Change_%', ascending=False)

print("\nIndustry Impact Ranking:")
print(industry_ranking[['Change_%']].head(10))

# 6. Time Series Decomposition
print("\n6. TIME SERIES PATTERNS")
print("-" * 40)

# Daily aggregation
daily_metrics = df.groupby('Date').agg({
    'Amount': 'sum',
    'Net_Amount': 'sum'
}).reset_index()

# Calculate rolling averages
daily_metrics['MA_7'] = daily_metrics['Amount'].rolling(window=7).mean()
daily_metrics['MA_30'] = daily_metrics['Amount'].rolling(window=30).mean()

# Plot time series with events marked
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True)

# Trading volume
ax1.plot(daily_metrics['Date'], daily_metrics['Amount']/1e6, alpha=0.3, label='Daily')
ax1.plot(daily_metrics['Date'], daily_metrics['MA_7']/1e6, label='7-day MA')
ax1.plot(daily_metrics['Date'], daily_metrics['MA_30']/1e6, label='30-day MA')
ax1.axvline(x=trump_date, color='red', linestyle='--', label='Trump Inauguration')
ax1.axvline(x=apr_event, color='orange', linestyle='--', label='April Event')
ax1.set_ylabel('Trading Volume (Million $)')
ax1.set_title('Trading Volume Time Series')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Net position
ax2.plot(daily_metrics['Date'], daily_metrics['Net_Amount']/1e6, alpha=0.5)
ax2.axvline(x=trump_date, color='red', linestyle='--')
ax2.axvline(x=apr_event, color='orange', linestyle='--')
ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
ax2.set_xlabel('Date')
ax2.set_ylabel('Net Position (Million $)')
ax2.set_title('Daily Net Trading Position (Buy - Sell)')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('time_series_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

# 7. Recommendation Matrix
print("\n7. ACTIONABLE INSIGHTS FOR RMs")
print("-" * 40)

# Identify high-opportunity customers
opportunity_customers = customer_clusters[
    (customer_clusters['Foreign_Change_%'] < -10) & 
    (customer_clusters['Volume_Change_%'] > 20)
].sort_values('Volume_Change_%', ascending=False).head(20)

print(f"\nHigh-Priority Customers for RM Outreach: {len(opportunity_customers)}")
print("These customers significantly reduced foreign exposure and increased trading activity")

# Create final summary statistics
summary_stats = {
    'Total Customers Analyzed': len(customer_clusters),
    'Customers with Reduced Foreign Exposure': len(customer_clusters[customer_clusters['Foreign_Change_%'] < 0]),
    'Customers with Increased Trading Volume': len(customer_clusters[customer_clusters['Volume_Change_%'] > 0]),
    'Average Foreign Stock Reduction': f"{customer_clusters['Foreign_Change_%'].mean():.1f}%",
    'Average Volume Increase': f"{customer_clusters['Volume_Change_%'].mean():.1f}%",
    'Statistical Significance': 'Yes' if stat_results['Volume_Change']['p_value'] < 0.05 else 'No'
}

print("\n" + "=" * 60)
print("EXECUTIVE SUMMARY")
print("=" * 60)
for key, value in summary_stats.items():
    print(f"{key}: {value}")

print("\n" + "=" * 60)
print("Analysis complete. Generated files:")
print("- trump_event_study.png")
print("- portfolio_shift.html")
print("- customer_clusters.png")
print("- industry_heatmap.png")
print("- time_series_analysis.png")
print("=" * 60)
