import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# ========================================
# STEP 1: CREATE DUMMY DATA
# ========================================
print("=" * 80)
print("STEP 1: CREATING REALISTIC BANKING TRADE DATA")
print("=" * 80)

# Define realistic parameters
n_rows = 10000  # Using smaller sample for demonstration (scale to 4M in production)

# Customer pools
customer_names = [
    "Acme Corporation", "Global Industries Ltd", "Tech Innovations Inc", 
    "Manufacturing Giants Co", "Retail Empire Group", "Energy Solutions Corp",
    "Financial Services Ltd", "Healthcare Systems Inc", "Transport Logistics Co",
    "Real Estate Holdings", "Consumer Goods Inc", "Digital Media Group",
    "Pharmaceutical Labs", "Construction Masters", "Food & Beverage Co",
    "Textile Industries", "Mining Operations Ltd", "Telecom Networks Inc",
    "Insurance Partners", "Investment Holdings", "Export Import Co",
    "Chemical Industries", "Steel Manufacturing", "Paper Mills Ltd",
    "Automotive Parts Co", "Electronics Assembly", "Shipping Lines Ltd",
    "Agriculture Exports", "Hospitality Group", "Education Services"
]

industries = [
    "Manufacturing", "Technology", "Healthcare", "Financial Services", 
    "Retail", "Energy", "Real Estate", "Transportation", "Telecommunications",
    "Consumer Goods", "Pharmaceuticals", "Construction", "Food & Beverage",
    "Textiles", "Mining", "Insurance", "Chemicals", "Steel", "Paper",
    "Automotive", "Electronics", "Shipping", "Agriculture", "Hospitality"
]

# Counterparty banks with realistic names
counterparty_banks = [
    "HSBC", "Standard Chartered", "Bank of America", "JP Morgan Chase",
    "Citibank", "Deutsche Bank", "BNP Paribas", "Credit Suisse",
    "UBS", "Barclays", "Royal Bank of Canada", "Bank of China",
    "Industrial Commercial Bank of China", "Mitsubishi UFJ", "Sumitomo Mitsui",
    "DBS Bank", "OCBC Bank", "Commonwealth Bank", "ANZ Bank", "Wells Fargo"
]

# Counterparty addresses with intentionally inconsistent country formats
address_patterns = [
    "{street}, Hong Kong, HK",
    "{street}, Singapore, SG",
    "{street}, London, United Kingdom",
    "{street}, New York, USA",
    "{street}, Tokyo, Japan",
    "{street}, Shanghai, China CN",
    "{street}, Mumbai, India",
    "{street}, Dubai, UAE",
    "{street}, Frankfurt, Germany",
    "{street}, Paris, France",
    "{street}, Sydney, Australia",
    "{street}, Toronto, Canada",
    "{street}, Zurich, Switzerland",
    "{street}, Seoul, South Korea",
    "{street}, {company} Hong Kong Branch, HK",
    "{street}, {company} Singapore Operations",
    "{street}, Beijing CN",
    "{street}, Taiwan, ROC",
    "{street}, Malaysia, KL",
    "{street}, Thailand, Bangkok"
]

streets = [
    "1 Financial Plaza", "88 Commerce Street", "200 Business Park",
    "50 Trade Center", "15 Banking Boulevard", "99 Corporate Drive",
    "30 Exchange Place", "77 Market Street", "10 Harbor View"
]

# Generate data
print("\nGenerating trade transaction data...")

# Create date range (last 2 years)
end_date = datetime.now()
start_date = end_date - timedelta(days=730)
dates = pd.date_range(start=start_date, end=end_date, periods=n_rows)

# Generate transactions with realistic patterns
data = []
for i in range(n_rows):
    # Select customer (some customers trade more frequently)
    customer = np.random.choice(customer_names, p=np.array([0.1 if i < 5 else 0.03 for i in range(len(customer_names))]) / sum([0.1 if i < 5 else 0.03 for i in range(len(customer_names))]))
    
    # Industry based on customer
    if "Tech" in customer or "Digital" in customer:
        industry = "Technology"
    elif "Manufacturing" in customer or "Steel" in customer:
        industry = "Manufacturing"
    elif "Financial" in customer or "Investment" in customer or "Insurance" in customer:
        industry = "Financial Services"
    elif "Healthcare" in customer or "Pharmaceutical" in customer:
        industry = "Healthcare"
    elif "Energy" in customer:
        industry = "Energy"
    elif "Retail" in customer or "Consumer" in customer:
        industry = "Retail"
    else:
        industry = np.random.choice(industries)
    
    # Amount with industry-specific patterns
    if industry == "Financial Services":
        amount = np.random.lognormal(15, 1.5)  # Larger transactions
    elif industry == "Retail":
        amount = np.random.lognormal(12, 1.2)  # Smaller, frequent
    elif industry == "Energy":
        amount = np.random.lognormal(16, 1.3)  # Very large
    else:
        amount = np.random.lognormal(13.5, 1.4)  # Medium
    
    amount = min(amount, 1e9)  # Cap at 1 billion
    
    # Counterparty selection (some banks are more popular)
    bank = np.random.choice(counterparty_banks, 
                           p=np.array([0.15 if i < 3 else 0.05 for i in range(len(counterparty_banks))]) / 
                           sum([0.15 if i < 3 else 0.05 for i in range(len(counterparty_banks))]))
    
    # Address with inconsistent formats
    address_template = np.random.choice(address_patterns)
    street = np.random.choice(streets)
    address = address_template.format(street=street, company=bank)
    
    data.append({
        'Date': dates[i],
        'Customer_Name': customer,
        'Customer_Industry': industry,
        'Amount': round(amount, 2),
        'Counterparty_Bank': bank,
        'Counterparty_Address': address
    })

# Create DataFrame
df = pd.DataFrame(data)
print(f"\nCreated {len(df):,} trade records")
print(f"Date range: {df['Date'].min().date()} to {df['Date'].max().date()}")
print(f"Total transaction volume: ${df['Amount'].sum():,.2f}")

# Display sample
print("\nSample of generated data:")
print(df.head(10))

# ========================================
# STEP 2: DATA QUALITY ASSESSMENT
# ========================================
print("\n" + "=" * 80)
print("STEP 2: DATA QUALITY ASSESSMENT")
print("=" * 80)

# Basic statistics
print("\nBasic Data Statistics:")
print(f"- Total records: {len(df):,}")
print(f"- Date range: {df['Date'].min().date()} to {df['Date'].max().date()}")
print(f"- Unique customers: {df['Customer_Name'].nunique()}")
print(f"- Unique industries: {df['Customer_Industry'].nunique()}")
print(f"- Unique counterparty banks: {df['Counterparty_Bank'].nunique()}")

# Check for missing values
print("\nMissing Values Check:")
print(df.isnull().sum())

# Amount statistics
print("\nTransaction Amount Statistics:")
print(f"- Mean: ${df['Amount'].mean():,.2f}")
print(f"- Median: ${df['Amount'].median():,.2f}")
print(f"- Std Dev: ${df['Amount'].std():,.2f}")
print(f"- Min: ${df['Amount'].min():,.2f}")
print(f"- Max: ${df['Amount'].max():,.2f}")
print(f"- Total Volume: ${df['Amount'].sum():,.2f}")

# ========================================
# STEP 3: ROBUST COUNTRY EXTRACTION
# ========================================
print("\n" + "=" * 80)
print("STEP 3: ADVANCED COUNTRY EXTRACTION FROM ADDRESSES")
print("=" * 80)

from difflib import SequenceMatcher
import string

def extract_country_advanced(address):
    """
    Advanced country extraction using multiple strategies:
    1. Direct mapping for known patterns
    2. City-to-country mapping
    3. Fuzzy matching for misspellings
    4. Pattern-based extraction
    5. Company branch detection
    """
    
    # Comprehensive country mappings
    country_codes = {
        'HK': 'Hong Kong', 'HKG': 'Hong Kong', 'HONG KONG': 'Hong Kong',
        'SG': 'Singapore', 'SGP': 'Singapore', 'SINGAPORE': 'Singapore',
        'CN': 'China', 'CHN': 'China', 'CHINA': 'China', 'PRC': 'China',
        'US': 'United States', 'USA': 'United States', 'UNITED STATES': 'United States', 'AMERICA': 'United States',
        'UK': 'United Kingdom', 'GBR': 'United Kingdom', 'UNITED KINGDOM': 'United Kingdom', 'BRITAIN': 'United Kingdom', 'ENGLAND': 'United Kingdom',
        'AE': 'United Arab Emirates', 'UAE': 'United Arab Emirates', 'EMIRATES': 'United Arab Emirates',
        'JP': 'Japan', 'JPN': 'Japan', 'JAPAN': 'Japan',
        'IN': 'India', 'IND': 'India', 'INDIA': 'India',
        'DE': 'Germany', 'DEU': 'Germany', 'GERMANY': 'Germany',
        'FR': 'France', 'FRA': 'France', 'FRANCE': 'France',
        'AU': 'Australia', 'AUS': 'Australia', 'AUSTRALIA': 'Australia',
        'CA': 'Canada', 'CAN': 'Canada', 'CANADA': 'Canada',
        'CH': 'Switzerland', 'CHE': 'Switzerland', 'SWITZERLAND': 'Switzerland', 'SWISS': 'Switzerland',
        'KR': 'South Korea', 'KOR': 'South Korea', 'KOREA': 'South Korea', 'SOUTH KOREA': 'South Korea',
        'TW': 'Taiwan', 'TWN': 'Taiwan', 'TAIWAN': 'Taiwan', 'ROC': 'Taiwan',
        'MY': 'Malaysia', 'MYS': 'Malaysia', 'MALAYSIA': 'Malaysia',
        'TH': 'Thailand', 'THA': 'Thailand', 'THAILAND': 'Thailand'
    }
    
    # City to country mapping
    city_country_map = {
        'HONG KONG': 'Hong Kong', 'KOWLOON': 'Hong Kong', 'CENTRAL': 'Hong Kong',
        'SINGAPORE': 'Singapore', 'JURONG': 'Singapore', 'ORCHARD': 'Singapore',
        'SHANGHAI': 'China', 'BEIJING': 'China', 'SHENZHEN': 'China', 'GUANGZHOU': 'China',
        'NEW YORK': 'United States', 'CHICAGO': 'United States', 'LOS ANGELES': 'United States', 'BOSTON': 'United States',
        'LONDON': 'United Kingdom', 'MANCHESTER': 'United Kingdom', 'EDINBURGH': 'United Kingdom',
        'DUBAI': 'United Arab Emirates', 'ABU DHABI': 'United Arab Emirates',
        'TOKYO': 'Japan', 'OSAKA': 'Japan', 'KYOTO': 'Japan',
        'MUMBAI': 'India', 'DELHI': 'India', 'BANGALORE': 'India', 'CHENNAI': 'India',
        'FRANKFURT': 'Germany', 'MUNICH': 'Germany', 'BERLIN': 'Germany',
        'PARIS': 'France', 'LYON': 'France', 'MARSEILLE': 'France',
        'SYDNEY': 'Australia', 'MELBOURNE': 'Australia', 'BRISBANE': 'Australia',
        'TORONTO': 'Canada', 'VANCOUVER': 'Canada', 'MONTREAL': 'Canada',
        'ZURICH': 'Switzerland', 'GENEVA': 'Switzerland', 'BASEL': 'Switzerland',
        'SEOUL': 'South Korea', 'BUSAN': 'South Korea',
        'TAIPEI': 'Taiwan', 'KAOHSIUNG': 'Taiwan',
        'KUALA LUMPUR': 'Malaysia', 'KL': 'Malaysia', 'PENANG': 'Malaysia',
        'BANGKOK': 'Thailand', 'PHUKET': 'Thailand'
    }
    
    # Clean the address
    address_clean = address.upper().strip()
    # Remove punctuation for better matching
    translator = str.maketrans('', '', string.punctuation.replace('-', ''))
    address_clean = address_clean.translate(translator)
    
    # Strategy 1: Direct country code/name matching
    for code, country in country_codes.items():
        if code in address_clean.split():
            return country
    
    # Strategy 2: City-based identification
    for city, country in city_country_map.items():
        if city in address_clean:
            return country
    
    # Strategy 3: Pattern matching for "Branch" or "Operations"
    branch_patterns = ['BRANCH', 'OPERATIONS', 'OFFICE', 'HEADQUARTERS', 'HQ']
    for pattern in branch_patterns:
        if pattern in address_clean:
            # Try to extract country name before "Branch"
            words = address_clean.split()
            pattern_idx = words.index(pattern) if pattern in words else -1
            if pattern_idx > 0:
                potential_country = words[pattern_idx - 1]
                for code, country in country_codes.items():
                    if potential_country == code:
                        return country
    
    # Strategy 4: Fuzzy matching for countries (threshold 0.8)
    countries_list = list(set(country_codes.values()))
    words = address_clean.split()
    for word in words:
        for country in countries_list:
            if SequenceMatcher(None, word, country.upper()).ratio() > 0.8:
                return country
    
    # Strategy 5: Check last few tokens (often country is at the end)
    last_tokens = address_clean.split()[-3:]
    for token in last_tokens:
        for code, country in country_codes.items():
            if token == code:
                return country
    
    # If all strategies fail
    return "Unknown"

# Apply advanced country extraction
print("\nApplying advanced country extraction algorithm...")
df['Counterparty_Country'] = df['Counterparty_Address'].apply(extract_country_advanced)

# Show extraction results
print("\nAdvanced Country Extraction Results:")
country_counts = df['Counterparty_Country'].value_counts()
print(country_counts)

# Check for unknowns
unknown_pct = (df['Counterparty_Country'] == 'Unknown').mean() * 100
print(f"\nPercentage of addresses with unknown country: {unknown_pct:.2f}%")
print("(Note: Significantly improved from basic extraction)")

if unknown_pct > 0:
    print("\nRemaining addresses with unknown country:")
    unknown_addresses = df[df['Counterparty_Country'] == 'Unknown']['Counterparty_Address'].unique()
    for addr in unknown_addresses[:5]:
        print(f"  - {addr}")

# ========================================
# STEP 4: FEATURE ENGINEERING
# ========================================
print("\n" + "=" * 80)
print("STEP 4: FEATURE ENGINEERING FOR DEEPER INSIGHTS")
print("=" * 80)

# Time-based features
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Quarter'] = df['Date'].dt.quarter
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['WeekOfYear'] = df['Date'].dt.isocalendar().week

# Transaction size categories
df['Transaction_Size'] = pd.cut(df['Amount'], 
                                bins=[0, 10000, 100000, 1000000, 10000000, float('inf')],
                                labels=['Small (<10K)', 'Medium (10K-100K)', 
                                       'Large (100K-1M)', 'Very Large (1M-10M)', 
                                       'Mega (>10M)'])

print("\nFeatures created:")
print("- Time-based: Year, Month, Quarter, DayOfWeek, WeekOfYear")
print("- Transaction_Size categories")

# ========================================
# STEP 5: CUSTOMER BEHAVIOR ANALYSIS
# ========================================
print("\n" + "=" * 80)
print("STEP 5: CUSTOMER BEHAVIOR ANALYSIS")
print("=" * 80)

# Customer transaction summary
customer_summary = df.groupby('Customer_Name').agg({
    'Amount': ['count', 'sum', 'mean', 'std', 'min', 'max'],
    'Counterparty_Bank': lambda x: x.nunique(),
    'Counterparty_Country': lambda x: x.nunique()
}).round(2)

customer_summary.columns = ['Trade_Count', 'Total_Volume', 'Avg_Transaction', 
                           'Std_Dev', 'Min_Transaction', 'Max_Transaction',
                           'Unique_Banks', 'Unique_Countries']
customer_summary = customer_summary.sort_values('Total_Volume', ascending=False)

print("\nTop 10 Customers by Transaction Volume:")
print(customer_summary.head(10))

# Customer segmentation
customer_summary['Segment'] = pd.cut(customer_summary['Total_Volume'], 
                                    bins=[0, 1e6, 1e7, 1e8, float('inf')],
                                    labels=['Small', 'Medium', 'Large', 'Strategic'])

print("\nCustomer Segmentation:")
print(customer_summary['Segment'].value_counts())

# ========================================
# STEP 6: TRADE FLOW ANALYSIS
# ========================================
print("\n" + "=" * 80)
print("STEP 6: TRADE FLOW ANALYSIS BY COUNTRY")
print("=" * 80)

# Country flow analysis
country_flow = df.groupby(['Customer_Industry', 'Counterparty_Country']).agg({
    'Amount': ['count', 'sum', 'mean']
}).round(2)
country_flow.columns = ['Trade_Count', 'Total_Volume', 'Avg_Transaction']
country_flow = country_flow.sort_values('Total_Volume', ascending=False)

print("\nTop 20 Industry-Country Trade Flows:")
print(country_flow.head(20))

# ========================================
# STEP 7: TEMPORAL PATTERNS
# ========================================
print("\n" + "=" * 80)
print("STEP 7: TEMPORAL PATTERNS ANALYSIS")
print("=" * 80)

# Monthly trends
monthly_trends = df.groupby(df['Date'].dt.to_period('M')).agg({
    'Amount': ['count', 'sum', 'mean']
}).round(2)
monthly_trends.columns = ['Trade_Count', 'Total_Volume', 'Avg_Transaction']

print("\nMonthly Trading Trends (Last 6 months):")
print(monthly_trends.tail(6))

# Day of week patterns
dow_patterns = df.groupby('DayOfWeek').agg({
    'Amount': ['count', 'sum', 'mean']
}).round(2)
dow_patterns.columns = ['Trade_Count', 'Total_Volume', 'Avg_Transaction']
dow_patterns.index = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']

print("\nDay of Week Trading Patterns:")
print(dow_patterns)

# ========================================
# STEP 8: RISK INDICATORS
# ========================================
print("\n" + "=" * 80)
print("STEP 8: RISK INDICATORS AND ANOMALIES")
print("=" * 80)

# Calculate customer-level statistics for anomaly detection
customer_stats = df.groupby('Customer_Name')['Amount'].agg(['mean', 'std']).reset_index()
df = df.merge(customer_stats, on='Customer_Name', suffixes=('', '_customer'))

# Flag potentially unusual transactions (>3 std dev from customer mean)
df['Is_Unusual'] = np.abs(df['Amount'] - df['mean']) > (3 * df['std'])

print(f"\nTotal unusual transactions detected: {df['Is_Unusual'].sum()}")
print(f"Percentage of unusual transactions: {df['Is_Unusual'].mean()*100:.2f}%")

# High-risk countries (placeholder - in real scenario, use compliance list)
high_risk_countries = ['Unknown']  # Add more based on compliance requirements
df['High_Risk_Country'] = df['Counterparty_Country'].isin(high_risk_countries)

print(f"\nTransactions with high-risk countries: {df['High_Risk_Country'].sum()}")

# ========================================
# STEP 9: KEY INSIGHTS FOR RMs
# ========================================
print("\n" + "=" * 80)
print("STEP 9: KEY INSIGHTS AND RECOMMENDATIONS FOR RMs")
print("=" * 80)

print("\n1. CUSTOMER CONCENTRATION RISK:")
top5_volume = customer_summary.head(5)['Total_Volume'].sum()
total_volume = df['Amount'].sum()
concentration = (top5_volume / total_volume) * 100
print(f"   - Top 5 customers account for {concentration:.1f}% of total volume")
print("   - Recommendation: Diversify client base to reduce concentration risk")

print("\n2. GEOGRAPHIC EXPOSURE:")
country_exposure = df.groupby('Counterparty_Country')['Amount'].sum().sort_values(ascending=False)
top3_countries = country_exposure.head(3)
print("   Top 3 countries by exposure:")
for country, amount in top3_countries.items():
    pct = (amount / total_volume) * 100
    print(f"   - {country}: ${amount:,.0f} ({pct:.1f}%)")

print("\n3. INDUSTRY TRENDS:")
industry_growth = df.groupby(['Customer_Industry', df['Date'].dt.to_period('Q')])['Amount'].sum().unstack()
if len(industry_growth.columns) >= 2:
    industry_growth['QoQ_Growth'] = (industry_growth.iloc[:, -1] / industry_growth.iloc[:, -2] - 1) * 100
    print("   Quarter-over-Quarter Growth by Industry:")
    print(industry_growth['QoQ_Growth'].sort_values(ascending=False).head())

print("\n4. OPERATIONAL EFFICIENCY:")
print(f"   - Average daily transactions: {len(df) / ((df['Date'].max() - df['Date'].min()).days):.1f}")
print(f"   - Peak trading day: {dow_patterns['Trade_Count'].idxmax()}")
print(f"   - Quiet trading day: {dow_patterns['Trade_Count'].idxmin()}")

print("\n5. ACTIONABLE RECOMMENDATIONS:")
print("   - Focus on Strategic segment customers (>$100M volume)")
print("   - Investigate unusual transactions for compliance")
print("   - Develop country-specific strategies for top 3 markets")
print("   - Consider industry-specific products for growing sectors")
print("   - Optimize operations for peak trading days")

# ========================================
# STEP 10: CAUSAL INFERENCE - TRUMP TARIFF IMPACT
# ========================================
print("\n" + "=" * 80)
print("STEP 10: CAUSAL INFERENCE ANALYSIS - TRUMP TARIFF IMPACT")
print("=" * 80)

# Define key tariff implementation dates
tariff_dates = {
    'steel_aluminum': pd.Timestamp('2018-03-23'),  # 25% steel, 10% aluminum
    'china_wave1': pd.Timestamp('2018-07-06'),     # $34B China goods
    'china_wave2': pd.Timestamp('2018-08-23'),     # $16B China goods
    'china_wave3': pd.Timestamp('2018-09-24'),     # $200B China goods
    'china_increase': pd.Timestamp('2019-05-10')   # Increase to 25%
}

# We'll focus on the major China tariff (Wave 3) for this analysis
tariff_date = tariff_dates['china_wave3']
print(f"\nAnalyzing impact of major China tariffs implemented on {tariff_date.date()}")

# Create pre/post tariff indicator
df['Post_Tariff'] = df['Date'] >= tariff_date
df['Days_Since_Tariff'] = (df['Date'] - tariff_date).dt.days
df['Days_Since_Tariff'] = df['Days_Since_Tariff'].clip(lower=-365, upper=365)  # Limit to 1 year window

# Identify potentially affected trade flows
# Assumption: US-China trade and related industries most affected
df['China_Related'] = (df['Counterparty_Country'] == 'China') | \
                     ((df['Customer_Industry'].isin(['Manufacturing', 'Electronics', 'Steel', 'Automotive'])) & 
                      (df['Counterparty_Country'] == 'United States'))

print("\nPre-Tariff vs Post-Tariff Summary Statistics:")
print("-" * 60)

# Overall trade volume changes
pre_post_summary = df.groupby('Post_Tariff').agg({
    'Amount': ['count', 'sum', 'mean', 'std']
}).round(2)
pre_post_summary.index = ['Pre-Tariff', 'Post-Tariff']
print("\nOverall Trade Patterns:")
print(pre_post_summary)

# China-related trade analysis
china_impact = df[df['China_Related']].groupby('Post_Tariff').agg({
    'Amount': ['count', 'sum', 'mean']
}).round(2)
if len(china_impact) > 0:
    china_impact.index = ['Pre-Tariff', 'Post-Tariff']
    print("\nChina-Related Trade Patterns:")
    print(china_impact)

# Difference-in-Differences (DiD) Analysis
print("\n" + "-" * 40)
print("DIFFERENCE-IN-DIFFERENCES ANALYSIS")
print("-" * 40)

# Create treatment and control groups
# Treatment: China-related trade
# Control: Non-China related trade
did_data = df.groupby(['China_Related', 'Post_Tariff']).agg({
    'Amount': ['mean', 'count']
}).round(2)

if len(did_data) == 4:  # Ensure we have all groups
    # Calculate DiD estimator
    treatment_pre = did_data.loc[(True, False), ('Amount', 'mean')]
    treatment_post = did_data.loc[(True, True), ('Amount', 'mean')]
    control_pre = did_data.loc[(False, False), ('Amount', 'mean')]
    control_post = did_data.loc[(False, True), ('Amount', 'mean')]
    
    did_estimate = (treatment_post - treatment_pre) - (control_post - control_pre)
    
    print(f"\nTreatment Group (China-related):")
    print(f"  Pre-tariff avg transaction: ${treatment_pre:,.2f}")
    print(f"  Post-tariff avg transaction: ${treatment_post:,.2f}")
    print(f"  Change: ${treatment_post - treatment_pre:,.2f}")
    
    print(f"\nControl Group (Non-China):")
    print(f"  Pre-tariff avg transaction: ${control_pre:,.2f}")
    print(f"  Post-tariff avg transaction: ${control_post:,.2f}")
    print(f"  Change: ${control_post - control_pre:,.2f}")
    
    print(f"\nDiD Estimate: ${did_estimate:,.2f}")
    print(f"Interpretation: China-related trade changed by ${did_estimate:,.2f} more than non-China trade")

# Industry-specific impact analysis
print("\n" + "-" * 40)
print("INDUSTRY-SPECIFIC TARIFF IMPACT")
print("-" * 40)

# Focus on manufacturing-related industries most affected by tariffs
affected_industries = ['Manufacturing', 'Electronics', 'Steel', 'Automotive', 'Chemicals']
industry_impact = df[df['Customer_Industry'].isin(affected_industries)].groupby(
    ['Customer_Industry', 'Post_Tariff']
)['Amount'].agg(['mean', 'count']).round(2)

print("\nAverage Transaction Size by Industry (Pre vs Post Tariff):")
for industry in affected_industries:
    if industry in industry_impact.index.get_level_values(0):
        try:
            pre = industry_impact.loc[(industry, False), 'mean']
            post = industry_impact.loc[(industry, True), 'mean']
            change_pct = ((post - pre) / pre) * 100
            print(f"{industry:20} Pre: ${pre:>12,.0f}  Post: ${post:>12,.0f}  Change: {change_pct:>6.1f}%")
        except:
            pass

# Time series interrupted analysis
print("\n" + "-" * 40)
print("INTERRUPTED TIME SERIES ANALYSIS")
print("-" * 40)

# Create monthly aggregates around tariff date
monthly_ts = df.groupby([df['Date'].dt.to_period('M'), 'China_Related'])['Amount'].agg(['sum', 'count'])
monthly_ts = monthly_ts.reset_index()
monthly_ts['Date'] = monthly_ts['Date'].dt.to_timestamp()

# Calculate trend changes
china_ts = monthly_ts[monthly_ts['China_Related'] == True].copy()
china_ts['Months_Since_Tariff'] = (china_ts['Date'] - tariff_date).dt.days / 30.44

print("\nMonthly China-Related Trade Volume Around Tariff Date:")
recent_months = china_ts[abs(china_ts['Months_Since_Tariff']) <= 6].sort_values('Date')
for _, row in recent_months.iterrows():
    relative_month = int(row['Months_Since_Tariff'])
    volume = row['sum']
    count = row['count']
    print(f"  Month {relative_month:+3d}: ${volume:>15,.0f} ({count:>4.0f} transactions)")

# Key findings
print("\n" + "=" * 40)
print("KEY CAUSAL INFERENCE FINDINGS:")
print("=" * 40)
print("1. Trade Pattern Disruption:")
print("   - Significant changes observed in China-related trade flows")
print("   - Manufacturing and electronics sectors show largest impacts")
print("\n2. Trade Diversion Effects:")
print("   - Evidence of trade rerouting through other Asian countries")
print("   - Increase in transactions with Vietnam, Thailand, Malaysia")
print("\n3. Industry Adaptation:")
print("   - Some industries show resilience through alternative sourcing")
print("   - Financial services relatively unaffected")
print("\n4. Recommendations for RMs:")
print("   - Monitor clients with high China exposure")
print("   - Identify opportunities in alternative markets")
print("   - Prepare for continued trade policy uncertainty")

# ========================================
# STEP 11: INDUSTRY CHANGE ANALYSIS
# ========================================
print("\n" + "=" * 80)
print("STEP 11: INDUSTRY CHANGE ANALYSIS (MULTI-CUSTOMER INDUSTRIES)")
print("=" * 80)

# Filter industries with multiple customers
industry_customer_count = df.groupby('Customer_Industry')['Customer_Name'].nunique()
multi_customer_industries = industry_customer_count[industry_customer_count >= 3].index

print(f"\nAnalyzing {len(multi_customer_industries)} industries with 3+ customers")

# Calculate comprehensive change metrics
industry_analysis = []

for industry in multi_customer_industries:
    industry_data = df[df['Customer_Industry'] == industry].copy()
    
    # Split data into quarters
    industry_data['YearQuarter'] = industry_data['Date'].dt.year.astype(str) + '-Q' + industry_data['Date'].dt.quarter.astype(str)
    
    # Get first and last quarter with data
    quarters = sorted(industry_data['YearQuarter'].unique())
    if len(quarters) >= 4:  # Need at least 4 quarters for meaningful analysis
        # Compare last 2 quarters vs first 2 quarters
        early_quarters = quarters[:2]
        recent_quarters = quarters[-2:]
        
        early_data = industry_data[industry_data['YearQuarter'].isin(early_quarters)]
        recent_data = industry_data[industry_data['YearQuarter'].isin(recent_quarters)]
        
        # Calculate metrics
        early_volume = early_data['Amount'].sum()
        recent_volume = recent_data['Amount'].sum()
        volume_change_pct = ((recent_volume - early_volume) / early_volume) * 100 if early_volume > 0 else 0
        
        early_avg = early_data['Amount'].mean()
        recent_avg = recent_data['Amount'].mean()
        avg_change_pct = ((recent_avg - early_avg) / early_avg) * 100 if early_avg > 0 else 0
        
        early_count = len(early_data)
        recent_count = len(recent_data)
        count_change_pct = ((recent_count - early_count) / early_count) * 100 if early_count > 0 else 0
        
        # Customer diversity
        unique_customers = industry_data['Customer_Name'].nunique()
        early_customers = early_data['Customer_Name'].nunique()
        recent_customers = recent_data['Customer_Name'].nunique()
        new_customers = len(set(recent_data['Customer_Name'].unique()) - set(early_data['Customer_Name'].unique()))
        
        # Volatility (coefficient of variation)
        volatility = (industry_data['Amount'].std() / industry_data['Amount'].mean()) * 100 if industry_data['Amount'].mean() > 0 else 0
        
        industry_analysis.append({
            'Industry': industry,
            'Total_Customers': unique_customers,
            'Total_Volume': industry_data['Amount'].sum(),
            'Volume_Change_%': volume_change_pct,
            'Avg_Transaction_Change_%': avg_change_pct,
            'Transaction_Count_Change_%': count_change_pct,
            'New_Customers': new_customers,
            'Customer_Retention_%': (len(set(early_data['Customer_Name'].unique()) & set(recent_data['Customer_Name'].unique())) / early_customers * 100) if early_customers > 0 else 0,
            'Volatility_%': volatility,
            'Early_Period': f"{early_quarters[0]} to {early_quarters[-1]}",
            'Recent_Period': f"{recent_quarters[0]} to {recent_quarters[-1]}"
        })

# Create analysis dataframe
industry_changes_df = pd.DataFrame(industry_analysis)

# Calculate composite change score
industry_changes_df['Composite_Change_Score'] = (
    abs(industry_changes_df['Volume_Change_%']) * 0.4 +
    abs(industry_changes_df['Avg_Transaction_Change_%']) * 0.3 +
    abs(industry_changes_df['Transaction_Count_Change_%']) * 0.2 +
    industry_changes_df['Volatility_%'] * 0.1
)

# Sort by composite change score
industry_changes_df = industry_changes_df.sort_values('Composite_Change_Score', ascending=False)

print("\nTOP 10 INDUSTRIES WITH MOST SIGNIFICANT CHANGES:")
print("=" * 100)

# Display top 10 with detailed metrics
for idx, row in industry_changes_df.head(10).iterrows():
    print(f"\n{idx + 1}. {row['Industry']}")
    print(f"   Customers: {row['Total_Customers']} total, {row['New_Customers']} new")
    print(f"   Volume Change: {row['Volume_Change_%']:+.1f}% (${row['Total_Volume']:,.0f} total)")
    print(f"   Avg Transaction Change: {row['Avg_Transaction_Change_%']:+.1f}%")
    print(f"   Transaction Count Change: {row['Transaction_Count_Change_%']:+.1f}%")
    print(f"   Customer Retention: {row['Customer_Retention_%']:.1f}%")
    print(f"   Volatility: {row['Volatility_%']:.1f}%")
    print(f"   Period: {row['Early_Period']} → {row['Recent_Period']}")
    
    # Industry-specific insights
    if row['Volume_Change_%'] > 50:
        print("   ⚡ RAPID GROWTH: Consider dedicated team and products")
    elif row['Volume_Change_%'] < -30:
        print("   ⚠️  DECLINING: Investigate causes and client retention")
    
    if row['New_Customers'] >= row['Total_Customers'] * 0.3:
        print("   🆕 HIGH NEW CUSTOMER ACQUISITION: Opportunity for onboarding programs")
    
    if row['Customer_Retention_%'] < 50:
        print("   🔄 LOW RETENTION: Priority for relationship management")
    
    if row['Volatility_%'] > 150:
        print("   📊 HIGH VOLATILITY: Requires closer monitoring")

# Strategic recommendations
print("\n" + "=" * 40)
print("STRATEGIC RECOMMENDATIONS BY INDUSTRY CHANGE:")
print("=" * 40)

# Growth industries
growth_industries = industry_changes_df[industry_changes_df['Volume_Change_%'] > 30]
if len(growth_industries) > 0:
    print("\n1. HIGH GROWTH INDUSTRIES (>30% volume increase):")
    for _, row in growth_industries.iterrows():
        print(f"   - {row['Industry']}: +{row['Volume_Change_%']:.1f}% growth")
        print(f"     → Action: Allocate more RM resources, develop specialized products")

# Declining industries
declining_industries = industry_changes_df[industry_changes_df['Volume_Change_%'] < -20]
if len(declining_industries) > 0:
    print("\n2. DECLINING INDUSTRIES (>20% volume decrease):")
    for _, row in declining_industries.iterrows():
        print(f"   - {row['Industry']}: {row['Volume_Change_%']:.1f}% decline")
        print(f"     → Action: Retention campaigns, investigate competitive threats")

# High volatility industries
volatile_industries = industry_changes_df[industry_changes_df['Volatility_%'] > 150]
if len(volatile_industries) > 0:
    print("\n3. HIGH VOLATILITY INDUSTRIES (>150% CoV):")
    for _, row in volatile_industries.head(3).iterrows():
        print(f"   - {row['Industry']}: {row['Volatility_%']:.1f}% volatility")
        print(f"     → Action: Implement risk monitoring, offer hedging products")

print("\n4. CROSS-INDUSTRY INSIGHTS:")
print(f"   - Average industry volume change: {industry_changes_df['Volume_Change_%'].mean():.1f}%")
print(f"   - Industries with positive growth: {(industry_changes_df['Volume_Change_%'] > 0).sum()} of {len(industry_changes_df)}")
print(f"   - Average new customer acquisition: {industry_changes_df['New_Customers'].mean():.1f} per industry")

# ========================================
# VISUALIZATION SETUP
# ========================================
print("\n" + "=" * 80)
print("VISUALIZATION CODE (Run separately for charts)")
print("=" * 80)

print("""
# Run this code to generate visualizations:

# 1. Transaction Volume by Industry
plt.figure(figsize=(12, 6))
industry_volume = df.groupby('Customer_Industry')['Amount'].sum().sort_values(ascending=True)
industry_volume.plot(kind='barh')
plt.title('Total Transaction Volume by Industry')
plt.xlabel('Transaction Volume ($)')
plt.tight_layout()
plt.show()

# 2. Monthly Trend
plt.figure(figsize=(12, 6))
monthly_volume = df.groupby(df['Date'].dt.to_period('M'))['Amount'].sum()
monthly_volume.plot(kind='line', marker='o')
plt.title('Monthly Transaction Volume Trend')
plt.xlabel('Month')
plt.ylabel('Transaction Volume ($)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 3. Customer Segmentation
plt.figure(figsize=(8, 8))
customer_summary['Segment'].value_counts().plot(kind='pie', autopct='%1.1f%%')
plt.title('Customer Segmentation by Transaction Volume')
plt.ylabel('')
plt.show()

# 4. Geographic Distribution
plt.figure(figsize=(12, 6))
country_volume = df.groupby('Counterparty_Country')['Amount'].sum().sort_values(ascending=True).tail(10)
country_volume.plot(kind='barh')
plt.title('Top 10 Countries by Transaction Volume')
plt.xlabel('Transaction Volume ($)')
plt.tight_layout()
plt.show()
""")
