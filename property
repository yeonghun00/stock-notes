import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import re
from urllib.parse import urljoin
from typing import Dict, List, Optional, Tuple

class SimplePropertyScraper:
    """Simplified synchronous version with improved parsing"""
    
    def __init__(self, verify_ssl=True, proxy=None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Corporate server settings
        self.session.verify = verify_ssl  # Set to False for corporate servers
        if proxy:
            self.session.proxies = {
                'http': proxy,
                'https': proxy
            }
        
        # Disable SSL warnings if verify is False
        if not verify_ssl:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    def extract_value_from_row(self, soup: BeautifulSoup, label_text: str) -> str:
        """Extract value from table row by label"""
        elem = soup.find(text=re.compile(label_text))
        if elem:
            parent = elem.find_parent('tr') or elem.find_parent('td')
            if parent:
                if parent.name == 'td':
                    next_cell = parent.find_next_sibling('td')
                    if next_cell:
                        return next_cell.get_text(strip=True)
                elif parent.name == 'tr':
                    cells = parent.find_all('td')
                    if len(cells) >= 2:
                        return cells[1].get_text(strip=True)
        return ""
    
    def parse_price_to_millions(self, price_str: str) -> Optional[float]:
        """Convert price string to numeric millions"""
        if not price_str:
            return None
        price_str = re.sub(r'HKD\$?', '', price_str).strip()
        match = re.search(r'([\d.]+)\s*(M|Million|Millions)?', price_str, re.IGNORECASE)
        if match:
            value = float(match.group(1))
            return value if match.group(2) else value / 1_000_000
        return None
    
    def scrape_property(self, url: str) -> Dict:
        """Scrape a single property page with improved extraction"""
        print(f"Scraping: {url}")
        
        try:
            # Use stream=True for large responses if needed
            response = self.session.get(url, stream=False, timeout=30)
            # For streaming: response = self.session.get(url, stream=True)
            # Then: for chunk in response.iter_content(chunk_size=8192): ...
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Initialize property data
            property_data = {
                'url': url,
                'property_id': re.search(r'property-(\d+)', url).group(1) if re.search(r'property-(\d+)', url) else '',
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # Extract location hierarchy
            # Look for region/district/estate in page structure
            page_text = soup.get_text()
            regions = ['New Territories', 'Hong Kong Island', 'Kowloon']
            for region in regions:
                if region in page_text:
                    property_data['region'] = region
                    break
            
            # Extract estate name - improved logic
            estate_row = self.extract_value_from_row(soup, 'Estate')
            if estate_row:
                # Extract estate name before district info
                estate_match = re.search(r'^([^New][^Hong][^Kowloon]+?)(?:New Territories|Hong Kong|Kowloon|$)', estate_row)
                if estate_match:
                    property_data['estate_name'] = estate_match.group(1).strip()
                # Extract block/unit if present
                block_match = re.search(r'Block.*?:?\s*([A-Z0-9]+)', estate_row)
                if block_match:
                    property_data['block_unit'] = f"Block {block_match.group(1)}"
            
            # Extract full address
            property_data['full_address'] = self.extract_value_from_row(soup, 'Address')
            
            # Extract prices with numeric values
            sell_price = self.extract_value_from_row(soup, r'Sell Price|Sell HKD\
    
    def get_property_listings(self, base_url: str, max_pages: int = 3) -> List[str]:
        """Get property URLs from listing pages"""
        property_urls = []
        
        for page in range(1, max_pages + 1):
            url = f"{base_url}?page={page}" if page > 1 else base_url
            print(f"Fetching page {page}...")
            
            try:
                response = self.session.get(url, stream=False, timeout=30)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find property links
                links = soup.find_all('a', href=re.compile(r'/buy/residential/property-\d+'))
                
                for link in links:
                    full_url = urljoin(base_url, link['href'])
                    if full_url not in property_urls:
                        property_urls.append(full_url)
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                print(f"Error fetching page {page}: {str(e)}")
        
        return property_urls
    
    def scrape_all(self, base_url: str, max_properties: int = 10) -> List[Dict]:
        """Main method to scrape all properties"""
        # Get property listings
        print("Getting property listings...")
        property_urls = self.get_property_listings(base_url, max_pages=2)
        print(f"Found {len(property_urls)} properties")
        
        # Limit to max_properties
        property_urls = property_urls[:max_properties]
        
        # Scrape each property
        properties = []
        for i, url in enumerate(property_urls, 1):
            print(f"\nProcessing property {i}/{len(property_urls)}")
            property_data = self.scrape_property(url)
            if property_data:
                properties.append(property_data)
            time.sleep(1.5)  # Rate limiting
        
        return properties
    
    def save_to_json(self, properties: List[Dict], filename: str = "properties_simple.json"):
        """Save results to JSON file"""
        output = {
            'total_properties': len(properties),
            'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'properties': properties
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"\nSaved {len(properties)} properties to {filename}")
    
    def save_to_csv(self, properties: List[Dict], 
                    properties_file: str = "properties_simple.csv",
                    transactions_file: str = "transactions_simple.csv"):
        """Save results to CSV files"""
        # Prepare properties data
        properties_rows = []
        for prop in properties:
            prop_row = {k: v for k, v in prop.items() if k not in ['transactions', 'transaction_urls']}
            properties_rows.append(prop_row)
        
        # Write properties CSV
        if properties_rows:
            with open(properties_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=properties_rows[0].keys())
                writer.writeheader()
                writer.writerows(properties_rows)
            print(f"Saved {len(properties_rows)} properties to {properties_file}")
        
        # Prepare transactions data
        transactions_rows = []
        for prop in properties:
            for trans in prop.get('transactions', []):
                trans_row = trans.copy()
                trans_row['property_id'] = prop.get('property_id', '')
                trans_row['property_url'] = prop.get('url', '')
                transactions_rows.append(trans_row)
        
        # Write transactions CSV
        if transactions_rows:
            with open(transactions_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=transactions_rows[0].keys())
                writer.writeheader()
                writer.writerows(transactions_rows)
            print(f"Saved {len(transactions_rows)} transactions to {transactions_file}")
        
        # Print summary
        print(f"Total transactions scraped: {len(transactions_rows)}")

# Usage example
if __name__ == "__main__":
    # For corporate servers with SSL issues
    scraper = SimplePropertyScraper(
        verify_ssl=False,  # Same as requests verify=False
        proxy='http://proxy.company.com:8080'  # Your proxy
    )
    
    # For streaming (if needed for large pages)
    # response = scraper.session.get(url, stream=True)
    # for chunk in response.iter_content(chunk_size=8192):
    #     process(chunk)
    
    # Scrape properties
    base_url = "https://www.28hse.com/en/buy/residential"
    properties = scraper.scrape_all(base_url, max_properties=5)
    
    # Save results in both formats
    scraper.save_to_json(properties)
    scraper.save_to_csv(properties)
    
    # Print example
    if properties:
        print("\nExample property:")
        example = properties[0]
        print(f"- ID: {example.get('property_id')}")
        print(f"- Estate: {example.get('estate_name')}")
        print(f"- Address: {example.get('full_address')}")
        print(f"- Price: {example.get('sell_price')} (${example.get('sell_price_millions', 0):.2f}M)")
        print(f"- Gross Area: {example.get('gross_area_sqft')} ft² @ ${example.get('gross_area_unit_price')}/ft²")
        print(f"- Transactions found: {len(example.get('transactions', []))}")
)
            if sell_price:
                property_data['sell_price'] = sell_price
                property_data['sell_price_millions'] = self.parse_price_to_millions(sell_price)
            
            valuation = self.extract_value_from_row(soup, 'Valuation')
            if valuation:
                property_data['valuation'] = valuation
                property_data['valuation_millions'] = self.parse_price_to_millions(valuation)
            
            # Extract mortgage info
            mortgage = self.extract_value_from_row(soup, 'Mortgage Monthly Repayment')
            if mortgage:
                property_data['mortgage_monthly_payment'] = mortgage
                amount_match = re.search(r'HKD\$?([\d,]+)', mortgage)
                if amount_match:
                    property_data['mortgage_monthly_amount'] = int(amount_match.group(1).replace(',', ''))
            
            # Extract area information - separated values
            gross_area_text = self.extract_value_from_row(soup, 'Gross Area')
            if gross_area_text:
                sqft_match = re.search(r'(\d+)\s*ft²', gross_area_text)
                if sqft_match:
                    property_data['gross_area_sqft'] = int(sqft_match.group(1))
                price_match = re.search(r'@\s*([\d,]+)', gross_area_text)
                if price_match:
                    property_data['gross_area_unit_price'] = int(price_match.group(1).replace(',', ''))
            
            saleable_area_text = self.extract_value_from_row(soup, 'Saleable Area')
            if saleable_area_text:
                sqft_match = re.search(r'(\d+)\s*ft²', saleable_area_text)
                if sqft_match:
                    property_data['saleable_area_sqft'] = int(sqft_match.group(1))
                price_match = re.search(r'@\s*([\d,]+)', saleable_area_text)
                if price_match:
                    property_data['saleable_area_unit_price'] = int(price_match.group(1).replace(',', ''))
            
            # Extract other details
            property_data['floor_info'] = self.extract_value_from_row(soup, 'Floor zone')
            
            # Extract rooms
            rooms_text = self.extract_value_from_row(soup, 'Room and Bathroom')
            if rooms_text:
                bed_match = re.search(r'(\d+)\s*Bedroom', rooms_text)
                if bed_match:
                    property_data['bedrooms'] = int(bed_match.group(1))
            
            # Extract building age
            age_text = self.extract_value_from_row(soup, 'Building age')
            if age_text:
                age_match = re.search(r'(\d+)\s*Year', age_text)
                if age_match:
                    property_data['building_age_years'] = int(age_match.group(1))
            
            # Extract school networks
            property_data['pri_school_net'] = self.extract_value_from_row(soup, 'Pri School Net')
            property_data['sec_school_net'] = self.extract_value_from_row(soup, 'Sec School Net')
            
            # Find transaction links
            transaction_links = soup.find_all('a', href=re.compile(r'/transaction/deal-\d+'))
            property_data['transaction_urls'] = [urljoin(url, link['href']) for link in transaction_links[:5]]
            
            # Scrape transactions
            property_data['transactions'] = []
            for trans_url in property_data['transaction_urls']:
                time.sleep(1)  # Rate limiting
                transaction = self.scrape_transaction(trans_url)
                if transaction:
                    property_data['transactions'].append(transaction)
            
            return property_data
            
        except Exception as e:
            print(f"Error scraping {url}: {str(e)}")
            return None
    
    def scrape_transaction(self, url: str) -> Optional[Dict]:
        """Scrape a transaction page with improved extraction"""
        try:
            response = self.session.get(url, stream=False, timeout=30)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            transaction = {'url': url}
            
            # Extract property full name from header
            header = soup.find('h1') or soup.find('div', class_='property-title')
            if header:
                full_name = header.get_text(strip=True)
                transaction['property_full_name'] = full_name
                
                # Parse components
                parts = full_name.split(',')
                if parts:
                    transaction['estate_name'] = parts[0].strip()
                    if len(parts) > 1:
                        flat_parts = []
                        for part in parts[1:]:
                            part = part.strip()
                            if any(kw in part for kw in ['Flat', 'Block', 'Floor', 'Low', 'Mid', 'High']):
                                flat_parts.append(part)
                        transaction['flat_info'] = ', '.join(flat_parts)
            
            # Extract location
            transaction['location'] = self.extract_value_from_row(soup, 'Address|Location')
            
            # Extract sold price
            sold_price = self.extract_value_from_row(soup, 'Sold Price')
            if sold_price:
                transaction['sold_price'] = sold_price
                transaction['sold_price_millions'] = self.parse_price_to_millions(sold_price)
            
            # Extract other details
            transaction['registry_date'] = self.extract_value_from_row(soup, 'Registry Date')
            
            # Extract areas
            gross_text = self.extract_value_from_row(soup, 'Gross area')
            if gross_text:
                sqft_match = re.search(r'(\d+)\s*ft²', gross_text)
                if sqft_match:
                    transaction['gross_area_sqft'] = int(sqft_match.group(1))
            
            saleable_text = self.extract_value_from_row(soup, 'Saleable area')
            if saleable_text:
                sqft_match = re.search(r'(\d+)\s*ft²', saleable_text)
                if sqft_match:
                    transaction['saleable_area_sqft'] = int(sqft_match.group(1))
            
            # Extract prices
            transaction['building_unit_price'] = self.extract_value_from_row(soup, 'Building Unit Price')
            transaction['unit_price'] = self.extract_value_from_row(soup, r'Unit Price(?!.*Building)')
            
            # Extract bedrooms
            bedroom_text = self.extract_value_from_row(soup, 'Bedroom')
            if bedroom_text:
                bed_match = re.search(r'(\d+)', bedroom_text)
                if bed_match:
                    transaction['bedrooms'] = int(bed_match.group(1))
            
            transaction['transaction_format'] = self.extract_value_from_row(soup, 'Transaction Format')
            
            return transaction
            
        except Exception as e:
            print(f"Error scraping transaction {url}: {str(e)}")
            return None
    
    def get_property_listings(self, base_url: str, max_pages: int = 3) -> List[str]:
        """Get property URLs from listing pages"""
        property_urls = []
        
        for page in range(1, max_pages + 1):
            url = f"{base_url}?page={page}" if page > 1 else base_url
            print(f"Fetching page {page}...")
            
            try:
                response = self.session.get(url)
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Find property links
                links = soup.find_all('a', href=re.compile(r'/buy/residential/property-\d+'))
                
                for link in links:
                    full_url = urljoin(base_url, link['href'])
                    if full_url not in property_urls:
                        property_urls.append(full_url)
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                print(f"Error fetching page {page}: {str(e)}")
        
        return property_urls
    
    def scrape_all(self, base_url: str, max_properties: int = 10) -> List[Dict]:
        """Main method to scrape all properties"""
        # Get property listings
        print("Getting property listings...")
        property_urls = self.get_property_listings(base_url, max_pages=2)
        print(f"Found {len(property_urls)} properties")
        
        # Limit to max_properties
        property_urls = property_urls[:max_properties]
        
        # Scrape each property
        properties = []
        for i, url in enumerate(property_urls, 1):
            print(f"\nProcessing property {i}/{len(property_urls)}")
            property_data = self.scrape_property(url)
            if property_data:
                properties.append(property_data)
            time.sleep(1.5)  # Rate limiting
        
        return properties
    
    def save_results(self, properties: List[Dict], filename: str = "properties_simple.json"):
        """Save results to JSON file"""
        output = {
            'total_properties': len(properties),
            'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'properties': properties
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"\nSaved {len(properties)} properties to {filename}")
        
        # Print summary
        total_transactions = sum(len(p.get('transactions', [])) for p in properties)
        print(f"Total transactions scraped: {total_transactions}")

# Usage example
if __name__ == "__main__":
    scraper = SimplePropertyScraper()
    
    # Scrape properties
    base_url = "https://www.28hse.com/en/buy/residential"
    properties = scraper.scrape_all(base_url, max_properties=5)
    
    # Save results
    scraper.save_results(properties)
    
    # Print example
    if properties:
        print("\nExample property:")
        example = properties[0]
        print(f"- ID: {example.get('property_id')}")
        print(f"- Price: {example.get('sell_price')}")
        print(f"- Area: {example.get('gross_area')}")
        print(f"- Transactions found: {len(example.get('transactions', []))}")
