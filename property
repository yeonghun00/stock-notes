#!/usr/bin/env python3
"""
28hse.com Property Scraper - Advanced async version
Requirements: pip install aiohttp beautifulsoup4 lxml pandas
Optional: pip install selenium (for JS-heavy pages)
"""

import asyncio
import aiohttp
from bs4 import BeautifulSoup
import json
import csv
from dataclasses import dataclass, asdict, field
from typing import List, Dict, Optional, Any, Tuple
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging
from concurrent.futures import ThreadPoolExecutor
import time
import pandas as pd

# Installation requirements:
# pip install aiohttp beautifulsoup4 lxml pandas
# For Selenium support (optional): pip install selenium
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class Transaction:
    """Data structure for property transactions"""
    # Property identification
    property_full_name: str  # e.g., "City One Shatin, Flat H, Low, Block 30, Site 3"
    estate_name: str
    flat_info: str  # Flat, floor, block info
    location: str
    
    # Transaction details
    sold_price: Optional[str] = None
    sold_price_millions: Optional[float] = None  # Numeric value for calculations
    registry_date: Optional[str] = None
    
    # Area information
    gross_area_sqft: Optional[int] = None
    saleable_area_sqft: Optional[int] = None
    
    # Unit prices
    building_unit_price: Optional[str] = None
    unit_price: Optional[str] = None
    
    # Other info
    bedrooms: Optional[int] = None
    transaction_format: Optional[str] = None
    url: Optional[str] = None

@dataclass
class Property:
    """Main property data structure"""
    # URLs and IDs
    url: str
    property_id: str
    
    # Location hierarchy from breadcrumb
    location_breadcrumb: Optional[str] = None  # e.g., "Islands/Tung Chung/Seaview Crescent"
    estate_name: Optional[str] = None  # Estate name from breadcrumb or table
    full_address: Optional[str] = None  # Complete address from table
    
    # Property identification
    block_unit: Optional[str] = None
    floor_info: Optional[str] = None
    
    # Price information
    sell_price: Optional[str] = None
    sell_price_millions: Optional[float] = None  # Numeric value
    valuation: Optional[str] = None
    valuation_millions: Optional[float] = None
    
    # Area information (separated)
    gross_area_sqft: Optional[int] = None
    gross_area_unit_price: Optional[int] = None
    saleable_area_sqft: Optional[int] = None
    saleable_area_unit_price: Optional[int] = None
    
    # Property details
    building_age_years: Optional[int] = None
    bedrooms: Optional[int] = None
    bathrooms: Optional[int] = None
    
    # School networks
    pri_school_net: Optional[str] = None
    sec_school_net: Optional[str] = None
    
    # Related transactions
    transactions: List[Transaction] = field(default_factory=list)
    
    # Metadata
    scraped_at: str = field(default_factory=lambda: datetime.now().isoformat())

class PropertyScraper:
    def __init__(self, use_selenium=False, max_concurrent=5, delay=1.0, 
                 verify_ssl=True, proxy=None, timeout=30):
        """
        Initialize scraper
        Args:
            use_selenium: Use Selenium for JS-heavy pages (slower but more reliable)
            max_concurrent: Max concurrent requests (for async mode)
            delay: Delay between requests in seconds
            verify_ssl: Whether to verify SSL certificates (False for corporate servers)
            proxy: Proxy URL if needed (e.g., 'http://proxy.company.com:8080')
            timeout: Request timeout in seconds
        """
        self.use_selenium = use_selenium
        self.max_concurrent = max_concurrent
        self.delay = delay
        self.verify_ssl = verify_ssl
        self.proxy = proxy
        self.timeout = timeout
        self.session = None
        self.driver = None
        
        if use_selenium:
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options
            
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
            
            # Add proxy for Selenium if provided
            if proxy:
                chrome_options.add_argument(f'--proxy-server={proxy}')
            
            self.driver = webdriver.Chrome(options=chrome_options)
    
    async def fetch_page(self, url: str) -> str:
        """Fetch page content using aiohttp with corporate server support"""
        if not self.session:
            # Configure SSL context
            ssl_context = None
            if not self.verify_ssl:
                import ssl
                ssl_context = ssl.create_default_context()
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_NONE
            
            # Configure timeout
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            
            # Configure connector with SSL settings
            connector = aiohttp.TCPConnector(
                ssl=ssl_context,
                limit=100,  # Connection pool size
                ttl_dns_cache=300
            )
            
            # Create session with all configurations
            self.session = aiohttp.ClientSession(
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                },
                connector=connector,
                timeout=timeout
            )
        
        try:
            # Make request with proxy if provided
            async with self.session.get(
                url, 
                proxy=self.proxy,
                ssl=False if not self.verify_ssl else None
            ) as response:
                if response.status == 200:
                    # Read response (streaming is automatic in aiohttp)
                    return await response.text()
                else:
                    logger.error(f"Failed to fetch {url}: Status {response.status}")
                    return None
        except aiohttp.ClientError as e:
            logger.error(f"Client error fetching {url}: {str(e)}")
            return None
        except Exception as e:
            logger.error(f"Error fetching {url}: {str(e)}")
            return None
    
    def fetch_page_selenium(self, url: str) -> str:
        """Fetch page using Selenium (for JS-heavy pages)"""
        try:
            self.driver.get(url)
            time.sleep(2)  # Wait for JS to load
            return self.driver.page_source
        except Exception as e:
            logger.error(f"Selenium error fetching {url}: {str(e)}")
            return None
    
    def extract_property_id(self, url: str) -> str:
        """Extract property ID from URL"""
        match = re.search(r'property-(\d+)', url)
        return match.group(1) if match else ""
    
    def parse_price_to_millions(self, price_str: str) -> Optional[float]:
        """Convert price string to numeric millions value"""
        if not price_str:
            return None
        
        # Remove HKD$ and clean
        price_str = re.sub(r'HKD\$?', '', price_str).strip()
        
        # Extract number
        match = re.search(r'([\d.]+)\s*(M|Million|Millions)?', price_str, re.IGNORECASE)
        if match:
            value = float(match.group(1))
            if match.group(2):  # Already in millions
                return value
            else:  # Assume it's in dollars
                return value / 1_000_000
        return None
    
    def extract_breadcrumb_location(self, soup: BeautifulSoup) -> Tuple[str, str]:
        """Extract location from breadcrumb navigation"""
        # Try different breadcrumb selectors
        breadcrumb = (
            soup.find('div', class_='c_breadcrumb') or 
            soup.find('div', class_='ui c_breadcrumb container') or
            soup.find('nav', class_='breadcrumb') or
            soup.find('div', {'class': re.compile(r'breadcrumb', re.I)})
        )
        
        if breadcrumb:
            # Get all text from breadcrumb
            breadcrumb_text = breadcrumb.get_text(' ', strip=True)
            
            # Remove unwanted parts
            exclude = ['Home', 'Buy Property', 'Property Info', 'Buy', 'Property', 'Sell']
            
            # Split by common separators
            parts = re.split(r'[>›/]|\s{2,}', breadcrumb_text)
            filtered_parts = []
            
            for part in parts:
                part = part.strip()
                if part and part not in exclude and len(part) > 1:
                    # Also filter out if it's just numbers (like property ID)
                    if not part.isdigit():
                        filtered_parts.append(part)
            
            if filtered_parts:
                # Join with / separator
                location = '/'.join(filtered_parts)
                # Estate name is usually the last meaningful item
                estate = filtered_parts[-1] if filtered_parts else None
                return location, estate
        
        return None, None
    
    def _extract_value_from_row(self, element) -> str:
        """Helper to extract value from table row"""
        # Try to find the parent row and get the next cell
        if hasattr(element, 'find_parent'):
            parent = element.find_parent('tr') or element.find_parent('td')
            if parent:
                if parent.name == 'td':
                    next_cell = parent.find_next_sibling('td')
                    if next_cell:
                        return next_cell.get_text(strip=True)
                elif parent.name == 'tr':
                    cells = parent.find_all('td')
                    if len(cells) >= 2:
                        return cells[1].get_text(strip=True)
        
        # For NavigableString objects
        elif hasattr(element, 'parent'):
            parent = element.parent
            if parent and parent.name == 'td':
                next_cell = parent.find_next_sibling('td')
                if next_cell:
                    return next_cell.get_text(strip=True)
        
        return ""
    
    def parse_property_page(self, html: str, url: str) -> Tuple[Property, List[str]]:
        """Parse individual property page with improved extraction"""
        soup = BeautifulSoup(html, 'html.parser')
        
        property_obj = Property(
            url=url,
            property_id=self.extract_property_id(url)
        )
        
        # Extract location from breadcrumb
        location_breadcrumb, estate_from_breadcrumb = self.extract_breadcrumb_location(soup)
        property_obj.location_breadcrumb = location_breadcrumb
        
        # Set estate name from breadcrumb first
        if estate_from_breadcrumb:
            property_obj.estate_name = estate_from_breadcrumb
        
        # Look for estate name in table as fallback
        estate_row = soup.find('td', text=re.compile(r'Estate'))
        if estate_row and not property_obj.estate_name:
            estate_info = estate_row.find_next_sibling('td')
            if estate_info:
                estate_text = estate_info.get_text(strip=True)
                # Extract estate name (before district info)
                estate_match = re.search(r'^([^New][^Hong][^Kowloon]+?)(?:New Territories|Hong Kong|Kowloon|$)', estate_text)
                if estate_match:
                    property_obj.estate_name = estate_match.group(1).strip()
                
                # Also extract block/unit info if present
                block_match = re.search(r'Block.*?:?\s*([A-Z0-9]+)', estate_text)
                if block_match:
                    property_obj.block_unit = f"Block {block_match.group(1)}"
        
        # Extract full address
        address_row = soup.find('td', text='Address')
        if address_row:
            address_info = address_row.find_next_sibling('td')
            if address_info:
                property_obj.full_address = address_info.get_text(strip=True)
        
        # Extract price information - Multiple methods to ensure we get it
        # Method 1: Look for table row with td tag
        sell_price_td = soup.find('td', text=re.compile(r'Sell\s*Price'))
        if sell_price_td:
            price_cell = sell_price_td.find_next_sibling('td')
            if price_cell:
                price_text = price_cell.get_text(strip=True)
                property_obj.sell_price = price_text
                property_obj.sell_price_millions = self.parse_price_to_millions(price_text)
        
        # Method 2: Try with text search if td search fails
        if not property_obj.sell_price:
            sell_price_elem = soup.find(text=re.compile(r'Sell\s*Price'))
            if sell_price_elem:
                price_text = self._extract_value_from_row(sell_price_elem)
                if price_text:
                    property_obj.sell_price = price_text
                    property_obj.sell_price_millions = self.parse_price_to_millions(price_text)
        
        # Method 3: Look for price in page text if still not found
        if not property_obj.sell_price:
            page_text = soup.get_text()
            price_patterns = [
                r'Sell\s*(?:Price:?\s*)?HKD\$?([\d.,]+\s*(?:M|Million|Millions))',
                r'Price[:\s]+HKD\$?([\d.,]+\s*(?:M|Million|Millions))',
                r'HKD\$?([\d.,]+\s*(?:M|Million|Millions))'
            ]
            
            for pattern in price_patterns:
                price_match = re.search(pattern, page_text, re.IGNORECASE)
                if price_match:
                    property_obj.sell_price = f"HKD${price_match.group(1)}"
                    property_obj.sell_price_millions = self.parse_price_to_millions(price_match.group(1))
                    break
        
        # Extract valuation
        valuation_td = soup.find('td', text=re.compile(r'Valuation'))
        if valuation_td:
            val_cell = valuation_td.find_next_sibling('td')
            if val_cell:
                val_text = val_cell.get_text(strip=True)
                property_obj.valuation = val_text
                property_obj.valuation_millions = self.parse_price_to_millions(val_text)
        else:
            valuation_elem = soup.find(text=re.compile(r'Valuation'))
            if valuation_elem:
                val_text = self._extract_value_from_row(valuation_elem)
                if val_text:
                    property_obj.valuation = val_text
                    property_obj.valuation_millions = self.parse_price_to_millions(val_text)
        
        # Extract area information with separated values
        gross_area_td = soup.find('td', text=re.compile(r'Gross Area'))
        if gross_area_td:
            area_cell = gross_area_td.find_next_sibling('td')
            if area_cell:
                area_text = area_cell.get_text(strip=True)
                # Extract sqft
                sqft_match = re.search(r'(\d+)\s*ft²', area_text)
                if sqft_match:
                    property_obj.gross_area_sqft = int(sqft_match.group(1))
                # Extract unit price
                unit_price_match = re.search(r'@\s*([\d,]+)', area_text)
                if unit_price_match:
                    property_obj.gross_area_unit_price = int(unit_price_match.group(1).replace(',', ''))
        
        saleable_area_td = soup.find('td', text=re.compile(r'Saleable Area'))
        if saleable_area_td:
            area_cell = saleable_area_td.find_next_sibling('td')
            if area_cell:
                area_text = area_cell.get_text(strip=True)
                # Extract sqft
                sqft_match = re.search(r'(\d+)\s*ft²', area_text)
                if sqft_match:
                    property_obj.saleable_area_sqft = int(sqft_match.group(1))
                # Extract unit price
                unit_price_match = re.search(r'@\s*([\d,]+)', area_text)
                if unit_price_match:
                    property_obj.saleable_area_unit_price = int(unit_price_match.group(1).replace(',', ''))
        
        # Extract building age
        age_td = soup.find('td', text=re.compile(r'Building age'))
        if age_td:
            age_cell = age_td.find_next_sibling('td')
            if age_cell:
                age_text = age_cell.get_text(strip=True)
                age_match = re.search(r'(\d+)\s*Year', age_text)
                if age_match:
                    property_obj.building_age_years = int(age_match.group(1))
        
        # Extract floor info
        floor_td = soup.find('td', text=re.compile(r'Floor zone'))
        if floor_td:
            floor_cell = floor_td.find_next_sibling('td')
            if floor_cell:
                property_obj.floor_info = floor_cell.get_text(strip=True)
        
        # Extract rooms and bathrooms
        rooms_td = soup.find('td', text=re.compile(r'Room and Bathroom'))
        if rooms_td:
            rooms_cell = rooms_td.find_next_sibling('td')
            if rooms_cell:
                rooms_text = rooms_cell.get_text(strip=True)
                # Extract bedrooms
                bed_match = re.search(r'(\d+)\s*Bedroom', rooms_text)
                if bed_match:
                    property_obj.bedrooms = int(bed_match.group(1))
                # Extract bathrooms if present
                bath_match = re.search(r'(\d+)\s*Bathroom', rooms_text)
                if bath_match:
                    property_obj.bathrooms = int(bath_match.group(1))
        
        # Extract school networks
        pri_school_td = soup.find('td', text=re.compile(r'Pri School Net'))
        if pri_school_td:
            school_cell = pri_school_td.find_next_sibling('td')
            if school_cell:
                property_obj.pri_school_net = school_cell.get_text(strip=True)
        
        sec_school_td = soup.find('td', text=re.compile(r'Sec School Net'))
        if sec_school_td:
            school_cell = sec_school_td.find_next_sibling('td')
            if school_cell:
                property_obj.sec_school_net = school_cell.get_text(strip=True)
        
        # Find transaction links
        transaction_links = soup.find_all('a', href=re.compile(r'/transaction/deal-\d+'))
        transaction_urls = [urljoin(url, link['href']) for link in transaction_links]
        
        return property_obj, transaction_urls
    
    def _extract_value_from_row(self, element) -> str:
        """Helper to extract value from table row"""
        # Try to find the parent row and get the next cell
        parent = element.find_parent('tr') or element.find_parent('td')
        if parent:
            if parent.name == 'td':
                next_cell = parent.find_next_sibling('td')
                if next_cell:
                    return next_cell.get_text(strip=True)
            elif parent.name == 'tr':
                cells = parent.find_all('td')
                if len(cells) >= 2:
                    return cells[1].get_text(strip=True)
        
        # Fallback: get text after the label
        if hasattr(element, 'parent'):
            parent_text = element.parent.get_text(strip=True)
            label_text = element.strip() if isinstance(element, str) else element.get_text(strip=True)
            return parent_text.replace(label_text, '').strip()
        
        return ""
    
    def parse_transaction_page(self, html: str, url: str) -> Transaction:
        """Parse transaction page with improved property name extraction"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Initialize transaction
        transaction = Transaction(
            url=url,
            property_full_name="",
            estate_name="",
            flat_info="",
            location=""
        )
        
        # Extract property full name from header
        header = soup.find('h1') or soup.find('div', class_='property-title')
        if header:
            full_name = header.get_text(strip=True)
            transaction.property_full_name = full_name
            
            # Parse components
            # Pattern: "Estate Name, Flat X, Floor, Block Y"
            parts = full_name.split(',')
            if parts:
                transaction.estate_name = parts[0].strip()
                if len(parts) > 1:
                    # Combine flat info
                    flat_parts = []
                    for part in parts[1:]:
                        part = part.strip()
                        if any(keyword in part for keyword in ['Flat', 'Block', 'Floor', 'Low', 'Mid', 'High']):
                            flat_parts.append(part)
                    transaction.flat_info = ', '.join(flat_parts)
        
        # Extract location/address
        address_td = soup.find('td', text=re.compile(r'Address|Location'))
        if address_td:
            location_cell = address_td.find_next_sibling('td')
            if location_cell:
                transaction.location = location_cell.get_text(strip=True)
        else:
            # Try alternative method
            address_elem = soup.find(text=re.compile(r'Address|Location'))
            if address_elem:
                location_text = self._extract_value_from_row(address_elem)
                if location_text:
                    transaction.location = location_text
        
        # Extract sold price
        sold_price_td = soup.find('td', text=re.compile(r'Sold Price'))
        if sold_price_td:
            price_cell = sold_price_td.find_next_sibling('td')
            if price_cell:
                price_text = price_cell.get_text(strip=True)
                transaction.sold_price = price_text
                transaction.sold_price_millions = self.parse_price_to_millions(price_text)
        else:
            sold_price_elem = soup.find(text=re.compile(r'Sold Price'))
            if sold_price_elem:
                price_text = self._extract_value_from_row(sold_price_elem)
                if price_text:
                    transaction.sold_price = price_text
                    transaction.sold_price_millions = self.parse_price_to_millions(price_text)
        
        # Extract registry date
        date_td = soup.find('td', text=re.compile(r'Registry Date'))
        if date_td:
            date_cell = date_td.find_next_sibling('td')
            if date_cell:
                transaction.registry_date = date_cell.get_text(strip=True)
        
        # Extract areas
        gross_td = soup.find('td', text=re.compile(r'Gross area'))
        if gross_td:
            area_cell = gross_td.find_next_sibling('td')
            if area_cell:
                area_text = area_cell.get_text(strip=True)
                sqft_match = re.search(r'(\d+)\s*ft²', area_text)
                if sqft_match:
                    transaction.gross_area_sqft = int(sqft_match.group(1))
        
        saleable_td = soup.find('td', text=re.compile(r'Saleable area'))
        if saleable_td:
            area_cell = saleable_td.find_next_sibling('td')
            if area_cell:
                area_text = area_cell.get_text(strip=True)
                sqft_match = re.search(r'(\d+)\s*ft²', area_text)
                if sqft_match:
                    transaction.saleable_area_sqft = int(sqft_match.group(1))
        
        # Extract unit prices
        building_price_td = soup.find('td', text=re.compile(r'Building Unit Price'))
        if building_price_td:
            price_cell = building_price_td.find_next_sibling('td')
            if price_cell:
                transaction.building_unit_price = price_cell.get_text(strip=True)
        
        unit_price_td = soup.find('td', text=re.compile(r'^Unit Price'))
        if unit_price_td and unit_price_td != building_price_td:
            price_cell = unit_price_td.find_next_sibling('td')
            if price_cell:
                transaction.unit_price = price_cell.get_text(strip=True)
        
        # Extract bedrooms
        bedroom_td = soup.find('td', text=re.compile(r'Bedroom'))
        if bedroom_td:
            bedroom_cell = bedroom_td.find_next_sibling('td')
            if bedroom_cell:
                bedroom_text = bedroom_cell.get_text(strip=True)
                bed_match = re.search(r'(\d+)', bedroom_text)
                if bed_match:
                    transaction.bedrooms = int(bed_match.group(1))
        
        # Extract transaction format
        format_td = soup.find('td', text=re.compile(r'Transaction Format'))
        if format_td:
            format_cell = format_td.find_next_sibling('td')
            if format_cell:
                transaction.transaction_format = format_cell.get_text(strip=True)
        
        return transaction
    
    async def scrape_property_listings(self, base_url: str, max_pages: int = 5) -> List[str]:
        """Scrape property listing URLs from main page"""
        property_urls = []
        
        for page in range(1, max_pages + 1):
            url = f"{base_url}?page={page}" if page > 1 else base_url
            logger.info(f"Fetching listing page {page}: {url}")
            
            if self.use_selenium:
                html = self.fetch_page_selenium(url)
            else:
                html = await self.fetch_page(url)
            
            if not html:
                continue
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # Find property links
            property_links = soup.find_all('a', href=re.compile(r'/property-\d+'))
            
            for link in property_links:
                full_url = urljoin(base_url, link['href'])
                if full_url not in property_urls:
                    property_urls.append(full_url)
            
            await asyncio.sleep(self.delay)
        
        logger.info(f"Found {len(property_urls)} property listings")
        return property_urls
    
    async def scrape_property(self, url: str) -> Property:
        """Scrape individual property and its transactions"""
        logger.info(f"Scraping property: {url}")
        
        if self.use_selenium:
            html = self.fetch_page_selenium(url)
        else:
            html = await self.fetch_page(url)
        
        if not html:
            return None
        
        property_obj, transaction_urls = self.parse_property_page(html, url)
        
        # Scrape transactions
        for trans_url in transaction_urls[:5]:  # Limit to 5 transactions per property
            await asyncio.sleep(self.delay)
            
            if self.use_selenium:
                trans_html = self.fetch_page_selenium(trans_url)
            else:
                trans_html = await self.fetch_page(trans_url)
            
            if trans_html:
                transaction = self.parse_transaction_page(trans_html, trans_url)
                property_obj.transactions.append(transaction)
        
        return property_obj
    
    async def scrape_all(self, base_url: str, max_pages: int = 5, max_properties: int = 20):
        """Main scraping method"""
        # Get property listings
        property_urls = await self.scrape_property_listings(base_url, max_pages)
        
        # Limit properties to scrape
        property_urls = property_urls[:max_properties]
        
        # Scrape properties with concurrency control
        properties = []
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def scrape_with_semaphore(url):
            async with semaphore:
                return await self.scrape_property(url)
        
        tasks = [scrape_with_semaphore(url) for url in property_urls]
        results = await asyncio.gather(*tasks)
        
        properties = [p for p in results if p is not None]
        
        return properties
    
    def save_to_json(self, properties: List[Property], filename: str = "properties.json"):
        """Save scraped data to JSON file"""
        data = {
            'scraped_at': datetime.now().isoformat(),
            'total_properties': len(properties),
            'properties': [asdict(p) for p in properties]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved {len(properties)} properties to {filename}")
    
    def save_to_csv(self, properties: List[Property], 
                    properties_file: str = "properties.csv",
                    transactions_file: str = "transactions.csv"):
        """Save scraped data to CSV files with proper structure"""
        
        # Prepare properties data for CSV
        properties_data = []
        for prop in properties:
            prop_dict = asdict(prop)
            # Remove transactions list from main property data
            prop_dict.pop('transactions', None)
            properties_data.append(prop_dict)
        
        # Save properties CSV
        if properties_data:
            df_properties = pd.DataFrame(properties_data)
            df_properties.to_csv(properties_file, index=False, encoding='utf-8')
            logger.info(f"Saved {len(properties_data)} properties to {properties_file}")
        
        # Prepare transactions data for CSV
        transactions_data = []
        for prop in properties:
            for trans in prop.transactions:
                trans_dict = asdict(trans)
                # Add property reference
                trans_dict['property_id'] = prop.property_id
                trans_dict['property_url'] = prop.url
                transactions_data.append(trans_dict)
        
        # Save transactions CSV
        if transactions_data:
            df_transactions = pd.DataFrame(transactions_data)
            df_transactions.to_csv(transactions_file, index=False, encoding='utf-8')
            logger.info(f"Saved {len(transactions_data)} transactions to {transactions_file}")
    
    async def close(self):
        """Clean up resources"""
        if self.session:
            await self.session.close()
        if self.driver:
            self.driver.quit()

# Example usage
async def main():
    # Initialize scraper
    scraper = PropertyScraper(use_selenium=False, max_concurrent=3, delay=1.5)
    
    try:
        # Scrape properties
        base_url = "https://www.28hse.com/en/buy/residential"
        properties = await scraper.scrape_all(
            base_url=base_url,
            max_pages=2,  # Number of listing pages to scrape
            max_properties=10  # Total properties to scrape
        )
        
        # Save results in both formats
        scraper.save_to_json(properties, "28hse_properties.json")
        scraper.save_to_csv(properties, "28hse_properties.csv", "28hse_transactions.csv")
        
        # Print summary
        print(f"\nScraping completed!")
        print(f"Total properties scraped: {len(properties)}")
        total_transactions = sum(len(p.transactions) for p in properties)
        print(f"Total transactions scraped: {total_transactions}")
        
        # Example: Print first property details
        if properties:
            prop = properties[0]
            print(f"\nExample property:")
            print(f"  ID: {prop.property_id}")
            print(f"  Location: {prop.location_breadcrumb}")
            print(f"  Estate: {prop.estate_name}")
            if prop.sell_price_millions:
                print(f"  Price: {prop.sell_price} (${prop.sell_price_millions:.2f}M)")
            else:
                print(f"  Price: {prop.sell_price or 'Not found'}")
            print(f"  Gross Area: {prop.gross_area_sqft} ft² @ ${prop.gross_area_unit_price}/ft²")
            print(f"  Saleable Area: {prop.saleable_area_sqft} ft² @ ${prop.saleable_area_unit_price}/ft²")
            print(f"  Address: {prop.full_address}")
            print(f"  Transactions: {len(prop.transactions)}")
            
            if prop.transactions:
                trans = prop.transactions[0]
                print(f"\n  First transaction:")
                print(f"    Property: {trans.property_full_name}")
                print(f"    Sold Price: {trans.sold_price}")
                print(f"    Date: {trans.registry_date}")
        
    finally:
        await scraper.close()

if __name__ == "__main__":
    # Run the scraper
    asyncio.run(main())
