#!/usr/bin/env python3
"""
28hse.com Simple Property Scraper - Optimized version
Requirements: pip install requests beautifulsoup4 lxml
"""

import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import re
from urllib.parse import urljoin
from typing import Dict, List, Optional, Tuple

class SimplePropertyScraper:
    """Simplified synchronous version with improved parsing"""
    
    def __init__(self, verify_ssl=True, proxy=None):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # Corporate server settings
        self.session.verify = verify_ssl  # Set to False for corporate servers
        if proxy:
            self.session.proxies = {
                'http': proxy,
                'https': proxy
            }
        
        # Disable SSL warnings if verify is False
        if not verify_ssl:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    
    def extract_value_from_row(self, soup: BeautifulSoup, label_text: str) -> str:
        """Extract value from table row by label"""
        elem = soup.find(text=re.compile(label_text))
        if elem:
            # Try parent td first
            parent = elem.find_parent('td')
            if parent:
                next_cell = parent.find_next_sibling('td')
                if next_cell:
                    return next_cell.get_text(strip=True)
            
            # Try parent tr
            parent = elem.find_parent('tr')
            if parent:
                cells = parent.find_all('td')
                if len(cells) >= 2:
                    return cells[1].get_text(strip=True)
            
            # Fallback
            if hasattr(elem, 'parent'):
                parent_text = elem.parent.get_text(strip=True)
                label = elem.strip() if isinstance(elem, str) else elem.get_text(strip=True)
                value = parent_text.replace(label, '').strip()
                if value and value != parent_text:
                    return value
        return ""
    
    def parse_price_to_millions(self, price_str: str) -> Optional[float]:
        """Convert price string to numeric millions"""
        if not price_str:
            return None
        price_str = re.sub(r'HKD\$?', '', price_str).strip()
        match = re.search(r'([\d.]+)\s*(M|Million|Millions)?', price_str, re.IGNORECASE)
        if match:
            value = float(match.group(1))
            return value if match.group(2) else value / 1_000_000
        return None
    
    def extract_location_from_breadcrumb(self, soup: BeautifulSoup) -> Tuple[str, str, str, str]:
        """Extract location hierarchy from page breadcrumb"""
        location_hierarchy = ""
        region = ""
        district = ""
        estate = ""
        
        # Look for breadcrumb or navigation
        page_text = soup.get_text()
        lines = [line.strip() for line in page_text.split('\n') if line.strip()]
        
        # Find line with pattern: Home Buy Property [Region] [District] [Estate] Property Info
        for line in lines[:30]:  # Check first 30 lines
            if 'Home' in line and 'Property' in line:
                # Split and filter
                parts = line.split()
                # Remove common navigation words
                exclude_words = ['Home', 'Buy', 'Rent', 'Property', 'Info', 'PropertyInfo']
                location_parts = [p for p in parts if p not in exclude_words]
                
                if len(location_parts) >= 3:
                    region = location_parts[0]
                    district = location_parts[1]
                    estate = ' '.join(location_parts[2:])
                    location_hierarchy = f"{region}/{district}/{estate}"
                elif len(location_parts) == 2:
                    region = location_parts[0]
                    district = location_parts[1]
                    location_hierarchy = f"{region}/{district}"
                elif len(location_parts) == 1:
                    region = location_parts[0]
                    location_hierarchy = region
                break
        
        return location_hierarchy, region, district, estate
    
    def scrape_property(self, url: str) -> Dict:
        """Scrape a single property page with improved extraction"""
        print(f"Scraping: {url}")
        
        try:
            response = self.session.get(url, stream=False, timeout=30)
            soup = BeautifulSoup(response.content, 'lxml')  # Use lxml for speed
            
            # Initialize property data
            property_data = {
                'url': url,
                'property_id': re.search(r'property-(\d+)', url).group(1) if re.search(r'property-(\d+)', url) else '',
                'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            # Extract location hierarchy from breadcrumb
            location_hierarchy, region, district, estate = self.extract_location_from_breadcrumb(soup)
            property_data['location_hierarchy'] = location_hierarchy
            property_data['region'] = region
            property_data['district'] = district
            property_data['estate_name'] = estate
            
            # Extract full address
            property_data['full_address'] = self.extract_value_from_row(soup, 'Address')
            
            # Extract sell price - improved logic
            sell_price = None
            sell_price_patterns = [
                r'Sell\s*Price',
                r'Sell\s*HKD\$',
                r'Selling\s*Price',
                r'Price\s*HKD\$'
            ]
            
            for pattern in sell_price_patterns:
                price_elem = soup.find(text=re.compile(pattern, re.IGNORECASE))
                if price_elem:
                    sell_price = self.extract_value_from_row(soup, pattern)
                    if sell_price and 'HKD' in sell_price:
                        property_data['sell_price'] = sell_price
                        property_data['sell_price_millions'] = self.parse_price_to_millions(sell_price)
                        break
            
            # If not found in table, look for price in divs/spans
            if not sell_price:
                price_divs = soup.find_all(['div', 'span'], {'class': re.compile('price|amount')})
                for div in price_divs:
                    text = div.get_text(strip=True)
                    if 'HKD' in text and re.search(r'\d', text):
                        property_data['sell_price'] = text
                        property_data['sell_price_millions'] = self.parse_price_to_millions(text)
                        break
            
            # Extract valuation
            valuation = self.extract_value_from_row(soup, 'Valuation')
            if valuation:
                property_data['valuation'] = valuation
                property_data['valuation_millions'] = self.parse_price_to_millions(valuation)
            
            # Extract area information - separated values
            gross_area_text = self.extract_value_from_row(soup, 'Gross Area')
            if gross_area_text:
                sqft_match = re.search(r'(\d+)\s*ft²', gross_area_text)
                if sqft_match:
                    property_data['gross_area_sqft'] = int(sqft_match.group(1))
                price_match = re.search(r'@\s*([\d,]+)', gross_area_text)
                if price_match:
                    property_data['gross_area_unit_price'] = int(price_match.group(1).replace(',', ''))
            
            saleable_area_text = self.extract_value_from_row(soup, 'Saleable Area')
            if saleable_area_text:
                sqft_match = re.search(r'(\d+)\s*ft²', saleable_area_text)
                if sqft_match:
                    property_data['saleable_area_sqft'] = int(sqft_match.group(1))
                price_match = re.search(r'@\s*([\d,]+)', saleable_area_text)
                if price_match:
                    property_data['saleable_area_unit_price'] = int(price_match.group(1).replace(',', ''))
            
            # Extract other details
            property_data['floor_info'] = self.extract_value_from_row(soup, 'Floor zone')
            property_data['block_unit'] = self.extract_value_from_row(soup, 'Block and Unit')
            
            # Extract rooms
            rooms_text = self.extract_value_from_row(soup, 'Room and Bathroom')
            if rooms_text:
                bed_match = re.search(r'(\d+)\s*Bedroom', rooms_text)
                if bed_match:
                    property_data['bedrooms'] = int(bed_match.group(1))
            
            # Extract building age
            age_text = self.extract_value_from_row(soup, 'Building age')
            if age_text:
                age_match = re.search(r'(\d+)\s*Year', age_text)
                if age_match:
                    property_data['building_age_years'] = int(age_match.group(1))
            
            # Extract school networks
            property_data['pri_school_net'] = self.extract_value_from_row(soup, 'Pri School Net')
            property_data['sec_school_net'] = self.extract_value_from_row(soup, 'Sec School Net')
            
            # Find transaction links
            transaction_links = soup.find_all('a', href=re.compile(r'/transaction/deal-\d+'))
            property_data['transaction_urls'] = [urljoin(url, link['href']) for link in transaction_links[:5]]
            
            # Scrape transactions
            property_data['transactions'] = []
            for trans_url in property_data['transaction_urls']:
                time.sleep(0.5)  # Shorter delay for speed
                transaction = self.scrape_transaction(trans_url)
                if transaction:
                    property_data['transactions'].append(transaction)
            
            return property_data
            
        except Exception as e:
            print(f"Error scraping {url}: {str(e)}")
            return None
    
    def scrape_transaction(self, url: str) -> Optional[Dict]:
        """Scrape a transaction page with improved extraction"""
        try:
            response = self.session.get(url, stream=False, timeout=30)
            soup = BeautifulSoup(response.content, 'lxml')
            
            transaction = {'url': url}
            
            # Extract property full name from header
            header = soup.find('h1') or soup.find('div', class_='property-title')
            if header:
                full_name = header.get_text(strip=True)
                transaction['property_full_name'] = full_name
                
                # Parse components
                parts = full_name.split(',')
                if parts:
                    transaction['estate_name'] = parts[0].strip()
                    if len(parts) > 1:
                        flat_parts = []
                        for part in parts[1:]:
                            part = part.strip()
                            if any(kw in part for kw in ['Flat', 'Block', 'Floor', 'Low', 'Mid', 'High', 'Tower', 'Site']):
                                flat_parts.append(part)
                        transaction['flat_info'] = ', '.join(flat_parts)
            
            # Extract location
            transaction['location'] = self.extract_value_from_row(soup, 'Address|Location')
            
            # Extract sold price
            sold_price = self.extract_value_from_row(soup, 'Sold Price')
            if sold_price:
                transaction['sold_price'] = sold_price
                transaction['sold_price_millions'] = self.parse_price_to_millions(sold_price)
            
            # Extract other details
            transaction['registry_date'] = self.extract_value_from_row(soup, 'Registry Date')
            
            # Extract areas
            gross_text = self.extract_value_from_row(soup, 'Gross area')
            if gross_text:
                sqft_match = re.search(r'(\d+)\s*ft²', gross_text)
                if sqft_match:
                    transaction['gross_area_sqft'] = int(sqft_match.group(1))
            
            saleable_text = self.extract_value_from_row(soup, 'Saleable area')
            if saleable_text:
                sqft_match = re.search(r'(\d+)\s*ft²', saleable_text)
                if sqft_match:
                    transaction['saleable_area_sqft'] = int(sqft_match.group(1))
            
            # Extract prices
            transaction['building_unit_price'] = self.extract_value_from_row(soup, 'Building Unit Price')
            transaction['unit_price'] = self.extract_value_from_row(soup, r'Unit Price(?!.*Building)')
            
            # Extract bedrooms
            bedroom_text = self.extract_value_from_row(soup, 'Bedroom')
            if bedroom_text:
                bed_match = re.search(r'(\d+)', bedroom_text)
                if bed_match:
                    transaction['bedrooms'] = int(bed_match.group(1))
            
            transaction['transaction_format'] = self.extract_value_from_row(soup, 'Transaction Format')
            
            return transaction
            
        except Exception as e:
            print(f"Error scraping transaction {url}: {str(e)}")
            return None
    
    def get_property_listings(self, base_url: str, max_pages: int = 3) -> List[str]:
        """Get property URLs from listing pages"""
        property_urls = []
        
        for page in range(1, max_pages + 1):
            url = f"{base_url}?page={page}" if page > 1 else base_url
            print(f"Fetching page {page}...")
            
            try:
                response = self.session.get(url, stream=False, timeout=30)
                soup = BeautifulSoup(response.content, 'lxml')
                
                # Find property links
                links = soup.find_all('a', href=re.compile(r'/buy/residential/property-\d+'))
                
                for link in links:
                    full_url = urljoin(base_url, link['href'])
                    if full_url not in property_urls:
                        property_urls.append(full_url)
                
                time.sleep(0.5)  # Shorter delay for speed
                
            except Exception as e:
                print(f"Error fetching page {page}: {str(e)}")
        
        return property_urls
    
    def scrape_all(self, base_url: str, max_properties: int = 10) -> List[Dict]:
        """Main method to scrape all properties"""
        # Get property listings
        print("Getting property listings...")
        property_urls = self.get_property_listings(base_url, max_pages=2)
        print(f"Found {len(property_urls)} properties")
        
        # Limit to max_properties
        property_urls = property_urls[:max_properties]
        
        # Scrape each property
        properties = []
        for i, url in enumerate(property_urls, 1):
            print(f"\nProcessing property {i}/{len(property_urls)}")
            property_data = self.scrape_property(url)
            if property_data:
                properties.append(property_data)
            time.sleep(0.5)  # Shorter delay
        
        return properties
    
    def save_to_json(self, properties: List[Dict], filename: str = "properties_simple.json"):
        """Save results to JSON file"""
        output = {
            'total_properties': len(properties),
            'scraped_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'properties': properties
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        
        print(f"\nSaved {len(properties)} properties to {filename}")
    
    def save_to_csv(self, properties: List[Dict], 
                    properties_file: str = "properties_simple.csv",
                    transactions_file: str = "transactions_simple.csv"):
        """Save results to CSV files"""
        # Prepare properties data
        properties_rows = []
        for prop in properties:
            prop_row = {k: v for k, v in prop.items() if k not in ['transactions', 'transaction_urls']}
            properties_rows.append(prop_row)
        
        # Write properties CSV
        if properties_rows:
            with open(properties_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=properties_rows[0].keys())
                writer.writeheader()
                writer.writerows(properties_rows)
            print(f"Saved {len(properties_rows)} properties to {properties_file}")
        
        # Prepare transactions data
        transactions_rows = []
        for prop in properties:
            for trans in prop.get('transactions', []):
                trans_row = trans.copy()
                trans_row['property_id'] = prop.get('property_id', '')
                trans_row['property_url'] = prop.get('url', '')
                transactions_rows.append(trans_row)
        
        # Write transactions CSV
        if transactions_rows:
            with open(transactions_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=transactions_rows[0].keys())
                writer.writeheader()
                writer.writerows(transactions_rows)
            print(f"Saved {len(transactions_rows)} transactions to {transactions_file}")
        
        # Print summary
        print(f"Total transactions scraped: {len(transactions_rows)}")

# Usage example
if __name__ == "__main__":
    # For corporate servers with SSL issues
    scraper = SimplePropertyScraper(
        verify_ssl=False,  # Same as requests verify=False
        proxy=None  # Add your proxy here: 'http://proxy.company.com:8080'
    )
    
    # Scrape properties
    base_url = "https://www.28hse.com/en/buy/residential"
    properties = scraper.scrape_all(base_url, max_properties=10)
    
    # Save results in both formats
    scraper.save_to_json(properties)
    scraper.save_to_csv(properties)
    
    # Print example
    if properties:
        print("\nExample property:")
        example = properties[0]
        print(f"- ID: {example.get('property_id')}")
        print(f"- Location: {example.get('location_hierarchy')}")
        print(f"- Estate: {example.get('estate_name')}")
        print(f"- District: {example.get('district')}")
        print(f"- Region: {example.get('region')}")
        print(f"- Price: {example.get('sell_price')}")
        if example.get('sell_price_millions'):
            print(f"- Price (numeric): ${example.get('sell_price_millions'):.2f}M")
        print(f"- Gross Area: {example.get('gross_area_sqft')} ft² @ ${example.get('gross_area_unit_price')}/ft²")
        print(f"- Address: {example.get('full_address')}")
        print(f"- Transactions found: {len(example.get('transactions', []))}")
