import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

def scrape_centanet_property(url):
    # Initialize Chrome driver
    driver = webdriver.Chrome()
    
    try:
        # Load the page
        print(f"Loading page: {url}")
        driver.get(url)
        
        # Wait for page to load
        wait = WebDriverWait(driver, 20)
        
        # Wait for the table to be present
        time.sleep(3)  # Initial wait for page to render
        
        # Find and click "1個月內" button
        print("Looking for '1個月內' button...")
        try:
            # Try multiple possible selectors for the button
            one_month_button = None
            
            # Method 1: Look for button by text
            buttons = driver.find_elements(By.TAG_NAME, "button")
            for button in buttons:
                if "1個月內" in button.text:
                    one_month_button = button
                    break
            
            # Method 2: Look for span/div containing the text
            if not one_month_button:
                elements = driver.find_elements(By.XPATH, "//*[contains(text(), '1個月內')]")
                if elements:
                    one_month_button = elements[0]
            
            if one_month_button:
                print("Found '1個月內' button, clicking...")
                driver.execute_script("arguments[0].scrollIntoView(true);", one_month_button)
                time.sleep(1)
                driver.execute_script("arguments[0].click();", one_month_button)
                time.sleep(3)  # Wait for the update to apply
            else:
                print("Warning: Could not find '1個月內' button")
        
        except Exception as e:
            print(f"Error clicking '1個月內' button: {e}")
        
        # Get all rows before clicking to store original state
        print("Extracting table data...")
        
        # Find the main table container
        table_data = []
        
        # Try to find table rows - adjust selectors based on actual HTML structure
        rows = driver.find_elements(By.CSS_SELECTOR, "tr, [role='row'], .table-row")
        
        if not rows:
            # Alternative: look for divs that might contain row data
            rows = driver.find_elements(By.CSS_SELECTOR, "div[class*='row'], div[class*='item']")
        
        print(f"Found {len(rows)} rows")
        
        # Extract data from each row
        for i, row in enumerate(rows):
            try:
                # Get all cells in the row
                cells = row.find_elements(By.CSS_SELECTOR, "td, [role='cell'], div[class*='cell'], span[class*='cell']")
                
                if not cells:
                    continue
                
                # Extract text from each cell
                row_data = [cell.text.strip() for cell in cells if cell.text.strip()]
                
                if row_data:  # Only add non-empty rows
                    # Check if this row has been updated (usually indicated by a special class or style)
                    is_updated = False
                    
                    # Check for color changes or special classes that indicate updates
                    row_classes = row.get_attribute("class") or ""
                    row_style = row.get_attribute("style") or ""
                    
                    # Common indicators of updates
                    if any(indicator in row_classes.lower() for indicator in ["updated", "highlight", "active", "selected"]):
                        is_updated = True
                    elif any(color in row_style.lower() for color in ["background", "color", "rgb"]):
                        is_updated = True
                    
                    # Also check cells for highlighting
                    for cell in cells:
                        cell_classes = cell.get_attribute("class") or ""
                        cell_style = cell.get_attribute("style") or ""
                        if any(indicator in cell_classes.lower() for indicator in ["updated", "highlight", "active"]):
                            is_updated = True
                            break
                        if any(color in cell_style.lower() for color in ["background", "color"]):
                            is_updated = True
                            break
                    
                    # Add update status to row data
                    row_data.append("Yes" if is_updated else "No")
                    table_data.append(row_data)
                    
            except Exception as e:
                print(f"Error processing row {i}: {e}")
                continue
        
        # If no data found with first method, try alternative extraction
        if not table_data:
            print("Trying alternative extraction method...")
            # Look for any tabular data structure
            elements = driver.find_elements(By.XPATH, "//*[contains(@class, 'table') or contains(@class, 'grid') or contains(@class, 'list')]//descendant::*[text()]")
            
            current_row = []
            for elem in elements:
                text = elem.text.strip()
                if text:
                    current_row.append(text)
                    # Assume new row every 5-7 elements (adjust based on actual structure)
                    if len(current_row) >= 5:
                        current_row.append("No")  # Default update status
                        table_data.append(current_row)
                        current_row = []
        
        # Create DataFrame
        if table_data:
            # Estimate number of columns based on most common row length
            col_counts = {}
            for row in table_data:
                length = len(row)
                col_counts[length] = col_counts.get(length, 0) + 1
            
            most_common_length = max(col_counts, key=col_counts.get)
            
            # Filter rows to have consistent column count
            filtered_data = [row for row in table_data if len(row) == most_common_length]
            
            # Create column names
            columns = [f"Column_{i+1}" for i in range(most_common_length - 1)]
            columns.append("Updated_Within_1_Month")
            
            # Common column names for property data
            if most_common_length >= 5:
                columns[0] = "Floor/Unit"
                columns[1] = "Area_sqft"
                columns[2] = "Price"
                if most_common_length > 5:
                    columns[3] = "Price_per_sqft"
            
            df = pd.DataFrame(filtered_data, columns=columns)
            
            # Save to CSV
            output_file = "centanet_property_data.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"\nData saved to {output_file}")
            print(f"Total rows extracted: {len(df)}")
            print("\nFirst few rows:")
            print(df.head())
            
        else:
            print("No data could be extracted from the page")
    
    except Exception as e:
        print(f"An error occurred: {e}")
    
    finally:
        # Close the browser
        driver.quit()

if __name__ == "__main__":
    # URL to scrape
    url = "https://hk.centanet.com/findproperty/centadata-details/%E5%A4%AA%E5%8F%A4%E5%9F%8E_3-OVDUURFSRJ"
    
    # Run the scraper
    scrape_centanet_property(url)
