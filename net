import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field, asdict
from datetime import datetime
import re
import json
from urllib.parse import urljoin, urlparse
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class PropertyMetrics:
    """Store property metrics with None defaults for missing data"""
    avg_price_per_sqft: Optional[float] = None
    avg_rent: Optional[float] = None
    avg_return: Optional[float] = None

@dataclass
class Transaction:
    """Store individual transaction data"""
    unit: str
    address: str
    sold_price: float
    registry_date: str
    saleable_area: int
    unit_price: float
    bedrooms: Optional[int] = None
    transaction_format: str = ""

@dataclass
class Property:
    """Main property data structure"""
    property_name: str
    street_num: str
    address: str
    detail_url: str
    metrics: PropertyMetrics = field(default_factory=PropertyMetrics)
    transactions: List[Transaction] = field(default_factory=list)

class HSE28Scraper:
    def __init__(self, max_pages: int = 5, max_concurrent_requests: int = 5):
        self.base_url = "https://www.28hse.com"
        self.max_pages = max_pages
        self.max_concurrent_requests = max_concurrent_requests
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore: Optional[asyncio.Semaphore] = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        self.semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """Fetch a page with rate limiting and error handling"""
        async with self.semaphore:
            try:
                async with self.session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        logger.error(f"Failed to fetch {url}: Status {response.status}")
                        return None
            except Exception as e:
                logger.error(f"Error fetching {url}: {str(e)}")
                return None
    
    def parse_number(self, text: str) -> Optional[float]:
        """Extract number from text, handling various formats"""
        if not text:
            return None
        
        # Remove currency symbols and common text
        text = re.sub(r'[HKD$,]', '', text)
        text = re.sub(r'Millions?', '000000', text, flags=re.IGNORECASE)
        
        # Find number pattern
        match = re.search(r'[\d.]+', text)
        if match:
            try:
                return float(match.group())
            except ValueError:
                return None
        return None
    
    async def parse_listing_page(self, html: str) -> List[str]:
        """Parse property URLs from listing page"""
        soup = BeautifulSoup(html, 'html.parser')
        property_urls = []
        
        # Find property links - look for estate detail links specifically
        property_links = soup.select('a[href*="/estate/detail/"]')
        
        # Also try alternative selectors if needed
        if not property_links:
            property_links = soup.select('a.estate-link, a.property-link, .estate-item a')
        
        for link in property_links:
            href = link.get('href')
            if href and '/estate/detail/' in href:
                # Skip transaction/photo/other sub-pages
                if any(sub in href for sub in ['/transaction', '/photo', '/floor-plan']):
                    continue
                    
                full_url = urljoin(self.base_url, href)
                if full_url not in property_urls:  # Avoid duplicates
                    property_urls.append(full_url)
        
        logger.info(f"Found {len(property_urls)} unique property URLs")
        return property_urls
    
    async def parse_property_details(self, html: str, url: str) -> Optional[Property]:
        """Parse property details from detail page"""
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # Extract property name - try multiple selectors
            property_name = ""
            name_selectors = ['h1', '.property-name', '.estate-name', '[class*="title"]']
            for selector in name_selectors:
                elem = soup.select_one(selector)
                if elem and elem.text.strip():
                    property_name = elem.text.strip()
                    break
            
            if not property_name:
                logger.warning(f"Could not find property name for {url}")
                return None
            
            # Extract street number from specific anchor tag
            street_num = ""
            street_elem = soup.select_one('a[href="#estate_id_map"]')
            if street_elem:
                street_num = street_elem.text.strip()
            
            # Extract address from breadcrumb items
            address = ""
            breadcrumb_items = soup.select('a[itemprop="item"] span[itemprop="name"]')
            if breadcrumb_items:
                # Get all breadcrumb names and join them
                address_parts = [item.text.strip() for item in breadcrumb_items if item.text.strip()]
                # Filter out generic items like "Home" or "Estate"
                filtered_parts = [part for part in address_parts if part not in ['Home', 'Estate', 'Property']]
                address = ' > '.join(filtered_parts)
            
            # Create property object
            property_obj = Property(
                property_name=property_name,
                street_num=street_num,
                address=address,
                detail_url=url
            )
            
            # Extract metrics
            metrics_data = {}
            
            # Look for metric patterns in the page
            text_content = soup.get_text()
            
            # Average price per sqft
            price_match = re.search(r'([\d,]+)\s*(?:Avg\.?\s*)?price\s*per\s*sqft', text_content, re.IGNORECASE)
            if price_match:
                metrics_data['avg_price_per_sqft'] = self.parse_number(price_match.group(1))
            
            # Average rent
            rent_match = re.search(r'([\d.]+)\s*(?:Avg\.?\s*)?rent', text_content, re.IGNORECASE)
            if rent_match:
                metrics_data['avg_rent'] = self.parse_number(rent_match.group(1))
            
            # Average return
            return_match = re.search(r'([\d.]+)%?\s*(?:Avg\.?\s*)?return', text_content, re.IGNORECASE)
            if return_match:
                metrics_data['avg_return'] = self.parse_number(return_match.group(1))
            
            property_obj.metrics = PropertyMetrics(**metrics_data)
            
            return property_obj
            
        except Exception as e:
            logger.error(f"Error parsing property details from {url}: {str(e)}")
            return None
    
    async def parse_transactions(self, html: str) -> List[Transaction]:
        """Parse transaction data from transaction page"""
        soup = BeautifulSoup(html, 'html.parser')
        transactions = []
        
        try:
            # Find all transaction blocks - try multiple approaches
            # Method 1: Look for transaction items with specific classes
            transaction_blocks = soup.select('.transaction-item, .record-item, [class*="transaction-record"], .estate-record-item')
            
            # Method 2: If no blocks found, try to find table rows
            if not transaction_blocks:
                # Look for transaction table
                transaction_table = soup.find('table', class_=re.compile('transaction|record'))
                if transaction_table:
                    # Skip header row(s)
                    transaction_blocks = transaction_table.find_all('tr')[1:]
            
            # Method 3: Look for divs containing transaction data patterns
            if not transaction_blocks:
                # Find all elements containing "Sold Price" text
                sold_price_elements = soup.find_all(text=re.compile(r'Sold\s+Price'))
                for elem in sold_price_elements:
                    # Get the parent container of this transaction
                    parent = elem.parent
                    while parent and parent.name not in ['div', 'tr', 'article', 'section']:
                        parent = parent.parent
                    if parent and parent not in transaction_blocks:
                        transaction_blocks.append(parent)
            
            logger.info(f"Found {len(transaction_blocks)} transaction blocks")
            
            for idx, block in enumerate(transaction_blocks):
                try:
                    # Get all text content from the block
                    text = ' '.join(block.stripped_strings) if hasattr(block, 'stripped_strings') else block.get_text(separator=' ', strip=True)
                    
                    # Skip if this doesn't look like a transaction
                    if 'Sold Price' not in text and 'HKD' not in text:
                        continue
                    
                    # Extract unit info - updated pattern to be more flexible
                    unit = ""
                    # Try multiple patterns
                    unit_patterns = [
                        r'(Flat\s+[A-Z0-9]+(?:/[A-Z0-9]+)?),?\s*([\d/]+F),?\s*(Tower|Block|Building)\s+([A-Z0-9]+(?:\s+[A-Z0-9]+)?)',
                        r'(Unit\s+[A-Z0-9]+),?\s*([\d/]+F)',
                        r'(Room\s+[A-Z0-9]+),?\s*([\d/]+F)'
                    ]
                    
                    for pattern in unit_patterns:
                        unit_match = re.search(pattern, text, re.IGNORECASE)
                        if unit_match:
                            unit = unit_match.group(0)
                            break
                    
                    # Extract address - look for text between unit and "Sold Price"
                    trans_address = ""
                    if unit:
                        addr_pattern = rf'{re.escape(unit)}\s+(.+?)\s+Sold\s+Price'
                        addr_match = re.search(addr_pattern, text, re.IGNORECASE | re.DOTALL)
                        if addr_match:
                            trans_address = addr_match.group(1).strip()
                    
                    # Extract sold price with better pattern
                    sold_price = 0
                    price_patterns = [
                        r'HKD?\s*\$?\s*([\d.]+)\s*Million',
                        r'Sold\s+Price[:\s]+HKD?\s*\$?\s*([\d.]+)\s*Million',
                        r'\$\s*([\d.]+)\s*M(?:illion)?'
                    ]
                    
                    for pattern in price_patterns:
                        price_match = re.search(pattern, text, re.IGNORECASE)
                        if price_match:
                            sold_price = float(price_match.group(1)) * 1000000
                            break
                    
                    # Extract registry date
                    registry_date = ""
                    date_patterns = [
                        r'Registry\s+Date[:\s]+([\d]{4}-[\d]{2}-[\d]{2})',
                        r'Date[:\s]+([\d]{4}-[\d]{2}-[\d]{2})',
                        r'([\d]{4}-[\d]{2}-[\d]{2})'
                    ]
                    
                    for pattern in date_patterns:
                        date_match = re.search(pattern, text)
                        if date_match:
                            registry_date = date_match.group(1)
                            break
                    
                    # Extract saleable area
                    saleable_area = 0
                    area_patterns = [
                        r'Saleable\s+area[:\s]+([\d,]+)\s*(?:sq\s*)?ft',
                        r'Area[:\s]+([\d,]+)\s*(?:sq\s*)?ft',
                        r'([\d,]+)\s*ftÂ²'
                    ]
                    
                    for pattern in area_patterns:
                        area_match = re.search(pattern, text, re.IGNORECASE)
                        if area_match:
                            saleable_area = int(area_match.group(1).replace(',', ''))
                            break
                    
                    # Extract unit price
                    unit_price = 0
                    unit_price_patterns = [
                        r'Unit\s+Price[:\s]+\$?\s*([\d,]+)',
                        r'Price\s+per\s+ft[:\s]+\$?\s*([\d,]+)',
                        r'\$\s*([\d,]+)\s*/\s*ft'
                    ]
                    
                    for pattern in unit_price_patterns:
                        price_match = re.search(pattern, text, re.IGNORECASE)
                        if price_match:
                            unit_price = float(price_match.group(1).replace(',', ''))
                            break
                    
                    # Extract bedrooms
                    bedrooms = None
                    bedroom_patterns = [
                        r'Bedroom[s]?[:\s]+(\d+)',
                        r'(\d+)\s*Bedroom',
                        r'(\d+)\s*Room[s]?'
                    ]
                    
                    for pattern in bedroom_patterns:
                        bedroom_match = re.search(pattern, text, re.IGNORECASE)
                        if bedroom_match:
                            bedrooms = int(bedroom_match.group(1))
                            break
                    
                    # Extract transaction format
                    transaction_format = "Land Registry"  # Default
                    format_match = re.search(r'Transaction\s+Format[:\s]+([^,\n]+)', text, re.IGNORECASE)
                    if format_match:
                        transaction_format = format_match.group(1).strip()
                    
                    # Only add if we have essential data
                    if sold_price > 0 or saleable_area > 0:
                        transaction = Transaction(
                            unit=unit or f"Transaction {idx + 1}",
                            address=trans_address,
                            sold_price=sold_price,
                            registry_date=registry_date,
                            saleable_area=saleable_area,
                            unit_price=unit_price,
                            bedrooms=bedrooms,
                            transaction_format=transaction_format
                        )
                        transactions.append(transaction)
                        logger.debug(f"Added transaction: {unit} - ${sold_price/1000000:.2f}M")
                        
                except Exception as e:
                    logger.warning(f"Error parsing transaction {idx}: {str(e)}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error parsing transactions: {str(e)}")
        
        logger.info(f"Successfully parsed {len(transactions)} transactions")
        return transactions
    
    async def scrape_property_with_transactions(self, property_url: str) -> Optional[Property]:
        """Scrape a property and its transactions"""
        # Fetch property details
        html = await self.fetch_page(property_url)
        if not html:
            return None
        
        property_obj = await self.parse_property_details(html, property_url)
        if not property_obj:
            return None
        
        # Extract property ID from URL to construct transaction URL
        property_id_match = re.search(r'/estate/detail/([^/]+?)(?:-(\d+))?/?$', property_url)
        if property_id_match:
            property_slug = property_id_match.group(1)
            property_id = property_id_match.group(2) or ""
            
            # Construct transaction URL
            if property_id:
                transaction_url = f"{self.base_url}/en/estate/detail/{property_slug}-{property_id}/transaction"
            else:
                transaction_url = f"{self.base_url}/en/estate/detail/{property_slug}/transaction"
            
            # Fetch transactions
            trans_html = await self.fetch_page(transaction_url)
            if trans_html:
                property_obj.transactions = await self.parse_transactions(trans_html)
                logger.info(f"Found {len(property_obj.transactions)} transactions for {property_obj.property_name}")
        
        return property_obj
    
    async def scrape_all(self) -> List[Property]:
        """Main scraping method"""
        all_properties = []
        
        # Scrape listing pages
        for page_num in range(1, self.max_pages + 1):
            if page_num == 1:
                list_url = f"{self.base_url}/en/estate/"
            else:
                list_url = f"{self.base_url}/en/estate/?page={page_num}"
            
            logger.info(f"Scraping listing page {page_num}")
            html = await self.fetch_page(list_url)
            
            if not html:
                logger.warning(f"Failed to fetch listing page {page_num}")
                continue
            
            property_urls = await self.parse_listing_page(html)
            logger.info(f"Found {len(property_urls)} properties on page {page_num}")
            
            if not property_urls:
                break  # No more properties
            
            # Scrape each property concurrently
            tasks = [self.scrape_property_with_transactions(url) for url in property_urls]
            results = await asyncio.gather(*tasks)
            
            # Filter out None results
            valid_properties = [p for p in results if p is not None]
            all_properties.extend(valid_properties)
            
            logger.info(f"Successfully scraped {len(valid_properties)} properties from page {page_num}")
        
        return all_properties

async def main():
    """Example usage"""
    # Set the number of pages to scrape
    MAX_PAGES = 2  # Adjust as needed
    
    async with HSE28Scraper(max_pages=MAX_PAGES, max_concurrent_requests=5) as scraper:
        properties = await scraper.scrape_all()
        
        # Convert to JSON-serializable format
        data = []
        for prop in properties:
            prop_dict = asdict(prop)
            data.append(prop_dict)
        
        # Save to JSON file
        with open('28hse_properties.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # Print summary
        print(f"\nScraped {len(properties)} properties")
        total_transactions = sum(len(p.transactions) for p in properties)
        print(f"Total transactions: {total_transactions}")
        
        # Print sample data with more details
        if properties:
            print(f"\nSample property:")
            print(f"Name: {properties[0].property_name}")
            print(f"Street: {properties[0].street_num}")
            print(f"Address: {properties[0].address}")
            print(f"Metrics: {properties[0].metrics}")
            print(f"Transactions: {len(properties[0].transactions)}")
            
            # Show first few transactions if available
            if properties[0].transactions:
                print(f"\nFirst 3 transactions:")
                for i, trans in enumerate(properties[0].transactions[:3]):
                    print(f"\n  Transaction {i+1}:")
                    print(f"  Unit: {trans.unit}")
                    print(f"  Price: HKD${trans.sold_price/1000000:.2f}M")
                    print(f"  Date: {trans.registry_date}")
                    print(f"  Area: {trans.saleable_area} sqft")
                    print(f"  Unit Price: ${trans.unit_price}")

if __name__ == "__main__":
    asyncio.run(main())
