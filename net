import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field, asdict
from datetime import datetime
import re
import json
from urllib.parse import urljoin, urlparse
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class PropertyMetrics:
    """Store property metrics with None defaults for missing data"""
    avg_price_per_sqft: Optional[float] = None
    avg_rent: Optional[float] = None
    avg_return: Optional[float] = None

@dataclass
class Transaction:
    """Store individual transaction data"""
    unit: str
    address: str
    sold_price: float
    registry_date: str
    saleable_area: int
    unit_price: float
    bedrooms: Optional[int] = None
    transaction_format: str = ""

@dataclass
class Property:
    """Main property data structure"""
    property_name: str
    street_num: str
    address: str
    detail_url: str
    metrics: PropertyMetrics = field(default_factory=PropertyMetrics)
    transactions: List[Transaction] = field(default_factory=list)

class HSE28Scraper:
    def __init__(self, max_pages: int = 5, max_concurrent_requests: int = 5):
        self.base_url = "https://www.28hse.com"
        self.max_pages = max_pages
        self.max_concurrent_requests = max_concurrent_requests
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore: Optional[asyncio.Semaphore] = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        self.semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """Fetch a page with rate limiting and error handling"""
        async with self.semaphore:
            try:
                async with self.session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        logger.error(f"Failed to fetch {url}: Status {response.status}")
                        return None
            except Exception as e:
                logger.error(f"Error fetching {url}: {str(e)}")
                return None
    
    def parse_number(self, text: str) -> Optional[float]:
        """Extract number from text, handling various formats"""
        if not text:
            return None
        
        # Remove currency symbols and common text
        text = re.sub(r'[HKD$,]', '', text)
        text = re.sub(r'Millions?', '000000', text, flags=re.IGNORECASE)
        
        # Find number pattern
        match = re.search(r'[\d.]+', text)
        if match:
            try:
                return float(match.group())
            except ValueError:
                return None
        return None
    
    async def parse_listing_page(self, html: str) -> List[str]:
        """Parse property URLs from listing page"""
        soup = BeautifulSoup(html, 'html.parser')
        property_urls = []
        
        # Find property links - adjust selector based on actual HTML structure
        property_links = soup.select('a[href*="/estate/detail/"]')
        
        for link in property_links:
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                if full_url not in property_urls:  # Avoid duplicates
                    property_urls.append(full_url)
        
        return property_urls
    
    async def parse_property_details(self, html: str, url: str) -> Optional[Property]:
        """Parse property details from detail page"""
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # Extract property name - try multiple selectors
            property_name = ""
            name_selectors = ['h1', '.property-name', '.estate-name', '[class*="title"]']
            for selector in name_selectors:
                elem = soup.select_one(selector)
                if elem and elem.text.strip():
                    property_name = elem.text.strip()
                    break
            
            if not property_name:
                logger.warning(f"Could not find property name for {url}")
                return None
            
            # Extract street number and address
            street_num = ""
            address = ""
            
            # Look for address information in various places
            address_selectors = ['.address', '.location', '[class*="address"]', '.estate-address']
            for selector in address_selectors:
                elem = soup.select_one(selector)
                if elem:
                    full_address = elem.text.strip()
                    # Try to extract street number (No.XX pattern)
                    street_match = re.search(r'No\.\s*\d+[A-Za-z]?\s+[\w\s]+Road', full_address)
                    if street_match:
                        street_num = street_match.group()
                    
                    # Clean up address
                    address = re.sub(r'No\.\s*\d+[A-Za-z]?\s+[\w\s]+Road\s*,?\s*', '', full_address)
                    address = address.strip(', ')
                    break
            
            # Create property object
            property_obj = Property(
                property_name=property_name,
                street_num=street_num,
                address=address,
                detail_url=url
            )
            
            # Extract metrics
            metrics_data = {}
            
            # Look for metric patterns in the page
            text_content = soup.get_text()
            
            # Average price per sqft
            price_match = re.search(r'([\d,]+)\s*(?:Avg\.?\s*)?price\s*per\s*sqft', text_content, re.IGNORECASE)
            if price_match:
                metrics_data['avg_price_per_sqft'] = self.parse_number(price_match.group(1))
            
            # Average rent
            rent_match = re.search(r'([\d.]+)\s*(?:Avg\.?\s*)?rent', text_content, re.IGNORECASE)
            if rent_match:
                metrics_data['avg_rent'] = self.parse_number(rent_match.group(1))
            
            # Average return
            return_match = re.search(r'([\d.]+)%?\s*(?:Avg\.?\s*)?return', text_content, re.IGNORECASE)
            if return_match:
                metrics_data['avg_return'] = self.parse_number(return_match.group(1))
            
            property_obj.metrics = PropertyMetrics(**metrics_data)
            
            return property_obj
            
        except Exception as e:
            logger.error(f"Error parsing property details from {url}: {str(e)}")
            return None
    
    async def parse_transactions(self, html: str) -> List[Transaction]:
        """Parse transaction data from transaction page"""
        soup = BeautifulSoup(html, 'html.parser')
        transactions = []
        
        try:
            # Find transaction entries - adjust selector based on actual HTML
            transaction_blocks = soup.select('.transaction-item, .record-item, [class*="transaction"]')
            
            for block in transaction_blocks:
                try:
                    text = block.get_text(separator=' ', strip=True)
                    
                    # Extract unit info
                    unit_match = re.search(r'(Flat\s+[A-Z0-9]+,\s*[\d/]+F,\s*Tower\s+[\w\s]+)', text)
                    unit = unit_match.group(1) if unit_match else ""
                    
                    # Extract address (after unit, before Sold Price)
                    addr_match = re.search(r'Tower\s+[\w\s]+\s+(.+?)\s+Sold\s+Price', text)
                    trans_address = addr_match.group(1) if addr_match else ""
                    
                    # Extract sold price
                    price_match = re.search(r'HKD?\$?([\d.]+)\s*Millions?', text, re.IGNORECASE)
                    sold_price = self.parse_number(price_match.group(0)) if price_match else 0
                    
                    # Extract registry date
                    date_match = re.search(r'Registry\s+Date:\s*([\d-]+)', text)
                    registry_date = date_match.group(1) if date_match else ""
                    
                    # Extract saleable area
                    area_match = re.search(r'Saleable\s+area:\s*([\d,]+)\s*ft', text, re.IGNORECASE)
                    saleable_area = int(self.parse_number(area_match.group(1))) if area_match else 0
                    
                    # Extract unit price
                    unit_price_match = re.search(r'Unit\s+Price:\s*\$?([\d,]+)', text, re.IGNORECASE)
                    unit_price = self.parse_number(unit_price_match.group(1)) if unit_price_match else 0
                    
                    # Extract bedrooms
                    bedroom_match = re.search(r'Bedroom:\s*(\d+)', text)
                    bedrooms = int(bedroom_match.group(1)) if bedroom_match else None
                    
                    # Extract transaction format
                    format_match = re.search(r'Transaction\s+Format:\s*(.+?)(?:\s|$)', text)
                    transaction_format = format_match.group(1) if format_match else "Land Registry"
                    
                    if sold_price > 0:  # Only add valid transactions
                        transaction = Transaction(
                            unit=unit,
                            address=trans_address,
                            sold_price=sold_price,
                            registry_date=registry_date,
                            saleable_area=saleable_area,
                            unit_price=unit_price,
                            bedrooms=bedrooms,
                            transaction_format=transaction_format
                        )
                        transactions.append(transaction)
                        
                except Exception as e:
                    logger.warning(f"Error parsing individual transaction: {str(e)}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error parsing transactions: {str(e)}")
        
        return transactions
    
    async def scrape_property_with_transactions(self, property_url: str) -> Optional[Property]:
        """Scrape a property and its transactions"""
        # Fetch property details
        html = await self.fetch_page(property_url)
        if not html:
            return None
        
        property_obj = await self.parse_property_details(html, property_url)
        if not property_obj:
            return None
        
        # Extract property ID from URL to construct transaction URL
        property_id_match = re.search(r'/estate/detail/([^/]+?)(?:-(\d+))?/?$', property_url)
        if property_id_match:
            property_slug = property_id_match.group(1)
            property_id = property_id_match.group(2) or ""
            
            # Construct transaction URL
            if property_id:
                transaction_url = f"{self.base_url}/en/estate/detail/{property_slug}-{property_id}/transaction"
            else:
                transaction_url = f"{self.base_url}/en/estate/detail/{property_slug}/transaction"
            
            # Fetch transactions
            trans_html = await self.fetch_page(transaction_url)
            if trans_html:
                property_obj.transactions = await self.parse_transactions(trans_html)
                logger.info(f"Found {len(property_obj.transactions)} transactions for {property_obj.property_name}")
        
        return property_obj
    
    async def scrape_all(self) -> List[Property]:
        """Main scraping method"""
        all_properties = []
        
        # Scrape listing pages
        for page_num in range(1, self.max_pages + 1):
            if page_num == 1:
                list_url = f"{self.base_url}/en/estate/"
            else:
                list_url = f"{self.base_url}/en/estate/?page={page_num}"
            
            logger.info(f"Scraping listing page {page_num}")
            html = await self.fetch_page(list_url)
            
            if not html:
                logger.warning(f"Failed to fetch listing page {page_num}")
                continue
            
            property_urls = await self.parse_listing_page(html)
            logger.info(f"Found {len(property_urls)} properties on page {page_num}")
            
            if not property_urls:
                break  # No more properties
            
            # Scrape each property concurrently
            tasks = [self.scrape_property_with_transactions(url) for url in property_urls]
            results = await asyncio.gather(*tasks)
            
            # Filter out None results
            valid_properties = [p for p in results if p is not None]
            all_properties.extend(valid_properties)
            
            logger.info(f"Successfully scraped {len(valid_properties)} properties from page {page_num}")
        
        return all_properties

async def main():
    """Example usage"""
    # Set the number of pages to scrape
    MAX_PAGES = 2  # Adjust as needed
    
    async with HSE28Scraper(max_pages=MAX_PAGES, max_concurrent_requests=5) as scraper:
        properties = await scraper.scrape_all()
        
        # Convert to JSON-serializable format
        data = []
        for prop in properties:
            prop_dict = asdict(prop)
            data.append(prop_dict)
        
        # Save to JSON file
        with open('28hse_properties.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # Print summary
        print(f"\nScraped {len(properties)} properties")
        total_transactions = sum(len(p.transactions) for p in properties)
        print(f"Total transactions: {total_transactions}")
        
        # Print sample data
        if properties:
            print(f"\nSample property:")
            print(f"Name: {properties[0].property_name}")
            print(f"Address: {properties[0].street_num}, {properties[0].address}")
            print(f"Metrics: {properties[0].metrics}")
            print(f"Transactions: {len(properties[0].transactions)}")

if __name__ == "__main__":
    asyncio.run(main())
