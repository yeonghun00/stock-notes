import asyncio
import aiohttp
from aiohttp import ClientSession, ClientTimeout, TCPConnector
from bs4 import BeautifulSoup
import json
import time
from urllib.parse import urljoin, urlparse
from datetime import datetime
import logging
from typing import Dict, List, Optional, Union
import re
from dataclasses import dataclass, asdict
import csv
import ssl
import certifi

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class PropertyMetrics:
    """Data class for property metrics"""
    avg_price_per_sqft: Optional[str] = None
    avg_rent: Optional[str] = None
    avg_return: Optional[str] = None

@dataclass
class Transaction:
    """Data class for property transactions"""
    property_name: str
    unit_details: str
    address: str
    sold_price: Optional[str] = None
    registry_date: Optional[str] = None
    saleable_area: Optional[str] = None
    unit_price: Optional[str] = None
    bedroom: Optional[str] = None
    transaction_format: Optional[str] = None

@dataclass
class Property:
    """Data class for property information"""
    property_name: str
    street_num: str
    address: str
    metrics: PropertyMetrics
    transactions: List[Transaction]
    property_url: str
    scraped_at: str

class AsyncHSE28Scraper:
    def __init__(self, base_url="https://www.28hse.com", 
                 max_concurrent=10, 
                 delay=0.5,
                 proxy=None,
                 proxy_auth=None,
                 timeout=30):
        """
        Initialize async scraper
        
        Args:
            base_url: Base URL of the website
            max_concurrent: Maximum concurrent requests
            delay: Delay between requests in seconds
            proxy: Proxy URL (e.g., 'http://proxy.company.com:8080')
            proxy_auth: Tuple of (username, password) for proxy authentication
            timeout: Request timeout in seconds
        """
        self.base_url = base_url
        self.max_concurrent = max_concurrent
        self.delay = delay
        self.proxy = proxy
        self.proxy_auth = proxy_auth
        self.timeout = ClientTimeout(total=timeout)
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.session = None
        
        # Headers
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        
    async def create_session(self):
        """Create aiohttp session with proxy support"""
        # SSL context for corporate proxies
        ssl_context = ssl.create_default_context(cafile=certifi.where())
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # Connector with connection pooling
        connector = TCPConnector(
            limit=100,
            limit_per_host=30,
            ssl=ssl_context,
            force_close=True
        )
        
        # Proxy authentication
        auth = None
        if self.proxy_auth:
            auth = aiohttp.BasicAuth(self.proxy_auth[0], self.proxy_auth[1])
        
        self.session = ClientSession(
            connector=connector,
            timeout=self.timeout,
            headers=self.headers,
            auth=auth
        )
    
    async def close_session(self):
        """Close aiohttp session"""
        if self.session:
            await self.session.close()
    
    async def fetch_with_retry(self, url: str, retries: int = 3) -> Optional[str]:
        """Fetch URL with retry logic and rate limiting"""
        async with self.semaphore:
            for attempt in range(retries):
                try:
                    kwargs = {}
                    if self.proxy:
                        kwargs['proxy'] = self.proxy
                    
                    async with self.session.get(url, **kwargs) as response:
                        response.raise_for_status()
                        html = await response.text()
                        
                        # Rate limiting
                        await asyncio.sleep(self.delay)
                        
                        return html
                        
                except aiohttp.ClientError as e:
                    logger.warning(f"Attempt {attempt + 1} failed for {url}: {str(e)}")
                    if attempt < retries - 1:
                        await asyncio.sleep(self.delay * (attempt + 1))
                except Exception as e:
                    logger.error(f"Unexpected error for {url}: {str(e)}")
                    if attempt < retries - 1:
                        await asyncio.sleep(self.delay * (attempt + 1))
        
        logger.error(f"Failed to fetch {url} after {retries} attempts")
        return None
    
    async def get_soup(self, url: str) -> Optional[BeautifulSoup]:
        """Get BeautifulSoup object from URL"""
        html = await self.fetch_with_retry(url)
        if html:
            return BeautifulSoup(html, 'html.parser')
        return None
    
    async def extract_property_list(self, list_url: str) -> List[str]:
        """Extract property URLs from the listing page"""
        soup = await self.get_soup(list_url)
        if not soup:
            logger.error(f"Failed to fetch listing page: {list_url}")
            return []
        
        property_urls = []
        # Look for property links - adjust selector based on actual HTML structure
        property_links = soup.find_all('a', href=re.compile(r'/en/estate/detail/'))
        
        for link in property_links:
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                if full_url not in property_urls:
                    property_urls.append(full_url)
        
        logger.info(f"Found {len(property_urls)} properties")
        return property_urls
    
    def extract_property_metrics(self, soup: BeautifulSoup) -> PropertyMetrics:
        """Extract property metrics from the page"""
        metrics = PropertyMetrics()
        
        # Look for metric containers - these selectors need to be adjusted based on actual HTML
        metric_containers = soup.find_all(['div', 'span'], class_=re.compile(r'metric|stat|price'))
        
        for container in metric_containers:
            text = container.get_text(strip=True).lower()
            
            # Extract average price per sqft
            if 'avg. price per sqft' in text or 'average price' in text:
                value = self.extract_metric_value(container)
                if value:
                    metrics.avg_price_per_sqft = value
            
            # Extract average rent
            elif 'avg. rent' in text or 'average rent' in text:
                value = self.extract_metric_value(container)
                if value:
                    metrics.avg_rent = value
            
            # Extract average return
            elif 'avg. return' in text or 'average return' in text:
                value = self.extract_metric_value(container)
                if value:
                    metrics.avg_return = value
        
        return metrics
    
    def extract_metric_value(self, container) -> Optional[str]:
        """Extract numeric value from metric container"""
        text = container.get_text(strip=True)
        
        # Extract numbers with units (e.g., "11,849", "36.3", "3.42%")
        match = re.search(r'[\d,]+\.?\d*%?', text)
        if match:
            return match.group()
        
        # Check next sibling
        next_elem = container.find_next_sibling()
        if next_elem:
            match = re.search(r'[\d,]+\.?\d*%?', next_elem.get_text(strip=True))
            if match:
                return match.group()
        
        return None
    
    async def extract_property_details(self, url: str) -> Optional[Property]:
        """Extract property details from property page"""
        soup = await self.get_soup(url)
        if not soup:
            logger.error(f"Failed to fetch property page: {url}")
            return None
        
        try:
            # Extract property name
            property_name = ""
            name_elem = soup.find(['h1', 'h2'], class_=re.compile(r'property|title|name'))
            if name_elem:
                property_name = name_elem.get_text(strip=True)
            
            # Extract street number
            street_num = ""
            street_elem = soup.find(text=re.compile(r'No\.\s*\d+'))
            if street_elem:
                street_num = street_elem.strip()
            
            # Extract address
            address = ""
            address_elem = soup.find(['div', 'span'], class_=re.compile(r'address|location'))
            if address_elem:
                address = address_elem.get_text(strip=True)
            
            # Extract metrics
            metrics = self.extract_property_metrics(soup)
            
            # Get transactions
            transactions = await self.extract_transactions(url, property_name)
            
            property_obj = Property(
                property_name=property_name,
                street_num=street_num,
                address=address,
                metrics=metrics,
                transactions=transactions,
                property_url=url,
                scraped_at=datetime.now().isoformat()
            )
            
            logger.info(f"Successfully scraped: {property_name}")
            return property_obj
            
        except Exception as e:
            logger.error(f"Error extracting property details from {url}: {str(e)}")
            return None
    
    async def extract_transactions(self, property_url: str, property_name: str) -> List[Transaction]:
        """Extract transactions from property transaction page"""
        # Construct transaction URL
        transaction_url = property_url.rstrip('/') + '/transaction'
        
        soup = await self.get_soup(transaction_url)
        if not soup:
            logger.warning(f"No transaction page found for {property_name}")
            return []
        
        transactions = []
        
        # Look for transaction entries - adjust selector based on actual HTML
        transaction_elements = soup.find_all(['div', 'tr'], class_=re.compile(r'transaction|record'))
        
        for elem in transaction_elements:
            try:
                transaction = self.parse_transaction(elem, property_name)
                if transaction:
                    transactions.append(transaction)
            except Exception as e:
                logger.warning(f"Error parsing transaction: {str(e)}")
        
        logger.info(f"Found {len(transactions)} transactions for {property_name}")
        return transactions
    
    def parse_transaction(self, elem, property_name: str) -> Optional[Transaction]:
        """Parse individual transaction element"""
        text = elem.get_text(separator=' ', strip=True)
        
        transaction = Transaction(
            property_name=property_name,
            unit_details="",
            address=""
        )
        
        # Extract unit details (e.g., "Flat B6, 25/F, Tower 3b")
        unit_match = re.search(r'(Flat|Unit)\s+[A-Z0-9]+.*?(?=Tin|Sold|$)', text)
        if unit_match:
            transaction.unit_details = unit_match.group().strip()
        
        # Extract address
        addr_match = re.search(r'(Tin.*?Road|[A-Z][a-z]+.*?Road)', text)
        if addr_match:
            transaction.address = addr_match.group().strip()
        
        # Extract sold price
        price_match = re.search(r'Sold Price:?\s*HKD?\$?([\d.]+)\s*Million', text, re.IGNORECASE)
        if price_match:
            transaction.sold_price = f"HKD${price_match.group(1)} Million"
        
        # Extract registry date
        date_match = re.search(r'Registry Date:?\s*([\d-]+)', text)
        if date_match:
            transaction.registry_date = date_match.group(1)
        
        # Extract saleable area
        area_match = re.search(r'Saleable area:?\s*([\d,]+)\s*ft', text)
        if area_match:
            transaction.saleable_area = f"{area_match.group(1)}ftÂ²"
        
        # Extract unit price
        unit_price_match = re.search(r'Unit Price:?\s*\$?([\d,]+)', text)
        if unit_price_match:
            transaction.unit_price = f"${unit_price_match.group(1)}"
        
        # Extract bedroom info
        bedroom_match = re.search(r'Bedroom:?\s*(\d+)\s*Room', text)
        if bedroom_match:
            transaction.bedroom = f"{bedroom_match.group(1)} Rooms"
        
        # Extract transaction format
        format_match = re.search(r'Transaction Format:?\s*([^$]+?)(?=\s*$)', text)
        if format_match:
            transaction.transaction_format = format_match.group(1).strip()
        
        return transaction if any([transaction.sold_price, transaction.registry_date]) else None
    
    async def scrape_all_properties(self, list_url: str) -> List[Property]:
        """Scrape all properties from the listing page"""
        await self.create_session()
        
        try:
            property_urls = await self.extract_property_list(list_url)
            
            if not property_urls:
                logger.error("No properties found to scrape")
                return []
            
            # Process properties concurrently
            tasks = [self.extract_property_details(url) for url in property_urls]
            properties = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Filter out None values and exceptions
            valid_properties = []
            for prop in properties:
                if isinstance(prop, Property):
                    valid_properties.append(prop)
                elif isinstance(prop, Exception):
                    logger.error(f"Error during scraping: {str(prop)}")
            
            return valid_properties
            
        finally:
            await self.close_session()
    
    def save_to_json(self, properties: List[Property], filename: str = "properties.json"):
        """Save properties to JSON file"""
        data = [asdict(prop) for prop in properties]
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"Saved {len(properties)} properties to {filename}")
    
    def save_to_csv(self, properties: List[Property], filename: str = "properties.csv"):
        """Save properties to CSV file"""
        if not properties:
            return
        
        # Flatten data for CSV
        rows = []
        for prop in properties:
            base_data = {
                'property_name': prop.property_name,
                'street_num': prop.street_num,
                'address': prop.address,
                'avg_price_per_sqft': prop.metrics.avg_price_per_sqft,
                'avg_rent': prop.metrics.avg_rent,
                'avg_return': prop.metrics.avg_return,
                'property_url': prop.property_url,
                'scraped_at': prop.scraped_at,
                'transaction_count': len(prop.transactions)
            }
            rows.append(base_data)
        
        # Write properties CSV
        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=rows[0].keys())
            writer.writeheader()
            writer.writerows(rows)
        
        # Write transactions CSV
        trans_filename = filename.replace('.csv', '_transactions.csv')
        trans_rows = []
        for prop in properties:
            for trans in prop.transactions:
                trans_data = asdict(trans)
                trans_rows.append(trans_data)
        
        if trans_rows:
            with open(trans_filename, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=trans_rows[0].keys())
                writer.writeheader()
                writer.writerows(trans_rows)
        
        logger.info(f"Saved data to {filename} and {trans_filename}")

async def main():
    # Configuration with proxy support
    config = {
        'max_concurrent': 10,  # Higher concurrency with async
        'delay': 0.5,          # Shorter delay possible with async
        'timeout': 30,
        
        # Proxy configuration (uncomment and modify as needed)
        # 'proxy': 'http://proxy.company.com:8080',
        # 'proxy_auth': ('username', 'password'),  # If authentication required
    }
    
    # Initialize scraper
    scraper = AsyncHSE28Scraper(**config)
    
    # URL to scrape
    list_url = "https://www.28hse.com/en/estate/"
    
    # Scrape all properties
    logger.info("Starting async scraping process...")
    start_time = time.time()
    
    properties = await scraper.scrape_all_properties(list_url)
    
    elapsed_time = time.time() - start_time
    
    # Save results
    if properties:
        scraper.save_to_json(properties, "28hse_properties.json")
        scraper.save_to_csv(properties, "28hse_properties.csv")
        
        # Print summary
        total_transactions = sum(len(p.transactions) for p in properties)
        logger.info(f"\nScraping completed in {elapsed_time:.2f} seconds!")
        logger.info(f"Total properties: {len(properties)}")
        logger.info(f"Total transactions: {total_transactions}")
        logger.info(f"Average time per property: {elapsed_time/len(properties):.2f} seconds")
    else:
        logger.error("No properties were scraped successfully")

# Example usage with different proxy configurations
async def example_with_proxy():
    # HTTP proxy without authentication
    scraper1 = AsyncHSE28Scraper(
        proxy='http://proxy.company.com:8080',
        max_concurrent=5
    )
    
    # HTTP proxy with authentication
    scraper2 = AsyncHSE28Scraper(
        proxy='http://proxy.company.com:8080',
        proxy_auth=('username', 'password'),
        max_concurrent=5
    )
    
    # SOCKS proxy (requires aiohttp-socks)
    # scraper3 = AsyncHSE28Scraper(
    #     proxy='socks5://proxy.company.com:1080',
    #     proxy_auth=('username', 'password')
    # )
    
    list_url = "https://www.28hse.com/en/estate/"
    properties = await scraper1.scrape_all_properties(list_url)
    return properties

if __name__ == "__main__":
    # Install required packages:
    # pip install aiohttp beautifulsoup4 certifi
    
    # Run the main scraper
    asyncio.run(main())
    
    # Or run with proxy
    # asyncio.run(example_with_proxy())
