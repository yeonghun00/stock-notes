import asyncio
import aiohttp
import feedparser
import json
import csv
import logging
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any, Tuple

# --- Configuration ---
# List of keywords you want to search for.
KEYWORDS: List[str] = [
    "geopolitical risk",
    "supply chain disruption",
    "quantum computing breakthrough",
    "AI regulation",
    "carbon capture technology"
]

# Google News RSS feed URL template.
# %s will be replaced with the URL-encoded keyword.
# when:3d searches for news in the last 3 days.
GOOGLE_NEWS_URL_TEMPLATE: str = "https://news.google.com/rss/search?q=%s+when:3d&hl=en-US&gl=US&ceid=US:en"

# --- Output Files ---
JSON_OUTPUT_FILE: str = "news_results.json"
CSV_OUTPUT_FILE: str = "news_results.csv"

# --- Advanced Configuration ---
# Set a User-Agent to mimic a real browser, reducing the chance of being blocked.
HEADERS: Dict[str, str] = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
# Timeout for each HTTP request in seconds.
REQUEST_TIMEOUT: int = 15
# Configure logging to see progress and errors.
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


async def fetch_url_content(session: aiohttp.ClientSession, url: str) -> Optional[Tuple[str, str]]:
    """
    Asynchronously fetches content from a given URL, handling redirects.

    Args:
        session: The aiohttp client session.
        url: The initial URL to fetch.

    Returns:
        A tuple containing the final URL (after redirects) and the text content,
        or None if an error occurs.
    """
    try:
        async with session.get(url, timeout=REQUEST_TIMEOUT, headers=HEADERS, allow_redirects=True) as response:
            # Raise an exception for bad status codes (4xx or 5xx).
            response.raise_for_status()
            # The 'response.url' attribute holds the final URL after redirects.
            final_url = str(response.url)
            logging.info(f"Successfully fetched {final_url} (from initial: {url})")
            content = await response.text()
            return final_url, content
    except aiohttp.ClientError as e:
        logging.error(f"Aiohttp client error fetching {url}: {e}")
    except asyncio.TimeoutError:
        logging.error(f"Timeout error fetching {url}")
    except Exception as e:
        logging.error(f"An unexpected error occurred fetching {url}: {e}")
    return None

async def verify_article_match(
    session: aiohttp.ClientSession,
    entry: Dict[str, Any],
    keyword: str
) -> Optional[Dict[str, str]]:
    """
    Visits a single article page and checks if the keyword exists in its content.
    It now saves the final, redirected URL.

    Args:
        session: The aiohttp client session.
        entry: A parsed feed entry from feedparser.
        keyword: The keyword to search for within the article content.

    Returns:
        A dictionary with article details if the keyword is found, otherwise None.
    """
    original_article_url = entry.get("link")
    if not original_article_url:
        return None

    # fetch_url_content now returns a tuple: (final_url, content)
    fetched_data = await fetch_url_content(session, original_article_url)
    if not fetched_data:
        return None
    
    final_article_url, html_content = fetched_data # Unpack the tuple

    try:
        soup = BeautifulSoup(html_content, 'lxml')
        # Some pages might not have a body, so we check for it.
        if soup.body is None:
            logging.warning(f"No <body> tag found in {final_article_url}, skipping.")
            return None
            
        body_text = soup.body.get_text().lower()
        if keyword.lower() in body_text:
            logging.info(f"MATCH FOUND for '{keyword}' in: {final_article_url}")
            return {
                "keyword": keyword,
                "title": entry.get("title", "N/A"),
                "link": final_article_url,  # <-- CORRECTED: Using the final URL now
                "published": entry.get("published", "N/A"),
                "source": entry.get("source", {}).get("title", "N/A"),
            }
    except Exception as e:
        logging.error(f"Error parsing article {final_article_url}: {e}")

    return None

async def process_keyword(session: aiohttp.ClientSession, keyword: str) -> List[Dict[str, str]]:
    """
    Processes a single keyword: fetches the Google News RSS feed and
    verifies each article for a keyword match concurrently.

    Args:
        session: The aiohttp client session.
        keyword: The keyword to process.

    Returns:
        A list of verified articles that contain the keyword.
    """
    logging.info(f"--- Processing keyword: '{keyword}' ---")
    # URL-encode the keyword for the query parameter
    search_url = GOOGLE_NEWS_URL_TEMPLATE % keyword.replace(" ", "%20")
    
    fetched_data = await fetch_url_content(session, search_url)
    if not fetched_data:
        return []
    
    # For the RSS feed, we only need the content, not the final URL.
    _rss_url, rss_content = fetched_data

    # feedparser handles the XML parsing
    feed = feedparser.parse(rss_content)
    if not feed.entries:
        logging.warning(f"No articles found in RSS feed for keyword: '{keyword}'")
        return []

    # Create a list of concurrent tasks to verify each article
    tasks = [verify_article_match(session, entry, keyword) for entry in feed.entries]
    
    # Run all verification tasks concurrently and wait for them to complete
    verified_articles_results = await asyncio.gather(*tasks)
    
    # Filter out the None results (where no match was found)
    return [article for article in verified_articles_results if article]

def save_as_json(data: List[Dict[str, str]], filename: str):
    """Saves the final data to a JSON file."""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    logging.info(f"Successfully saved {len(data)} articles to {filename}")

def save_as_csv(data: List[Dict[str, str]], filename: str):
    """Saves the final data to a CSV file."""
    if not data:
        logging.warning("No data to save to CSV.")
        return
        
    # Use the keys from the first dictionary as headers
    headers = data[0].keys()
    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=headers)
        writer.writeheader()
        writer.writerows(data)
    logging.info(f"Successfully saved {len(data)} articles to {filename}")


async def main():
    """
    Main asynchronous function to orchestrate the entire process.
    """
    all_found_articles = []
    # aiohttp.ClientSession is used to manage connections efficiently
    async with aiohttp.ClientSession() as session:
        # Create a list of tasks for processing each keyword
        keyword_tasks = [process_keyword(session, keyword) for keyword in KEYWORDS]
        # Run all keyword processing tasks concurrently
        results_per_keyword = await asyncio.gather(*keyword_tasks)
        
        # Flatten the list of lists into a single list of articles
        all_found_articles = [article for sublist in results_per_keyword for article in sublist]

    if all_found_articles:
        # Save the results to the specified file formats
        save_as_json(all_found_articles, JSON_OUTPUT_FILE)
        save_as_csv(all_found_articles, CSV_OUTPUT_FILE)
    else:
        logging.info("--- No matching articles found for any keywords. ---")

if __name__ == "__main__":
    # This check ensures the code runs only when the script is executed directly
    # It's standard practice in Python.
    try:
        # On Windows, you might need to set a different event loop policy
        # for asyncio to work smoothly with aiohttp.
        # asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(main())
    except Exception as e:
        logging.critical(f"A critical error occurred in the main execution block: {e}")
