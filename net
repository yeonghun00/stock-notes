import asyncio
import aiohttp
from bs4 import BeautifulSoup
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field, asdict
from datetime import datetime
import re
import json
from urllib.parse import urljoin, urlparse
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class PropertyMetrics:
    """Store property metrics with None defaults for missing data"""
    avg_price_per_sqft: Optional[float] = None
    avg_rent: Optional[float] = None
    avg_return: Optional[float] = None

@dataclass
class Transaction:
    """Store individual transaction data"""
    unit: str
    address: str
    sold_price: float
    registry_date: str
    saleable_area: int
    unit_price: float
    bedrooms: Optional[int] = None
    transaction_format: str = ""

@dataclass
class Property:
    """Main property data structure"""
    property_name: str
    street_num: str
    address: str
    detail_url: str
    metrics: PropertyMetrics = field(default_factory=PropertyMetrics)
    transactions: List[Transaction] = field(default_factory=list)

class HSE28Scraper:
    def __init__(self, max_pages: int = 5, max_concurrent_requests: int = 5):
        """
        Initialize the scraper
        
        Args:
            max_pages: Maximum pages to scrape (applies to both property listings AND transaction listings)
            max_concurrent_requests: Maximum concurrent HTTP requests
        """
        self.base_url = "https://www.28hse.com"
        self.max_pages = max_pages
        self.max_concurrent_requests = max_concurrent_requests
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore: Optional[asyncio.Semaphore] = None
        
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession()
        self.semaphore = asyncio.Semaphore(self.max_concurrent_requests)
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """Fetch a page with rate limiting and error handling"""
        async with self.semaphore:
            try:
                async with self.session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        logger.error(f"Failed to fetch {url}: Status {response.status}")
                        return None
            except Exception as e:
                logger.error(f"Error fetching {url}: {str(e)}")
                return None
    
    def parse_number(self, text: str) -> Optional[float]:
        """Extract number from text, handling various formats"""
        if not text:
            return None
        
        # Remove currency symbols and common text
        text = re.sub(r'[HKD$,]', '', text)
        text = re.sub(r'Millions?', '000000', text, flags=re.IGNORECASE)
        
        # Find number pattern
        match = re.search(r'[\d.]+', text)
        if match:
            try:
                return float(match.group())
            except ValueError:
                return None
        return None
    
    async def parse_listing_page(self, html: str) -> List[str]:
        """Parse property URLs from listing page"""
        soup = BeautifulSoup(html, 'html.parser')
        property_urls = []
        
        # Find property links - look for estate detail links specifically
        property_links = soup.select('a[href*="/estate/detail/"]')
        
        # Also try alternative selectors if needed
        if not property_links:
            property_links = soup.select('a.estate-link, a.property-link, .estate-item a')
        
        for link in property_links:
            href = link.get('href')
            if href and '/estate/detail/' in href:
                # Skip transaction/photo/other sub-pages
                if any(sub in href for sub in ['/transaction', '/photo', '/floor-plan']):
                    continue
                    
                full_url = urljoin(self.base_url, href)
                if full_url not in property_urls:  # Avoid duplicates
                    property_urls.append(full_url)
        
        logger.info(f"Found {len(property_urls)} unique property URLs")
        return property_urls
    
    async def parse_property_details(self, html: str, url: str) -> Optional[Property]:
        """Parse property details from detail page"""
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # Extract property name - try multiple selectors
            property_name = ""
            name_selectors = ['h1', '.property-name', '.estate-name', '[class*="title"]']
            for selector in name_selectors:
                elem = soup.select_one(selector)
                if elem and elem.text.strip():
                    property_name = elem.text.strip()
                    break
            
            if not property_name:
                logger.warning(f"Could not find property name for {url}")
                return None
            
            # Extract street number from specific anchor tag
            street_num = ""
            street_elem = soup.select_one('a[href="#estate_id_map"]')
            if street_elem:
                street_num = street_elem.text.strip()
            
            # Extract address from breadcrumb items
            address = ""
            breadcrumb_items = soup.select('a[itemprop="item"] span[itemprop="name"]')
            if breadcrumb_items:
                # Get all breadcrumb names and join them
                address_parts = [item.text.strip() for item in breadcrumb_items if item.text.strip()]
                # Filter out generic items like "Home" or "Estate"
                filtered_parts = [part for part in address_parts if part not in ['Home', 'Estate', 'Property']]
                address = ' > '.join(filtered_parts)
            
            # Create property object
            property_obj = Property(
                property_name=property_name,
                street_num=street_num,
                address=address,
                detail_url=url
            )
            
            # Extract metrics
            metrics_data = {}
            
            # Look for metric patterns in the page
            text_content = soup.get_text()
            
            # Average price per sqft
            price_match = re.search(r'([\d,]+)\s*(?:Avg\.?\s*)?price\s*per\s*sqft', text_content, re.IGNORECASE)
            if price_match:
                metrics_data['avg_price_per_sqft'] = self.parse_number(price_match.group(1))
            
            # Average rent
            rent_match = re.search(r'([\d.]+)\s*(?:Avg\.?\s*)?rent', text_content, re.IGNORECASE)
            if rent_match:
                metrics_data['avg_rent'] = self.parse_number(rent_match.group(1))
            
            # Average return
            return_match = re.search(r'([\d.]+)%?\s*(?:Avg\.?\s*)?return', text_content, re.IGNORECASE)
            if return_match:
                metrics_data['avg_return'] = self.parse_number(return_match.group(1))
            
            property_obj.metrics = PropertyMetrics(**metrics_data)
            
            return property_obj
            
        except Exception as e:
            logger.error(f"Error parsing property details from {url}: {str(e)}")
            return None
    
    async def parse_transaction_listing_page(self, html: str) -> List[str]:
        """Parse individual transaction URLs from transaction listing page"""
        soup = BeautifulSoup(html, 'html.parser')
        transaction_urls = []
        
        # Find all transaction detail links
        detail_links = soup.select('a.detail_page[href*="/transaction/deal-"]')
        
        for link in detail_links:
            href = link.get('href')
            if href:
                full_url = urljoin(self.base_url, href)
                if full_url not in transaction_urls:
                    transaction_urls.append(full_url)
        
        logger.info(f"Found {len(transaction_urls)} transaction URLs on this page")
        return transaction_urls
    
    async def parse_single_transaction(self, html: str, url: str) -> Optional[Transaction]:
        """Parse a single transaction from its detail page"""
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # Extract property and unit info from title/header
            unit = ""
            header = soup.find('h1') or soup.find('title')
            if header:
                header_text = header.get_text(strip=True)
                # Extract unit info from header
                unit_match = re.search(r'(Flat|Unit|Room)\s+[A-Z0-9]+[,\s]+[\d/]+F[,\s]+(Tower|Block|Building)?\s*[A-Z0-9]*', header_text, re.IGNORECASE)
                if unit_match:
                    unit = unit_match.group(0).strip()
            
            # Initialize transaction data
            trans_data = {
                'unit': unit,
                'address': "",
                'sold_price': 0,
                'registry_date': "",
                'saleable_area': 0,
                'unit_price': 0,
                'bedrooms': None,
                'transaction_format': "Land Registry"
            }
            
            # Find all table cells with labels
            info_table = soup.find('table', class_=re.compile('info|detail|transaction'))
            if not info_table:
                # Try to find any table
                info_table = soup.find('table')
            
            if info_table:
                rows = info_table.find_all('tr')
                for row in rows:
                    cells = row.find_all(['td', 'th'])
                    if len(cells) >= 2:
                        label = cells[0].get_text(strip=True)
                        value = cells[1].get_text(strip=True)
                        
                        # Extract based on label
                        if 'Address' in label or 'Location' in label:
                            trans_data['address'] = value
                        elif 'Sold Price' in label:
                            # Parse price
                            price_match = re.search(r'([\d.]+)\s*M(?:illion)?', value, re.IGNORECASE)
                            if price_match:
                                trans_data['sold_price'] = float(price_match.group(1)) * 1000000
                            else:
                                # Try regular number
                                num_match = re.search(r'[\d,]+', value)
                                if num_match:
                                    trans_data['sold_price'] = float(num_match.group(0).replace(',', ''))
                        elif 'Registry Date' in label:
                            trans_data['registry_date'] = value
                        elif 'Saleable area' in label:
                            area_match = re.search(r'([\d,]+)', value)
                            if area_match:
                                trans_data['saleable_area'] = int(area_match.group(1).replace(',', ''))
                        elif 'Unit Price' in label and 'Building' not in label:
                            price_match = re.search(r'([\d,]+)', value)
                            if price_match:
                                trans_data['unit_price'] = float(price_match.group(1).replace(',', ''))
                        elif 'Bedroom' in label:
                            bed_match = re.search(r'(\d+)', value)
                            if bed_match:
                                trans_data['bedrooms'] = int(bed_match.group(1))
                        elif 'Transaction Format' in label:
                            trans_data['transaction_format'] = value
            
            # Only create transaction if we have essential data
            if trans_data['sold_price'] > 0 or trans_data['saleable_area'] > 0:
                return Transaction(**trans_data)
            else:
                logger.warning(f"No valid transaction data found at {url}")
                return None
                
        except Exception as e:
            logger.error(f"Error parsing transaction from {url}: {str(e)}")
            return None
    
    async def scrape_all_transactions(self, transaction_base_url: str, max_pages: int = None) -> List[Transaction]:
        """Scrape all transactions for a property with pagination"""
        all_transactions = []
        all_transaction_urls = []
        page_limit = max_pages or self.max_pages
        
        # Scrape transaction listing pages
        for page_num in range(1, page_limit + 1):
            if page_num == 1:
                list_url = transaction_base_url
            else:
                list_url = f"{transaction_base_url}?page={page_num}"
            
            logger.info(f"Fetching transaction listing page {page_num}")
            html = await self.fetch_page(list_url)
            
            if not html:
                logger.warning(f"Failed to fetch transaction listing page {page_num}")
                break
            
            # Get transaction URLs from this page
            transaction_urls = await self.parse_transaction_listing_page(html)
            
            if not transaction_urls:
                logger.info("No more transactions found")
                break
                
            all_transaction_urls.extend(transaction_urls)
            
            # If user didn't set a limit, check if there's a next page
            if max_pages is None:
                soup = BeautifulSoup(html, 'html.parser')
                next_link = soup.find('a', text=re.compile('Next|›|»'))
                if not next_link or 'disabled' in next_link.get('class', []):
                    break
        
        logger.info(f"Total transaction URLs collected: {len(all_transaction_urls)}")
        
        # Now scrape each individual transaction
        tasks = []
        for url in all_transaction_urls:
            task = self.fetch_and_parse_transaction(url)
            tasks.append(task)
        
        # Process in batches to avoid overwhelming the server
        results = await asyncio.gather(*tasks)
        
        # Filter out None results
        all_transactions = [t for t in results if t is not None]
        
        return all_transactions
    
    async def fetch_and_parse_transaction(self, url: str) -> Optional[Transaction]:
        """Fetch and parse a single transaction page"""
        html = await self.fetch_page(url)
        if html:
            return await self.parse_single_transaction(html, url)
        return None
    
    async def parse_transactions(self, html: str) -> List[Transaction]:
        """Parse transaction data from transaction page"""
        soup = BeautifulSoup(html, 'html.parser')
        transactions = []
        
        try:
            # Find all transaction blocks - try multiple approaches
            # Method 1: Look for transaction items with specific classes
            transaction_blocks = soup.select('.transaction-item, .record-item, [class*="transaction-record"], .estate-record-item')
            
            # Method 2: If no blocks found, try to find table rows
            if not transaction_blocks:
                # Look for transaction table
                transaction_table = soup.find('table', class_=re.compile('transaction|record'))
                if transaction_table:
                    # Skip header row(s)
                    transaction_blocks = transaction_table.find_all('tr')[1:]
            
            # Method 3: Look for divs containing transaction data patterns
            if not transaction_blocks:
                # Find all elements containing "Sold Price" text
                sold_price_elements = soup.find_all(text=re.compile(r'Sold\s+Price'))
                for elem in sold_price_elements:
                    # Get the parent container of this transaction
                    parent = elem.parent
                    while parent and parent.name not in ['div', 'tr', 'article', 'section']:
                        parent = parent.parent
                    if parent and parent not in transaction_blocks:
                        transaction_blocks.append(parent)
            
            logger.info(f"Found {len(transaction_blocks)} transaction blocks")
            
            for idx, block in enumerate(transaction_blocks):
                try:
                    # Get all text content from the block
                    text = ' '.join(block.stripped_strings) if hasattr(block, 'stripped_strings') else block.get_text(separator=' ', strip=True)
                    
                    # Skip if this doesn't look like a transaction
                    if 'Sold Price' not in text and 'HKD' not in text:
                        continue
                    
                    # Extract unit info - updated pattern to be more flexible
                    unit = ""
                    # Try multiple patterns
                    unit_patterns = [
                        r'(Flat\s+[A-Z0-9]+(?:/[A-Z0-9]+)?),?\s*([\d/]+F),?\s*(Tower|Block|Building)\s+([A-Z0-9]+(?:\s+[A-Z0-9]+)?)',
                        r'(Unit\s+[A-Z0-9]+),?\s*([\d/]+F)',
                        r'(Room\s+[A-Z0-9]+),?\s*([\d/]+F)'
                    ]
                    
                    for pattern in unit_patterns:
                        unit_match = re.search(pattern, text, re.IGNORECASE)
                        if unit_match:
                            unit = unit_match.group(0)
                            break
                    
                    # Extract address - look for text between unit and "Sold Price"
                    trans_address = ""
                    if unit:
                        addr_pattern = rf'{re.escape(unit)}\s+(.+?)\s+Sold\s+Price'
                        addr_match = re.search(addr_pattern, text, re.IGNORECASE | re.DOTALL)
                        if addr_match:
                            trans_address = addr_match.group(1).strip()
                    
                    # Extract sold price with better pattern
                    sold_price = 0
                    price_patterns = [
                        r'HKD?\s*\$?\s*([\d.]+)\s*Million',
                        r'Sold\s+Price[:\s]+HKD?\s*\$?\s*([\d.]+)\s*Million',
                        r'\$\s*([\d.]+)\s*M(?:illion)?'
                    ]
                    
                    for pattern in price_patterns:
                        price_match = re.search(pattern, text, re.IGNORECASE)
                        if price_match:
                            sold_price = float(price_match.group(1)) * 1000000
                            break
                    
                    # Extract registry date
                    registry_date = ""
                    date_patterns = [
                        r'Registry\s+Date[:\s]+([\d]{4}-[\d]{2}-[\d]{2})',
                        r'Date[:\s]+([\d]{4}-[\d]{2}-[\d]{2})',
                        r'([\d]{4}-[\d]{2}-[\d]{2})'
                    ]
                    
                    for pattern in date_patterns:
                        date_match = re.search(pattern, text)
                        if date_match:
                            registry_date = date_match.group(1)
                            break
                    
                    # Extract saleable area
                    saleable_area = 0
                    area_patterns = [
                        r'Saleable\s+area[:\s]+([\d,]+)\s*(?:sq\s*)?ft',
                        r'Area[:\s]+([\d,]+)\s*(?:sq\s*)?ft',
                        r'([\d,]+)\s*ft²'
                    ]
                    
                    for pattern in area_patterns:
                        area_match = re.search(pattern, text, re.IGNORECASE)
                        if area_match:
                            saleable_area = int(area_match.group(1).replace(',', ''))
                            break
                    
                    # Extract unit price
                    unit_price = 0
                    unit_price_patterns = [
                        r'Unit\s+Price[:\s]+\$?\s*([\d,]+)',
                        r'Price\s+per\s+ft[:\s]+\$?\s*([\d,]+)',
                        r'\$\s*([\d,]+)\s*/\s*ft'
                    ]
                    
                    for pattern in unit_price_patterns:
                        price_match = re.search(pattern, text, re.IGNORECASE)
                        if price_match:
                            unit_price = float(price_match.group(1).replace(',', ''))
                            break
                    
                    # Extract bedrooms
                    bedrooms = None
                    bedroom_patterns = [
                        r'Bedroom[s]?[:\s]+(\d+)',
                        r'(\d+)\s*Bedroom',
                        r'(\d+)\s*Room[s]?'
                    ]
                    
                    for pattern in bedroom_patterns:
                        bedroom_match = re.search(pattern, text, re.IGNORECASE)
                        if bedroom_match:
                            bedrooms = int(bedroom_match.group(1))
                            break
                    
                    # Extract transaction format
                    transaction_format = "Land Registry"  # Default
                    format_match = re.search(r'Transaction\s+Format[:\s]+([^,\n]+)', text, re.IGNORECASE)
                    if format_match:
                        transaction_format = format_match.group(1).strip()
                    
                    # Only add if we have essential data
                    if sold_price > 0 or saleable_area > 0:
                        transaction = Transaction(
                            unit=unit or f"Transaction {idx + 1}",
                            address=trans_address,
                            sold_price=sold_price,
                            registry_date=registry_date,
                            saleable_area=saleable_area,
                            unit_price=unit_price,
                            bedrooms=bedrooms,
                            transaction_format=transaction_format
                        )
                        transactions.append(transaction)
                        logger.debug(f"Added transaction: {unit} - ${sold_price/1000000:.2f}M")
                        
                except Exception as e:
                    logger.warning(f"Error parsing transaction {idx}: {str(e)}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error parsing transactions: {str(e)}")
        
        logger.info(f"Successfully parsed {len(transactions)} transactions")
        return transactions
    
    async def scrape_property_with_transactions(self, property_url: str) -> Optional[Property]:
        """Scrape a property and its transactions"""
        # Fetch property details
        html = await self.fetch_page(property_url)
        if not html:
            return None
        
        property_obj = await self.parse_property_details(html, property_url)
        if not property_obj:
            return None
        
        # Extract property ID from URL to construct transaction URL
        property_id_match = re.search(r'/estate/detail/([^/]+?)(?:-(\d+))?/?
    
    async def scrape_all(self) -> List[Property]:
        """Main scraping method"""
        all_properties = []
        
        # Scrape listing pages
        for page_num in range(1, self.max_pages + 1):
            if page_num == 1:
                list_url = f"{self.base_url}/en/estate/"
            else:
                list_url = f"{self.base_url}/en/estate/?page={page_num}"
            
            logger.info(f"Scraping listing page {page_num}")
            html = await self.fetch_page(list_url)
            
            if not html:
                logger.warning(f"Failed to fetch listing page {page_num}")
                continue
            
            property_urls = await self.parse_listing_page(html)
            logger.info(f"Found {len(property_urls)} properties on page {page_num}")
            
            if not property_urls:
                break  # No more properties
            
            # Scrape each property concurrently
            tasks = [self.scrape_property_with_transactions(url) for url in property_urls]
            results = await asyncio.gather(*tasks)
            
            # Filter out None results
            valid_properties = [p for p in results if p is not None]
            all_properties.extend(valid_properties)
            
            logger.info(f"Successfully scraped {len(valid_properties)} properties from page {page_num}")
        
        return all_properties

async def main():
    """
    Example usage of the HSE28 scraper
    
    The scraper works in the following steps:
    1. Scrapes property listing pages (up to max_pages)
    2. For each property, gets the details (name, address, metrics)
    3. Goes to the property's transaction listing page
    4. Paginates through transaction listings (up to max_pages)
    5. Visits each individual transaction page to get full details
    """
    # Set the number of pages to scrape
    MAX_PAGES = 2  # Applies to BOTH property listings AND transaction listings per property
    
    async with HSE28Scraper(max_pages=MAX_PAGES, max_concurrent_requests=5) as scraper:
        properties = await scraper.scrape_all()
        
        # Convert to JSON-serializable format
        data = []
        for prop in properties:
            prop_dict = asdict(prop)
            data.append(prop_dict)
        
        # Save to JSON file
        with open('28hse_properties.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # Print summary
        print(f"\nScraped {len(properties)} properties")
        total_transactions = sum(len(p.transactions) for p in properties)
        print(f"Total transactions: {total_transactions}")
        
        # Print sample data with more details
        if properties:
            print(f"\nSample property:")
            print(f"Name: {properties[0].property_name}")
            print(f"Street: {properties[0].street_num}")
            print(f"Address: {properties[0].address}")
            print(f"Metrics: {properties[0].metrics}")
            print(f"Transactions: {len(properties[0].transactions)}")
            
            # Show first few transactions if available
            if properties[0].transactions:
                print(f"\nFirst 3 transactions:")
                for i, trans in enumerate(properties[0].transactions[:3]):
                    print(f"\n  Transaction {i+1}:")
                    print(f"  Unit: {trans.unit}")
                    print(f"  Price: HKD${trans.sold_price/1000000:.2f}M")
                    print(f"  Date: {trans.registry_date}")
                    print(f"  Area: {trans.saleable_area} sqft")
                    print(f"  Unit Price: ${trans.unit_price}")

if __name__ == "__main__":
    # Example 1: Scrape with page limits
    asyncio.run(main())
    
    # Example 2: Scrape all pages (no limit)
    # async def scrape_all_pages():
    #     async with HSE28Scraper(max_pages=None, max_concurrent_requests=5) as scraper:
    #         properties = await scraper.scrape_all()
    #         print(f"Scraped {len(properties)} properties with no page limit")
    # 
    # asyncio.run(scrape_all_pages())
, property_url)
        if property_id_match:
            property_slug = property_id_match.group(1)
            property_id = property_id_match.group(2) or ""
            
            # Construct transaction URL
            if property_id:
                transaction_url = f"{self.base_url}/en/estate/detail/{property_slug}-{property_id}/transaction"
            else:
                transaction_url = f"{self.base_url}/en/estate/detail/{property_slug}/transaction"
            
            # Scrape all transactions with pagination
            property_obj.transactions = await self.scrape_all_transactions(transaction_url, self.max_pages)
            logger.info(f"Found {len(property_obj.transactions)} transactions for {property_obj.property_name}")
        
        return property_obj
    
    async def scrape_all(self) -> List[Property]:
        """Main scraping method"""
        all_properties = []
        
        # Scrape listing pages
        for page_num in range(1, self.max_pages + 1):
            if page_num == 1:
                list_url = f"{self.base_url}/en/estate/"
            else:
                list_url = f"{self.base_url}/en/estate/?page={page_num}"
            
            logger.info(f"Scraping listing page {page_num}")
            html = await self.fetch_page(list_url)
            
            if not html:
                logger.warning(f"Failed to fetch listing page {page_num}")
                continue
            
            property_urls = await self.parse_listing_page(html)
            logger.info(f"Found {len(property_urls)} properties on page {page_num}")
            
            if not property_urls:
                break  # No more properties
            
            # Scrape each property concurrently
            tasks = [self.scrape_property_with_transactions(url) for url in property_urls]
            results = await asyncio.gather(*tasks)
            
            # Filter out None results
            valid_properties = [p for p in results if p is not None]
            all_properties.extend(valid_properties)
            
            logger.info(f"Successfully scraped {len(valid_properties)} properties from page {page_num}")
        
        return all_properties

async def main():
    """Example usage"""
    # Set the number of pages to scrape
    MAX_PAGES = 2  # Adjust as needed
    
    async with HSE28Scraper(max_pages=MAX_PAGES, max_concurrent_requests=5) as scraper:
        properties = await scraper.scrape_all()
        
        # Convert to JSON-serializable format
        data = []
        for prop in properties:
            prop_dict = asdict(prop)
            data.append(prop_dict)
        
        # Save to JSON file
        with open('28hse_properties.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        # Print summary
        print(f"\nScraped {len(properties)} properties")
        total_transactions = sum(len(p.transactions) for p in properties)
        print(f"Total transactions: {total_transactions}")
        
        # Print sample data with more details
        if properties:
            print(f"\nSample property:")
            print(f"Name: {properties[0].property_name}")
            print(f"Street: {properties[0].street_num}")
            print(f"Address: {properties[0].address}")
            print(f"Metrics: {properties[0].metrics}")
            print(f"Transactions: {len(properties[0].transactions)}")
            
            # Show first few transactions if available
            if properties[0].transactions:
                print(f"\nFirst 3 transactions:")
                for i, trans in enumerate(properties[0].transactions[:3]):
                    print(f"\n  Transaction {i+1}:")
                    print(f"  Unit: {trans.unit}")
                    print(f"  Price: HKD${trans.sold_price/1000000:.2f}M")
                    print(f"  Date: {trans.registry_date}")
                    print(f"  Area: {trans.saleable_area} sqft")
                    print(f"  Unit Price: ${trans.unit_price}")

if __name__ == "__main__":
    asyncio.run(main())
